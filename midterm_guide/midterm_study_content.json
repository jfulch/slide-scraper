{
  "metadata": {
    "total_lectures": 10,
    "total_concepts": 867,
    "total_definitions": 609,
    "total_formulas": 88,
    "total_algorithms": 179,
    "total_examples": 382,
    "lectures_covered": [
      "deduplication",
      "info_retrieval",
      "inverted_indexing",
      "querying",
      "se-basics",
      "se-evaluation",
      "text_processing",
      "web_crawling",
      "web_serving_basics",
      "youtube"
    ]
  },
  "content_by_lecture": {
    "deduplication": [
      {
        "lecture": "deduplication",
        "concepts": [
          "1. **De-Duplication** -  (process of identifying and avoiding essentially identical web pages)",
          "2. **Locker Storage** -  (strategy where only single copy of file is stored with multiple links to the single file)"
        ],
        "definitions": [
          "1. **De-Duplication** -  (identifying and eliminating duplicate web pages)",
          "2. **Locker Storage** -  (storage strategy for maintaining a single copy of a file)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **Web Crawling with De-Duplication** -  (identifying identical and nearly identical web pages and indexing only a single version to return as search result)"
        ],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "s1.png",
        "raw_text": "¢ De-Duplication — the process of identifying and avoiding essentially\nidentical web pages\n¢ The term is often used in connection with locker storage where only\nsingle copy of  file is stored and multiple links to the single file are\nmanaged\n— Whether this strategy effectively saves space is not clear and.needs\nanalysis for each particular application\n— However, this is not the meaning of the term that we are concerned\nabout in this class\n* With respect to web crawling, de-duplication essentially refers to the\nidentification of identical and nearly identical web pages and indexing\nonly  single version to return as  search result\nCopyright 2011-2022 Ellis Horowitz 2\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "1. **Deduplication**:  - The process of removing duplicate copies of data or URLs while maintaining their unique identities.",
          "2. **Virtual hosts**:  - A feature that allows multiple hostnames to share the same document folder, but have different domain names.",
          "3. **URL structure**:  - The components of a URL (protocol, hostname, path, page name) can be distinct yet still point to the same page."
        ],
        "definitions": [
          "1. **Virtual hosts**:  - A method for hosting multiple websites on a single server, where each website has its own domain name and is accessible through different hostnames.",
          "2. **Deduplication**:  - The process of eliminating duplicate data or URLs while preserving their unique characteristics."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **Same page with different URLs**:  - Two URLs (http://espn.go.com, http://www.espn.com) can point to the same page due to virtual hosts and distinct URL structures.",
          "2. **Virtual hosts example**:  - A website with multiple hostnames sharing the same document folder, but having different domain names."
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "s2.png",
        "raw_text": "* One example is the same page, referenced by different URLs\nhttp://espn.go.com http://www.espn.com\n‘   ‘\n{|  ee  eed {|  ee  eed\nee ‘Oa S, ge |= MCS, ge SSS\nAntti-Climactic ween METRES Antti-Climactic ween METRES\nLa RS fia La RS fia\n«How can two URLs differ yet still point to the same page?\n* the URL’  host name can be distinct (virtual hosts) sharing the same document folder,\n* the URL’  protocol can be distinct (http, https), but still deliver the same document\n* the URL’  path and/or page name can be distinct\nCopyright 2011-2022 Ellis Horowitz 3\neT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Deduplication: removing duplicate data or records to improve efficiency and reduce storage needs."
        ],
        "definitions": [
          "**SCOP (Structural Classification of Proteins)**: a database that classifies proteins based on their structural characteristics.",
          "**Domain**: a unit of protein structure that performs a specific function.",
          "**Fold**: a common three-dimensional structure found in multiple proteins.",
          "**Superfamily**: a group of related folds with similar structures.",
          "**Family**: a group of related superfamilies with similar structures."
        ],
        "formulas": [],
        "algorithms": [
          "The process of deduplication involves identifying and removing duplicate data or records, which can be achieved through various methods such as:"
        ],
        "examples": [
          "The SCOP database provides an example of deduplication in action, where multiple URLs point to the same page, demonstrating the removal of duplicate data."
        ],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "s3.png",
        "raw_text": "SS\n. .  5CoP: structural Gassficaton = WB\n+ At one time* all 3 URLs below pointed to\naon €\nthe identical page\n+ — Structural Classification of Proteins ‘Structural Classification of Proteins al\n—_ http://scop.mre-lmb.cam.ac.uk/scop | —\n—_ http://scop.berkeley.edu/  eS | :\n— http://scop.protres.ru/ ‘Welcome to SCOP: Structural Classification of\nProtems.\n1.75 release (June 2009)\n38221 PDB Entries. 1 Literature Reference. 110800\n— The three URLs have distinct Domains, (excluding nucleic acids and theoretical models),\n‘ ; Folds, superfamlies, and families statistics here\ndomain names, but all redirect to Teun\nthe same page List of obsolete entries and their replacements\nAnthors, Alexey G. Murzin, John-Marc Chandonia, Antonina Andreeva, Dave Howorth, Loredana Lo Conte, Bartlet\nAiley, Steven E, Brenner, Tim J. P. Hubbard, and Cyrus Chothia. scop@mrc-Imb cam.ac.uk\nReference: Murzin A. G., Brenner S. E., Hubbard T., Chothia C. (1995), SCOP:  structural classification of proteins\n* At least they did when  took this database for the investigation of sequences and structures. J. Mol. Biol. 247, 536-540. [PDE]\nRecent changes are described in Lo Conte L., Brenner $. E., Hubbard T.JP., Chothia C., Murzin A. (2002). SCOP\nsnapshot, no longer database in 2002: refinements accommodate structural genomics. Nucl. Acid Res. 30(1), 264-267. [PDE],\nAndreeva A., Howorth D., Brenner S.E,, Hubbard T.P., Chothia C., Murzin A.G, (2004). SCOP database in 2004\nrefinements integrate structure and sequence family data. Mucl. Acid Res. 32D226-D229. [PDE], and\nAndteeva A, Howorth D., Chandonia J-M., Brenner S.E., Hubbard TI.P., Chothia C., Murzin AG. (2007). Data\nCopyright 2011-2022 Ellis Horowitz 4\nEOE eeOw|"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Data Duplication",
          "Deduplication (concept of removing duplicate data)",
          "Similarity between web pages (differing slightly)"
        ],
        "definitions": [
          "Deduplication: The process of identifying and eliminating duplicate copies of data.",
          "Snapshot: A copy of a webpage at a particular point in time."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Two web pages from www.nytimes.com with slight differences in content (ads and photo)."
        ],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "s4.png",
        "raw_text": "Another example is two web pages whose content differs slightly\norother “Ss ol\n— woawone | irijolas oe Protect ie }\naa om wom  == amazon your home ry\nEhe New York Times  Shop smart cameras + |\nwa aon  nnn ee The New York Eimes\nDesc on  hie & ] Rar Ren ir so ee ce He ete ee tH\nTwo copies of www.nytimes.com snapshot within  few seconds of each other;\nThe pages are essentially identical except for the ads-at the top and the photo in the middle\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Deduplication (not explicitly mentioned in this slide, but relevant to the topic)"
        ],
        "definitions": [
          "**Document Object Model (DOM)**: a tree-based structure for representing an HTML document"
        ],
        "formulas": [],
        "algorithms": [
          "1. Deconstructing web page structure by focusing on content blocks"
        ],
        "examples": [
          "Analyzing a web page's DOM structure using HTML tags (e.g., Document, Head, Body)"
        ],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "s5.png",
        "raw_text": ".  Gee ew tiny Grimes Doe te _—\n+ In examining  web page  €)2 [hosters <)(a- DIEEICSEIE\nsearch engine may want to | & Then Yokes Brean ws, Wns [|\nignore ads, navigation links HOME PAGE | TODAY' PAPER | VIDEO | MOST POPULAR | TIMES TOPICS ‘Subscribe: amen he\nand other elements that do not Make The New York Eines Florid4\nspecifically relate to the ae  gl aod sine &Key)\ncontents of the web page = om HLY | (BP + saeameto tone nt Pecan\n* One way to do this is to delve Gleb totion» By sourncanouna puma | | peel\ninto the structure of  web Bee After Victory by Varatiosnan RIM\npage and focus on content PER) <2 |= || console uma, 55 saint [oor ~ | Net Pave Sens  aoa\nblocks oo Recor eeenes\n. vprattypeUd 5\n+ E.g. the Document Object = rmnnsant ca\nModel for HTML displays- = ca\nweb page as  tree hjerarch: eer 4\nporettement Zz\n— Document & scripts [scrape Srgitce... aij, seript, script hrn4).2/s, 40 more 1\nweneert Occ { getimensone-tincton), getsotOtete-tencion), gen), more\n— Head — ee\nsricoer cs\n— Body aaa\n¢* However this is time bets vaceps: /mov-nyeines.con/*\nconsuming —\n& esos [nenumtTye | oneueairSennmenye, wedwine—raaa®, eodsTypeis, wore,\nEatin Conebacoremment, aurea Meisctmalehes lmao ren ts\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "* Mirroring ()"
        ],
        "definitions": [
          "* Mirroring: systematic replication of web pages across hosts ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "s6.png",
        "raw_text": "¢ Mirroring is the systematic replication of web pages across hosts.\n— Mirroring is the single largest cause of duplication on the web\n¢ Hostl/ and Host2/ are mirrors iff\nFor all (or most) paths  such that when\nhttp://Host1/  /  exists\nhttp://Host2/  /  exists as well\nwith identical (or near identical) content, and vice versa.\nCopyright 2011-2022 Ellis Horowitz 7\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Apache mirrors",
          "Deduplication ( implied by the context of the slide)"
        ],
        "definitions": [
          "Apache mirrors: a collection of servers that mirror or replicate content from a central server (implied by the context)",
          "Regions: geographic areas where Apache mirrors are located (listed in the \"regions\" section)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "List of countries with Apache mirrors (e.g. \"fi\", \"ge\", \"gr\", etc.)",
          "Number of sites in 55 regions (281)",
          "Update frequency (4 hours)"
        ],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "s7.png",
        "raw_text": "the sats of apace mirors- MrllaFisfox FE ESR)\nEle Edt Yew History Bookmarks Yahoo! Tools Help. 46.2 mutes saved @) >\n<¢-2>-€@ GDN pvt. apache oraimroes) +>)\nthe status of apache mirrors Th\nie!\" Apache\ndae Moniin72001901207)  SEFtware Foundation\nlast check » Mon Jan 22 00:19:01 2007 (GMT) http://www.apache.org/\n: : regions\nList of countries\nSS\nfi ge gr hk hr hn id ie il is it jp ky  hy by mx my al no pl pt 10\nse sg si sk th t& tw ua uk us za\n. . . report\n281 sites in 55 regions\nwow eu apache org @ | http 4 hours hours | ok,\nwoz wadeos @| hows Shows ok\napathelocahostnct  @ [bite 6 hon Shows ok |\n= Oive wy\nCopyright 20TT=Z0 SHOTOW\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "**Deduplication**: Not explicitly mentioned in the text, but implied to be related to finding software releases from Apache Software Foundation."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  **Finding the right download for a project**: Start at the project's webpage or resource page, rather than browsing links directly on Apache.org."
        ],
        "examples": [
          "**Apache Software Foundation projects**: Various software releases listed, including:"
        ],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "s8.png",
        "raw_text": "Indexof /- Mozilla Firefox Fe Fe\nfle Gat Mow Hsly Goomats Yahoo! Tole He 462mutassoved @)\nE> D-  CG BW [ rwoiteoxteines.on.«]=]) [er 5\npr Apache Software Foundation\nIndex of Jet Mozilla Firefox FS OS®) rn .\nco  ‘s62nndes sored Distribution Directory\nSE - -  GB MB [Nrewitreate poeivir con ake[=B) [30 2\n“The diecore inked below contin current sofware releases fom the Apache\nTesasreviowe. 6) | Li scievaveme.. | Wisbawdersa. Li Ninlenol int [> Software Foundation projects, Older non-recommended releases canbe found on\ncur achive ste.\nIndex of /dist\n‘To find he right download for aparicular project, you shoul tart atthe project'\ncum webpage or on our proect resource king rather than browsing the inks\nPlease donot download from apache. org! you are cutenty at apache.org\n@ vorent virectory ae-gan-2007 10:10 and would cet browe,pleae instead vst  neasby miro ote\nBax a2-dan-2007 10:10 ;\nOa a2-dan-2007 10:10 - Projects\nsoi 2a-way-2008 01:08\ncoggnner re-Jan-2007 20:18\ncocos 2a-dan-2007 10:10 = @ vacene directory oa-pec-z006 14:48 -\n@ consions/ 22-gan-2007 10:10 - DB anes 19-Dec-2006 23:24 -\nDB airectory/ 26-Feb-2006 10:12 - @ avaions 20-May-2004 12:08 -\nrea [free wort @ revue L) Panag at] match case DB conmone/ 04-Nov-2009 06:08 -\none os & Oa 13-Dec-2008 00:31 -\nDB ascectorys 25-Feb-2006 21:12 -\n@ excoripurr ae-ing-2005 23:38 =\ntos . Note identical\nSite in Australia  2\ndirectories Sone Bom Eh\nCopyright 2011-2022 Ellis Horowitz Site in Argentina 9\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "*  **Deduplication**: Avoiding or minimizing duplicate results in crawling",
          "*  **Smarter Crawling**: Optimizing crawling to reduce resources and increase politeness",
          "*  **Better Connectivity Analysis**: Combining in-links from multiple mirror sites for accurate PageRank"
        ],
        "definitions": [
          "*  **PageRank (measure of importance)**: A measure of a webpage's importance based on its in-links",
          "*  **Mirror Sites**: Copies of websites hosted on different servers"
        ],
        "formulas": [],
        "algorithms": [
          "*  **Smarter Crawling Algorithm**:",
          "*  **Better Connectivity Analysis Algorithm**:"
        ],
        "examples": [
          "*  **“If that fails you can try: <mirror>/samepath”**: An example of adding redundancy in result listings"
        ],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "s9.png",
        "raw_text": "¢ Smarter crawling\n— Avoid returning many duplicate results to  query\n— Allow fetching from the fastest or freshest server\n¢ Better connectivity analysis\n— By combining in-links from the multiple mirror sites to get an accurate\nPageRank (measure of importance)\n— Avoid double counting out-links\n¢ Add redundancy in result listings\n— “If that fails you can try: <mirror>/samepath”\n* Reduce Crawl Time: Crawlers need not crawl pages that are identical or near\nidentical\n* Ideally: given the web’  scale and complexity, priority must be given to\ncontent that has not already been seen before or has recently changed\n— Saves resources (on the crawler end, as well as the remote host)\n— Increases crawler politeness\n— Reduces the analysis that  crawler will have to do later\nCopyright 2011-2022 Ellis Horowitz 10\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "**Deduplication**: The process of identifying and removing duplicate or near-duplicate data."
        ],
        "definitions": [
          "*  **Clustering**: Grouping similar documents together.",
          "*  **Plagiarism detection**: Identifying pairs of documents that have significantly borrowed from each other.",
          "*  **Spam detection**: Using near-similarity techniques to identify spam emails."
        ],
        "formulas": [],
        "algorithms": [
          "None explicitly mentioned in the slide content."
        ],
        "examples": [
          "*  Identifying related articles describing the same event.",
          "*  Extracting and categorizing information from a collection of similar pages (e.g., movie reviews).",
          "*  Identifying near-duplicates arising out of revisions, modifications, copying or merging of documents within a domain."
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "s10.png",
        "raw_text": "* Clustering\n— Given  news article some people might wish to see “related articles” describing\nthe same event\n¢ Data extraction\n— Given  collection of similar pages, e.g. movie reviews,  search engine can\nextract and categorize the information\n¢ Plagiarism\n— Identify pairs that seem to have significantly borrowed from each other\n¢ Spam detection\n— Spammers typically send similar emails en masse, so one can use near-similarity\ntechniques to identify the spam\n¢ Duplicates within  domain\n— To identify near-duplicates arising out of revisions, modifications, copying or\nmerging of documents\nCopyright 2011-2022 Ellis Horowitz\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "*  Duplicate Problem: Exact match vs. Near-Duplicate Problem: Approximate match",
          "*  Cryptographic hashing for exact match detection",
          "*  Syntactic similarity with edit-distance measure for near-duplicate detection"
        ],
        "definitions": [
          "*  Duplicate Problem: A problem where duplicate data needs to be identified.",
          "*  Near-Duplicate Problem: A problem where approximate matching is required, e.g., detecting similar documents.",
          "*  Cryptographic hashing: A technique used for generating unique digital fingerprints of data."
        ],
        "formulas": [],
        "algorithms": [
          "*  Compute fingerprints using cryptographic hashing",
          "*  Syntactic similarity with edit-distance measure"
        ],
        "examples": [
          "*  URL matching using cryptographic hashing",
          "*  Detecting identical web pages using hash values"
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "s11.png",
        "raw_text": "1. Duplicate Problem: Exact match;\n¢ Solution: compute fingerprints using cryptographic hashing\n* Useful for URL matching and also works for detecting identical\nweb pages\n* Hashes can be stored in sorted order for Jog  access\n2. Near-Duplicate Problem: Approximate match\n* — Solution: compute the syntactic similarity with an edit-distance\nmeasure, and\n« Use  similarity threshold to detect near-duplicates\n— e.g., Similarity > 80% => Documents are “near duplicates”\n¢ The remaining slides are devoted to specific methods for\nduplicate and near duplicate detection\nCopyright 2011-2022 Ellis Horowitz 12\nLeone"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Cryptographic hash function",
          "Input Digest (a fixed-size alphanumeric string)",
          "Hash value (also known as message digest, digital fingerprint, or checksum)",
          "Properties of a cryptographic hash function:"
        ],
        "definitions": [
          "Cryptographic hash function: A hash function that takes an input (message) and returns a fixed-size alphanumeric string (hash value)",
          "Hash value: Also known as message digest, digital fingerprint, or checksum",
          "Input Digest: A fixed-size alphanumeric string resulting from a cryptographic hash function"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Example of how a small change in the input text can produce a major difference in the output hash value (referenced to Wikipedia article: https://en.wikipedia.org/wiki/Cryptographic_hash_function)"
        ],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "s12.png",
        "raw_text": "*  cryptographic hash function is Input Digest\nhash function which takes an input (or rm | OREO | lee 252 me  Sa\n'message') and returns  fixed-size\nalphanumeric string, which is called\nthe hash value (sometimes called eee, + ee | lt amr  ee\nmessage digest, digital fingerprint,\ndigest or  checksum). ese, + Oe | ofS Hs aes me |\n¢ The cryptographic hash function has\nfour main properties: (eee, | mee | gna Sa Sr\n1. It is extremely easy (i.e. fast) to\ncalculate  hash for any given data. eg mee | oles ss ie\n2. Itis extremely computationally\ndifficult to calculate an\nalphanumeric text that has  given  small change in  single word, “over” produces  major\nhash. Change in the output; see\nhttps://en.wikipedia.org/wiki/Cryptographic_hash_function\n3.  small change to the text yields\ntotally different hash value.\n4. It is extremely unlikely that two\nslightly different messages will\nhave the same hash.\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "**Cryptographic hash functions**: widely used for data deduplication and security.",
          "**Message-digest (MD) hash function**: produces a fixed-size string of characters that represents the original input data.",
          "**SHA-1**, **SHA-2**, and **SHA-3** families: types of cryptographic hash functions with different digest sizes.",
          "**RIPEMD-160**: family of cryptographic hash functions that produces 160-bit digests."
        ],
        "definitions": [
          "**MD5 (message-digest)**: a widely used cryptographic hash function producing a 128-bit (16-byte) hash value.",
          "**SHA-1**: a type of cryptographic hash function producing a 160-bit (20-byte) hash value.",
          "**SHA-2**: a family of algorithms that produce digests of size 224, 256, 384, and 512 bits."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**Verisign**: an example of a certificate authority that uses cryptographic hash functions."
        ],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "s13.png",
        "raw_text": "* The MD5 (message-digest) hash function is  widely used cryptographic hash\nfunction producing  128-bit (16-byte) hash value, typically expressed in text format\nas  32 digit hexadecimal number.\n— Invented by Ron Rivest of MIT in 1991; replaced the earlier MD4\n* The SHA-1, SHA-2 hash functions are also quite popular (160 bit, 20 byte value)\n— SHA-1 was broken in 2005; using SHA-2 family of algorithms is now favored,\nsee\n—_ https://en.wikipedia.org/wiki/SHA-2\n* SHA-3, released in 2015; it produces digests of size 224, 256, 384 and 512 bits\n« RIPEMD-160 —  family of cryptographic hash functions and so far has not been\nbroken; produces  160 bit (20 byte) digest\n+ E.g. See Chrome, Settings, Security and Privacy, Security, Manage certificates,\ncertificates, Verisign\nCopyright 2011-2022 Ellis Horowitz 14\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "*  **Deduplication**: process of identifying and eliminating duplicate documents",
          "*  **Hash function**: a one-way function that takes input data and produces a fixed-size output, known as a hash value or digest",
          "*  **Bucketing**: dividing documents into groups based on their hash values"
        ],
        "definitions": [],
        "formulas": [
          "*  **O(log n)**: time complexity for searching in sorted order (Note: no actual formula or equation is given in this content)"
        ],
        "algorithms": [],
        "examples": [
          "*  **Web pages with common prefix**: example of a problem where simple hashing approaches fail (e.g., all web pages start with \"<HTML>\")"
        ],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "s14.png",
        "raw_text": "I. Compare character by character two documents to see if they are identical\n— very time consuming !!\n2. Hash just the first few characters and compare only those documents that hash to\nthe same bucket\n— But what about web pages where every page begins with <HTML>.??\n3. Use  hash function that examines the entire document\n— But this requires lots of buckets\n4. Better approach - pick some fixed random positions for all documents and make the\nhash function depend only on these;\n— This avoids the problem of  common prefix for all or most documents, yet we\nneed not examine entire documents unless they fall into  bucket with another\ndocument\n— But we still need  lot of buckets\n5. Even better approach: Compute the cryptographic hash (SHA-2 or MDS) of each\nweb page and maintain in sorted order, O(log n) to search\nCopyright 2011-2022 Ellis Horowitz 15\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "s15.png",
        "raw_text": "1. Produce fingerprints and test for similarity - Treat web documents as\ndefined by  set of features, constituting an n-dimensional vector, and\ntransform this vector into an (bit fingerprint of  small size\n— Use Simhash or Hamming Distance to compute the fingerprint\n* SimHash is an algorithm for testing how similar two sets are\n— Compare fingerprints and look for  difference in at most  bits\n— E.g. see Manku et al. WWW 2007, Detecting Near-Duplicates for Web\nCrawling, http://www2007.org/papers/paper215.pdf\n2. Instead of documents defined by n-vector of features, compute subsets of\nwords (called shingles) and test for similarity of the sets\n— Broder et al., WWW 1997, Finding Near Duplicate Documents\nCopyright 2011-2022 Ellis Horowitz 16\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Deduplication",
          "Hash function",
          "Signature",
          "Fingerprint",
          "Near duplicates"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "s16.png",
        "raw_text": "1. Define  function  that captures the contents of each document in\nnumber\n—  E.g. hash function, signature, or  fingerprint\n2. Create the pair <f(doc,), ID of doc;> for all doc;\n3. Sort the pairs\n4. Documents that have the same f-value or an f-value within  small\nthreshold are believed to be duplicates or near duplicates\nCopyright 2011-2022 Ellis Horowitz 17\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Distance measure must satisfy 4 properties:"
        ],
        "definitions": [
          "Distance measure: a method to compute similarity between two objects or documents.",
          "Euclidean distance: a type of distance measure that calculates the straight-line distance between two points in n-dimensional space.",
          "Jaccard distance: a type of distance measure that calculates the ratio of the sizes of the intersection and union of sets.",
          "Cosine distance: a type of distance measure that calculates the angle between two vectors.",
          "Edit distance: a type of distance measure that calculates the minimum number of insertions and deletions needed to transform one string into another.",
          "Hamming distance: a type of distance measure that calculates the number of components in which two Boolean vectors differ."
        ],
        "formulas": [
          "Euclidean distance: D([X}..-Xn], [¥1.--sYnl) = sqrt(Sum(x;-y;)*2) i=1...0",
          "Jaccard distance: D(x,y) = 1 — SIM(x,y)",
          "Cosine distance: (usually represented as an angle between 0 and 180 degrees)"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "s17.png",
        "raw_text": "* To compute similarity, we need  distance measure\n«  distance measure must satisfy 4 properties\n1. No negative distances\n2. Dixy) = 0 iffx=\n3. D(x,y) = D(y,x) symmetric\n4. Dixy) <= D(x,z) + D(z, y) triangle inequality\n+ There are several distance measures that can play  role in locating duplicate and near-\nduplicate documents\n— Euclidean distance — D([X}..-Xn], [¥1.--sYnl) = sqrt(Sum(x;-y;)*2) i=1...0\n— Jaccard distance — D(x,y) = 1 — SIM(x,y) or 1 minus the ratio of the sizes of the\nintersection and union of sets  and\n— Cosine distance — the cosine distance between two points (two  element vectors) is the\nangle that the vectors to those points make; in the range 0 to 180 degrees\n— Edit distance — the distance between two strings is the smallest number of insertions and\ndeletions of single characters that will convert one string into the other\n— Hamming distance — between two vectors is the number of components in which they\ndiffer (usually used on Boolean vectors)\nCopyright 2011-2022 Ellis Horowitz 18\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "* Sets as unordered collections of objects (e.g., {a, b, c})",
          "* Distance between sets (d(A, B))",
          "* Similarity between sets (s(A, B))"
        ],
        "definitions": [
          "* d(A, B) = distance between two sets A and B",
          "* s(A, B) = similarity between two sets A and B"
        ],
        "formulas": [
          "* d(A, B) = 0 if A and B are the same",
          "* d(A, B) ∈ [0, ∞] (distance is in the range [0, infinity])",
          "* s(A, B) = 1 if A and B are the same",
          "* s(A, B) ∈ [0, 1] (similarity is in the range [0, 1])",
          "* d(A, B) = 1 - s(A, B) (relationship between distance and similarity)"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "s18.png",
        "raw_text": "¢ Aset is an unordered collection of objects, e.g. {a, b, c}\n¢ Focusing on the notion of distance of two sets we define  distance d(A, B) as\n— small, if objects in  and  are close;\n— large, if objects in  and  are far apart;\n— 0, if they are the same, and finally\n— d(A, B) is in the range [0, infinity]\n* Focusing on the notion of similarity we define s(A, B) as:\n— large, if the objects in  and  are close;\n— small, if the objects in  and  are far apart;\n— 1, if they are the same, and finally\n— s(A, B) is in the range [0, 1]\n¢ Often we can convert between the two, as d(A, B) = 1 — s(A, B)\nCopyright 2011-2022 Ellis Horowitz 19\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "* Jaccard similarity",
          "* Clustering"
        ],
        "definitions": [
          "* Intersection: size of intersection between two sets / size of union",
          "* Union: size of intersection between two sets / size of union  (same as above, but note that it's not a standard definition)"
        ],
        "formulas": [
          "* JS(A, B) = size( intersection A ∩ B )/size( union A ∪ B )",
          "* SISau(A, B) = IS(A∪B) = size( ( {C1, C3} intersect {C1, C2, C3, C4} )/( {C1, C3} union {C1, C2, C3, C4})"
        ],
        "algorithms": [],
        "examples": [
          "* Consider sets A = {0, 1, 2, 5, 6} and B = {0, 2, 3, 5, 7, 9}",
          "* Suppose we divide our items into four clusters..."
        ],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "s19.png",
        "raw_text": "* Consider  = {0, 1, 2, 5, 6} and  = {0, 2, 3, 5, 7, 9}\n¢ JS(A, B) = size( intersection B)/size( union B)\n. = size({0, 2, 53) / size({0, 1, 2, 3, 5, 6, 7, 93)\n° = 3/8 =0.375\n¢ Suppose we divide our items into four clusters, e.\n~ Cc, = {0, 1, 2}\n~— C,= {3,45 perhaps\n_ C; = {5, 6} C, represents action movies, C, comedies,\n_ CG _ (7, 8, 9} C; documentaries, Cy horror movies\n*  SISau(A, B) = IS(Acuy Baw) =\n*  size( ( {C1, C3} intersect {C1, C2, C3, C4} )/( {C1, C3} union {C1, C2, C3, C4}) )\n. =5/10 = 0.5\n+ If we are going to use Jaccard similarity to determine when two web pages are near\nduplicates; we need to say what are the elements of the sets we are comparing\nCopyright 2011-2022 Ellis Horowitz 20\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "**Shingle**: a contiguous subsequence of words in a document",
          "**Similarity Measures**: Jaccard(A,B), Containment(A,B)",
          "**Resemblance**: similarity measure between two documents (0 <= Resemblance <= 1)"
        ],
        "definitions": [
          "**Shingle Set (S(D,w))**: a set of shingles for a document D with width w",
          "**Jaccard(A,B)**: similarity measure defined as size of (S(A,w) intersect S(B,w)) / size of (S(A,w) union S(B,w))",
          "**Containment(A,B)**: similarity measure defined as size of (S(A,w) intersect S(B,w)) / size of (S(A,w))"
        ],
        "formulas": [
          "**Jaccard(A,B) = size of (S(A,w) intersect S(B,w)) / size of (S(A,w) union S(B,w))**",
          "**Containment(A,B) = size of (S(A,w) intersect S(B,w)) / size of (S(A,w))**"
        ],
        "algorithms": [],
        "examples": [
          "Shingleing example: \"rose is rose is rose\" produces a set S(D,w) with 5 items"
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "s20.png",
        "raw_text": "* Definition of Shingle:\n— acontiguous subsequence of words in  document is called  shingle;\nThe 4-shingling of the phrase below produces  bag of 5 items:\n“ rose is  rose is  rose” =>  set S(D,w) is defined as\n{ (a_rose_is_a), (rose_is_a_ rose), (is_a_rose_is),( Tose is_a),\n(rose_is  rose)}\n— S(D,w) is the set of shingles of  document  of width\n¢ Similarity Measures\n— Jaccard(A,B) (also known as Resemblance) is defined as\nsize of (S(A,w) intersect S(B,w)) / size of (S(A,w) union S(B,w))\n— Containment(A,B) is defined as\nsize of (S(A,w) intersect S(B,w)) / size of (S(A,w))\n— 0<=Resemblance <= 1\n— 0<=Containment <= 1\n* See On the resemblance and containment of documents, Conf. on Compression and\nComplexity, DEC Research Center, 1997\nCopyright 2011-2022 Ellis Horowitz 21\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [],
        "definitions": [
          "1.  **Shingle**: a fixed-size substring of a document",
          "2.  **Collision**: when two different documents match each other's shingles (exact wording preserved)",
          "3.  **Stop words**: common words that are typically omitted in deduplication"
        ],
        "formulas": [],
        "algorithms": [
          "1.  Deduplication process using shingling:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "s21.png",
        "raw_text": "* White space?\n— Should we include spaces and returns? Sometimes it makes sense, e.g.\n“plane has touch down” versus “threw  touchdown”\n(the space between “touch” and “down” is significant)\n* Capitalization?\n— Sam versus sam. Can help to distinguish proper nouns\n* Punctuation?\n— English is punctuated differently in the US and India; punctuation differs in articles, blogs,\nand tweets\n* How large should  be?\n— General rule: high enough so the probability of almost all shingles matching is low, so\ncollision is meaningful;\n* Count replicas?\n— Typically bag of words counts replicas, but shingling does not\n¢ Stop words? Typically omitted as they are so common\nCopyright 2011-2022 Ellis Horowitz 2\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "*  Deduplication: a process of finding near duplicates in a large dataset",
          "*  Tropical fish example: used to illustrate deduplication concept, not essential for exam",
          "*  3-shingles: sets of three consecutive words used as a basis for deduplication"
        ],
        "definitions": [
          "*  Hash value: a unique numerical representation of a string or sequence",
          "*  Fingerprints: selected hash values that are considered representative of the original data"
        ],
        "formulas": [],
        "algorithms": [
          "*  Step 1: Divide text into 3-shingles (sets of three consecutive words)",
          "*  Step 2: Calculate hash value for each 3-shingle",
          "*  Step 3: Select a subset of hash values that meet certain criteria (e.g., divisible by some number)"
        ],
        "examples": [
          "*  Tropical fish text with 3-shingles and their corresponding hash values",
          "*  Example of selecting a subset of hash values (e.g., those divisible by some number)"
        ],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "s22.png",
        "raw_text": "* Original text\n— “Tropical fish include fish found in tropical environments around the world, including\nboth freshwater and salt water species”\n+ All 3-shingles (there are 16 of them)\n— (Tropical fish include), (fish include fish), (include fish found), (fish found in), (found in\ntropical), (in tropical environments), (tropical environments around), (environments\naround the), (around the world), (the world including), (world including both), (including\nboth freshwater), (both freshwater and), (freshwater and salt), (and salt water), (salt water\nspecies)\n* Hash values for the 3-shingles (sets of shingles are large, so we hash them to make them\nmore manageable, and we select  subset)\n— 938, 664, 463, 822, 492, 798, 78, 969, 143, 236, 913, 908, 694, 553, 870, 779\n* Select only those hash values that are divisible by some number, e.g. here are selected\nhash values using 0 mod 4\n— 664, 492, 236, 908; these are considered the fingerprints\n* Near duplicates are found by comparing fingerprints and finding pairs with  high\noverlap\nCopyright 2011-2022 Ellis Horowitz 23\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "*  Deduplication",
          "*  Jaccard similarity of sets",
          "*  Jaccard distance of sets"
        ],
        "definitions": [
          "*  Jaccard similarity (J(A,B)) = |A ∩ B| / |A ∪ B|",
          "*  Jaccard distance = 1 - J(A,B)",
          "*  k-shingles",
          "*  Fingerprint"
        ],
        "formulas": [
          "*  Jaccard similarity (J(A,B)) = |A ∩ B| / |A ∪ B|",
          "*  Jaccard distance = 1 - J(A,B) or {(union B) - (intersect B)}/(union-B)"
        ],
        "algorithms": [
          "*"
        ],
        "examples": [
          "*  Testing if two pages are near duplicates using Jaccard similarity (e.g., greater than 0.9)"
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "s23.png",
        "raw_text": "* Recall the Jaccard similarity of sets  and B, J(A,B), is defined as\n|  intersect  | /\\ union  |\n¢ The Jaccard distance of sets  and B, measuring dissimilarity is defined as\n1 - J(A,B), or equivalently {( union B) - ( intersect B)}/( union-B)\n* We can test if two pages are near duplicate by\n1. First compute the k-shingles of the two pages\n2. Map the k-shingles into numbers (e.g. by hashing)\n3. Select  subset of the shingles to act as the fingerprints\n4. Compute the Jaccard similarity of the k-shingle fingerprints;\n¢ Ahigh Jaccard similarity (e.g. greater than 0.9), implies the pages are near\nduplicate; or\n° if ( (fingerprint(A), fingerprint(B)) ) > k, then the pages are similar;\nCopyright 2011-2022 Ellis Horowitz 24\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "SimHash is a method for determining near duplicates of web pages.",
          "Near duplicates are determined by an f-bit fingerprint, where a pair of documents are near duplicates if their fingerprints are at most k-bits apart."
        ],
        "definitions": [
          "**SimHash**: a method developed by Moses Charikar for determining near duplicates of web pages.",
          "**f-bit fingerprint**: a fingerprint obtained for each document using the SimHash method.",
          "**k-bits apart**: a measure of the distance between two fingerprints, where a pair of documents are near duplicates if their fingerprints are at most k-bits apart."
        ],
        "formulas": [
          "Documents D1 and D2 are near duplicates iff Hamming-Distance(Simhash(D1), Simhash(D2)) ≤ κ"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "HIGH",
        "slide_number": 24,
        "slide_file": "s29.png",
        "raw_text": "¢ There is another way to determine if two web pages are near duplicates\n* The method is called SimHash\n* It was developed by Moses Charikar and is described in his paper\nSimilarity Estimation Techniques from Rounding Algorithms, STOC\nMay 2002\n—_ https://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarEstim.\npdf\n* The basic idea is the same as before\n— obtain an f-bit fingerprint for each document\n—  pair of documents are near duplicate if and only if fingerprints are at most k-bits\napart\n— But in this case instead of using permutations and probability we use SimHash\n* Documents D, and D, are near duplicates iff\nHamming-Distance(Simhash( ,), Simhash(D,)) <=\n* Typically f= 64 and  =3\nCopyright 2011-2022 Ellis Horowitz 30\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "1. **** Ahash function: usually hashes different values to totally different hash values",
          "2. **** Simhash: a type of hashing where similar items are hashed to similar hash values (measured by bitwise Hamming distance)",
          "3. **** Bitwise Hamming distance: a measure of the similarity between two hash values"
        ],
        "definitions": [
          "1. **** Ahash function: a type of hash function that produces unique hash values for different inputs",
          "2. **** Simhash: a type of hashing that measures similarity between items using bitwise Hamming distance"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** Example 1: Three strings \"the cat sat on the mat\", \"the cat sat on mat\", and \"we all scream for ice cream\" are hashed using Ahash and Simhash functions",
          "2. **** Analysis of bitwise Hamming distance between similar items (p1,p2) and dissimilar items (p1,p3) and (p2,p3)"
        ],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "s30.png",
        "raw_text": "* Ahash function usually hashes different values to totally different hash values; here is an\nexample\npl ='the cat sat on the mat'\np2 = 'the cat sat on  mat’\np3 = 'we all scream for ice cream'\npl.hash => 415542861\np2.hash => 668720516\np3.hash => 767429688\n+ Simhash is one where similar items are hashed to similar hash values\n(by similar we mean the bitwise Hamming distance between hash values is small)\npl.simhash => 851459198\np2.simhash => 847263864\np3.simhash => 984968088\n+ in this case we can see the hamming distance of the similar items (p1,p2)=4 is small\nwhereas (p1,p3)=16 and (p2,p3)=12 are considerably larger\nCopyright 2011-2022 Ellis Horowitz 31\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "1. **Simhash**: A technique for calculating a similarity score between two phrases",
          "2. The concept of simhash is useful for determining the similarity between two phrases"
        ],
        "definitions": [
          "1. **Shingles**: Breaking down an input phrase into smaller sub-phrases (features)",
          "2. **Hash function**: A mathematical function that takes a feature as input and produces a fixed-size hash value",
          "3. **Bitwise Hamming distance**: The number of positions at which two binary strings differ"
        ],
        "formulas": [
          "1. V[i] = 1 if bit i of hash is set, otherwise V[i] = -1",
          "2. simhash bit i is 1 if V[i] > 0, otherwise it is 0"
        ],
        "algorithms": [
          "* Pick a hash size (e.g., 32 bits)",
          "* Initialize an array V of length hash size with zeros",
          "* Break down the input phrase into shingles",
          "* Hash each feature using a normal 32-bit hash algorithm (e.g., MD5 or SHA)",
          "* For each hash, update V[i] accordingly",
          "* If bit i of hash is set, add 1 to V[i], otherwise subtract 1 from V[i]"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "s31.png",
        "raw_text": "¢ The simhash of  phrase is calculated as follows:\n1. pick  hashsize, lets say 32 bits\n2. let  = [0] * 32 # (ie  vector of 32 zeros)\n3. break the input phrase up into shingles, e.g.\n‘the cat sat on  mat'.shingles(2) =>\n#<Set: {\"th\" \"he\" Ne moo ce\" \"eq\" Nat\" \" moot s\" \"Sa\" “at” oe ow o\"\n\"on\", \" \" \" a\", “ “sy wim\", *ma”, “eapry> , ’ , 3 ° ,\n4. hash each feature using  normal 32-bit hash algorithm (MD5 or SHA)\n\"th\" hash = -502157718 \"he\" hash = -369049682 ...\n5. for each hash\nif bit; of hash is set then add 1 to V[i]\nif bit; of hash is not set then subtract 1 from V{[i]\n6. simhash bit; is 1 if V[i] > 0 and 0 otherwise\n¢ Simhash is useful because if the Simhash bitwise Hamming distance of two\nphrases is low then their Jaccard coefficient is high\nCopyright 2011-2022 Ellis Horowitz 32\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "Deduplication: process of identifying and removing duplicate data or items."
        ],
        "definitions": [
          "8-bit hash values: a way to represent text as a binary number using 8 bits (0s and 1s).",
          "Fingerprint formed from 8-bit hash values: a unique representation of the original text.",
          "Weights: frequencies or importance of each word in the original text."
        ],
        "formulas": [
          "Replace each 0 by -1; multiply each 1 or -1 by weight (freq), sum these for each column:"
        ],
        "algorithms": [
          "Vector formed by summing weights:"
        ],
        "examples": [
          "Tropical fish text example, illustrating how to create a fingerprint from 8-bit hash values:"
        ],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "s31b.png",
        "raw_text": "Original text\nTropical fish include fish found in tropical environments around the world,\nincluding both freshwater and salt water species.\nWords with weights\ntropical 2 fish 2 include 1 found 1 environments 1 around 1 world 1\nincluding 1 both 1 freshwater 1 salt 1 water 1 species 1\n8 bit hash values\ntropical 01100001 fish 10101011 include 11100110\nfound 00011110 environments 09101101 around 10001011\nworld 00101010 including 11000000 both 10101110\nfreshwater OO1111110 salt 10110101 water 00100101\nspecies 11101110\nVector  formed by summing weights 8-bit fingerprint formed from\n1159-93133 10101111\n> eee\n-1*2 4-142 4-1414-14-14-14-14-1414+14+-14-1\n[replace each 0 by -1; multiply each 1 or -1 by weight (freq), sum these for each column]"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "**Deduplication**: The process of removing duplicate values from a list by sorting."
        ],
        "definitions": [
          "**Bitwise Hamming Distance (hdist)**: A measure of the difference between two numbers represented in binary, calculated as the number of positions at which the corresponding bits are different."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**Sorting and Deduplication Example**: The given example illustrates how sorting a list of numbers can lead to adjacent pairs with low bitwise Hamming distance. Specifically:"
        ],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "s32.png",
        "raw_text": "* In the case that two numbers have  low bitwise\nHamming distance and the difference in their bits\nare in the lower order bits then it turns out that\nthey will end up close to each other if the list is\nsorted.\n* consider the eight numbers and their bit representations if we sort them\n* 1 37586 1001001011010010 4 934 0000001110100110\n* 2 50086 1100001110100110 7 <-(this column lists hammin, | 3 2648 0000101001011000 9\n* 3 2648 0000101001011000 11 distance to previous entry) 6 2650 0000101001011010 1\n* 5 40957 10011111111111019 | 8 40955 1001111111111011 6\n* 6 2650 0000101001011010 9 5 40957 1001111111111101 2\nnotice that two pairs with very smallest hamming distance\nhdist(3,6)=1 and hdist(8,5)=2 have ended up adjacent to each other.\nCopyright 2011-2022 Ellis Horowitz 33\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "*  **Deduplication**: reducing runtime by checking adjacent pairs of a list instead of all combinations",
          "*  **Adjacent pair approach**: improves efficiency by focusing on nearby elements"
        ],
        "definitions": [
          "*  **Hamming distance**: the number of positions at which two strings are different (implied by \"low Hamming distance\")"
        ],
        "formulas": [
          "*  **O(n*(n-1)/2)**: original runtime calculation for checking all combinations",
          "*  **O(log n) + O(n) + O(log n)**: improved runtime calculation using adjacent pair approach"
        ],
        "algorithms": [
          "*  **Adjacent Pair Approach**:"
        ],
        "examples": [
          "*  **Low Hamming distance issue**: a pair with low Hamming distance that ends up apart due to sorting only picking up differences in lower order bits"
        ],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "s33.png",
        "raw_text": "¢ Rather than check every combo we could just check the adjacent\npairs of the list, each is  good candidate.\n¢ This reduces the runtime\nfrom n*(n-1)/2 coefficient calculations, O(n7) to\n—  fingerprints calculations O(n) +\n— asort O( log n) +\n—  coefficient calculations O(n),\n* which is O( log n) overall;\n*«  problem:\n— there is another pair with  low Hamming distance, /dist(4,2)=2 that\nhave ended up totally apart at other ends of the list...\n— sorting only picked up the pairs that differed in their lower order bits.\nCopyright 2011-2022 Ellis Horowitz 34\nFT"
      },
      {
        "lecture": "deduplication",
        "concepts": [
          "* Bitwise Hamming distance preserves its value under permutation of bits",
          "* Sorting by fingerprint can be used to identify pairs of identical elements"
        ],
        "definitions": [
          "* Bitwise Hamming distance: a measure of the number of different bits between two numbers",
          "* Permutation: rearrangement of bits or elements in a specific order",
          "* Rotating bits: shifting bits left and replacing lowest order bit with highest order bit"
        ],
        "formulas": [],
        "algorithms": [
          "* Rotate bits (bit shift left and replace lowest order bit with highest order bit) to create new fingerprints",
          "* Sort by fingerprint to identify pairs of identical elements"
        ],
        "examples": [
          "+ Hamming distance: same value under rotation and sorting",
          "+ Hamming distance: same value under rotation and sorting",
          "- (2,4), (3,6), (5,8)"
        ],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "s34.png",
        "raw_text": "¢ To get around this consider another convenient property of bitwise\nHamming distance,  permutation of the bits of two numbers preserves\nHamming distance\n¢ If we permute by 'rotating' the bits, i.e. bit shift left and replace lowest\norder bit with the 'lost' highest order bit we get 'new' fingerprints that\nhave the same Hamming distances\n‘rotate’ bits left twice if we sort again by fingerprint\n4 3736 0000111010011000 4 3736 0000111010011000\n3 10592 0010100101100000 9 2 3739 0000111010011011 2\n6 10600 0010100101101000 1 3 10592 0010100101100000 11\n1 19274 01001011010010105 | 6 10600 0010100101101000 1\n8 32750 0111111111101110 6 119274 0100101101001010 5\n5 32758 0111111111110110 2 8 32750 0111111111101110 6\n2.3739 0000111010011011 9 | 5 32758 0111111111110110 2\n7 61295 1110111101101111 9 7 61295 1110111101101111 6\nthis time the (2,4) pair ended up adjacent\nwe also identified the (3,6) and (5,8) pairs as candidates again\nCopyright 2011-2022 Ellis Horowitz 35\nFT"
      }
    ],
    "info_retrieval": [
      {
        "lecture": "info_retrieval",
        "concepts": [
          "1. **** Information Retrieval (IR) deals with indexing textual documents and retrieving relevant documents given a query.",
          "2. **** Searching for pages on the World Wide Web has become a primary application of IR."
        ],
        "definitions": [
          "1. **** Corpus: A set of documents to be indexed.",
          "2. **** Indexing: The process of creating an organized system of information from a corpus."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** None explicitly mentioned, but the content references a video by Jurafsky and Manning, which may provide examples or case studies."
        ],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "s1.png",
        "raw_text": "¢\ndecades\n— Traditionally it deals with the indexing of  given set of textual\ndocuments and the retrieval of relevant documents given  query\n¢ Searching for pages on the World Wide Web has become the “killer app.”\n* There has been  great deal of research on\n— How to index  set of documents ( corpus)\n— How to efficiently retrieve relevant documents\n* Jurafsky and Manning have an excellent video introducing the subject of\n* — http://\n[alt. link: use the ‘quick intro' YouTube one below!]\nCopyright Ellis Horowitz, 2011-2022 2\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "1. **KEY CONCEPTS** : Information Retrieval"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "5. **EXAMPLES** : The list (1. Docl, 2. Doc2, 3. Doc3) could be an example of documents being retrieved, with \"Doc\" possibly representing a document ID."
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "s2.png",
        "raw_text": "1. Docl\n2. Doc2\n3. Doc3\nCopyright Ellis Horowitz 2011-2022\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "+ Boolean model of retrieval",
          "+ Vector-space model of retrieval",
          "+ Prof. Salton and his students' research at Cornell University"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "s3.png",
        "raw_text": "* 1960-70 s:\n— Initial exploration of text retrieval systems for “small”\ncorpora of scientific abstracts, and law and business\ndocuments.\n— Development of the basic Boolean and vector-space models\nof retrieval.\n— Prof. Salton and his students at Cornell University were the\nleading researchers in the area.\nCopyright Ellis Horowitz, 2011-2022 4\nre"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* **Creation of large document database systems**"
        ],
        "definitions": [
          "- Publishes legal, tax and regulatory information for legal, corporate, government and academic markets",
          "- Contains data from more than 1.4 billion unique records of key information",
          "- National Library of Medicine health information database"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "- **Lexis-Nexis**: http://www.lexisnexis.com/  (MEDIUM [PRIORITY])",
          "- **Dialog**: http://www.dialog.com/  (LOW [PRIORITY])",
          "- **MEDLINE**: http://www.medlineplus.gov/  (MEDIUM [PRIORITY])"
        ],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "s4.png",
        "raw_text": "* 1980 s:\n— Creation of large document database systems, many run\nby companies:\n* Lexis-Nexis, http://www.lexisnexis.com/\n— information to legal, corporate, government and academic markets,\nand publishes legal, tax and regulatory information\n* Dialog, http://www.dialog.com/\n— data from more than 1.4 billion unique records of key information.\n* MEDLINE, http://www.medlineplus.gov/\n— National Library of Medicine health information\nCopyright Ellis Horowitz, 2011-2022 5\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Searching FTP'able documents on the Internet",
          "* Archie",
          "* WAIS",
          "* Lycos",
          "* Yahoo",
          "* Altavista"
        ],
        "definitions": [
          "* FTP'able documents: Documents that can be accessed via File Transfer Protocol (FTP)",
          "* Archie: A search engine developed in 1990 to index and search FTP sites",
          "* WAIS: An information retrieval system that uses a database of articles and documents"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "s5.png",
        "raw_text": "* 1990 s:\n— Searching FTP’ able documents on the Internet\n* Archie\n* WAIS\n— After the World Wide Web is invented, search engines\nappear\n« Lycos\n* Yahoo\n* Altavista\nCopyright Ellis Horowitz, 2011-2022 6"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Recommender Systems: computer programs that attempt to predict items users may be interested in, given some information about their profile.",
          "*  Automated Text Categorization & Clustering Systems: useful for grouping news articles."
        ],
        "definitions": [
          "*  TREC (Text REtrieval Conferences): a series of organized competitions sponsored by NIST to evaluate IR systems.",
          "*  Collaborative filtering algorithm: an approach used in recommender systems to predict user preferences based on the behavior of similar users."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  YouTube's recommendation system",
          "*  Amazon's recommendation system"
        ],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "s6.png",
        "raw_text": "* 1990  continued:\n— Organized Competitions\n¢ NIST TREC (Text REtrieval Conferences, http://trec.nist.gov/)\n* Sponsored by National Institute of Standards and Technology, NIST\n— Several New Types of IR Systems are Developed\n1. Recommender Systems: computer programs which attempt to predict items\n(movies, music, books, news, web pages) that  user may be interested in,\ngiven some information about the user' profile.\n— Often implemented as  collaborative filtering algorithm, examples include:\n» YouTube, perhaps the largest scale such system in existence\n» https://static.googleusercontent.com/media/research.google.com/en//pubs/are\nhive/45530.pdf\n» Amazon’  recommendation system, see\nhttps://stackoverflow.com/questions/2323768/how-does-the-amazon-\nrecommendation-feature-work\n» http://rejoiner.com/resources/amazon-recommendations-secret-selling-\nonline/\n2. Automated Text Categorization & Clustering Systems\n— Useful for grouping news articles\nCopyright Ellis Horowitz, 2011-2022 7\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Link analysis for Web Search",
          "*  Extension to retrieval of multimedia (images, music, video)",
          "*  Question Answering systems that return an actual answer rather than a ranked list of documents"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  TREC (Text REtrieval Conference) has had a Question/Answer track since 1999 (http://trec.nist.gov/data/qa.html)"
        ],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "s7.png",
        "raw_text": "« 2000\n— Link analysis for Web Search\n* Google started this\n— Extension to retrieval of multimedia: images, music, video\n* It is much harder to index multimedia artifacts\n— Question Answering\n* Question answering systems return an actual answer rather than\nranked list of documents\n* Since 1999 TREC has had  Question/Answer track, see\nhttp://trec.nist.gov/data/qa.html\nCopyright Ellis Horowitz, 2011-2022 8\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Information retrieval (IR) - process of searching for and retrieving relevant information from a large collection",
          "*  Data analysis - process of extracting insights and patterns from data",
          "*  Machine learning models - algorithms that enable systems to learn from data without being explicitly programmed"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "s8.png",
        "raw_text": "* Database Management\n¢ Library and Information Science\n¢ Artificial Intelligence\n¢ Natural Language Processing\n¢ Machine Learning\n* Data Science\nCopyright Elis Horowitz, 2011-2022 9"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Focused on structured data stored in relational tables rather than free-form text",
          "* Efficient processing of well-defined queries in formal language (SQL)",
          "* Clearer semantics for both data and queries"
        ],
        "definitions": [
          "* Document Object Model (DOM): provides some clues about web page structure"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Web pages are mostly unstructured",
          "* The DOM can provide some clues about web page structure"
        ],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "s9.png",
        "raw_text": "* Focused on structured data stored in relational\ntables rather than free-form text\n* Focused on efficient processing of well-defined\nqueries in  formal language (SQL)\n¢ Clearer semantics for both data and queries\n* Web pages are mostly unstructured, though the\nDocument Object Model (DOM) can provide\nsome clues"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "+ Human user aspects of interaction ()",
          "+ User interface ()",
          "+ Visualization ()",
          "+ Effective categorization of human knowledge ()",
          "+ Citation analysis ()",
          "+ Bibliometrics ()",
          "+ Digital libraries ()"
        ],
        "definitions": [
          "-  Human user aspects: The focus on how humans interact with information systems.",
          "-  User interface: The means by which users interact with a system or application.",
          "-  Visualization: The use of visual elements to represent and communicate information."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "s10.png",
        "raw_text": "* Focused on the human user aspects of\ninteraction, user interface, visualization).\n* Concerned with effective categorization of\nhuman knowledge.\n* Concerned with citation analysis and\nbibliometrics (structure of information).\n* Recent work on digital libraries brings it closer\nto Computer Science & IR.\nCopyright Ellis Horowitz, 2011-2022 ul"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Representation of knowledge, reasoning, and intelligent action",
          "*  Formalisms for representing knowledge and queries",
          "*  Integration with web ontologies and intelligent information agents"
        ],
        "definitions": [
          "*  **First-order Predicate Logic**: a formal system that uses quantified variables over a specified domain of discourse",
          "*  **Bayesian Networks**: directed acyclic graph model that represents a set of random variables and their dependencies",
          "*  **Web Ontology Language (OWL)**: a family of knowledge representation languages for authoring ontologies"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Bayesian Network that represents the probabilistic relationships between diseases and symptoms"
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "s11.png",
        "raw_text": "* Focused on the representation of knowledge, reasoning, and\nintelligent action.\n* Formalisms for representing knowledge and queries:\n— First-order Predicate Logic —  formal system that uses quantified\nvariables over  specified domain of discourse\n— Bayesian Networks —  directed acyclic graph model that represents\nset of random variables and their dependencies\n¢ E.g.  Bayesian Network that represents the probabilistic\nrelationships between diseases and symptoms\n* Recent work on web ontologies and intelligent information agents\nbrings it closer to IR\n— Web Ontology Language OWL is  family of knowledge\nrepresentation languages for authoring ontologies\n— See https://www.w3.org/OWL/\nCopyright Ellis Horowitz, 2011-2022 12\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  **Syntactic analysis**: focuses on syntax (phrase structure) in Natural Language Processing",
          "*  **Semantic analysis**: focuses on semantics to retrieve meaning based on natural language text",
          "*  **Pragmatic analysis**: analyzes the relationship between language and context",
          "*  **Extractive Summarization**: a method of summarizing text by extracting key information",
          "*  **Abstractive Text Summarization**: a method of summarizing text by generating new content"
        ],
        "definitions": [
          "*  **Entity Recognition**: the process of identifying specific entities in natural language text (e.g. people, places, organizations)",
          "*  **Natural Language Processing (NLP)**: a field of study that focuses on the interaction between computers and human language",
          "*  **Natural Language Generation (NLG)**: the process of generating human-like language from a computer system"
        ],
        "formulas": [],
        "algorithms": [
          "*  **Statistical models**: used to infer meaning from large amounts of data",
          "*  **Machine learning models**: used to infer meaning from large amounts of data"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "s12.png",
        "raw_text": "* Focused on the\nsyntactic, semantic, and Natural Language Understanding\npragmatic analysis of Extractive Summarisation\nnatural language text\nand discourse.\n+ Ability to analyze .\nsyntax (phrase NLP Natural Language Processing\nstructure) and Entity Recognition\nsemantics allows\nretrieval based on\nmeaning rather than Natural Language Generation\nkeywords Abstractive Text Summarization\n¢ NLP now uses vast\namounts of web pages\nas data to run statistical\nand machine learning\nmodels to infer meaning\nCopyright Ellis Horowitz, 2011-2022 13\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "1. **Machine Learning**  - a branch of Artificial Intelligence that allows computers to evolve their behavior based on empirical data",
          "2. Machine learning is focused on developing computational systems that improve their performance with experience"
        ],
        "definitions": [
          "1. **Supervised Learning**  - automated classification of examples based on learning concepts from labeled training examples",
          "2. **Unsupervised Learning**  - automated methods for clustering unlabeled examples into meaningful groups",
          "3. **Data Mining**  - the discovery of previously unknown properties of given data"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "s13.png",
        "raw_text": "*  branch of Artificial Intelligence concerned with algorithms that\nallow computers to evolve their behavior based on empirical data\n* Focused on the development of computational systems that\nimprove their performance with experience\n* Two major subtypes of machine learning are:\n— Automated classification of examples based on learning concepts\nfrom labeled training examples (supervised learning).\n— Automated methods for clustering unlabeled examples into\nmeaningful groups (unsupervised learning)\n* Machine learning is distinct from data mining, which focuses on\nthe discovery of previously unknown properties of the given data\n— Data mining is akin to query analysis and ranking\nCopyright Ellis Horowitz, 2011-2022 14\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "1. **** Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from various types of data.",
          "2. **** Data science is related to data mining, machine learning, and big data."
        ],
        "definitions": [
          "1. **** A data scientist is someone who knows how to extract meaning from and interpret data.",
          "2. **** The role of a data scientist requires both tools and methods from statistics and machine learning, as well as human review."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** Watch \"The Beauty of Data Visualization\" (18 min) to learn about data visualization.",
          "2. **** Data scientists spend a lot of time collecting and cleaning data, as data is never clean."
        ],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "s14.png",
        "raw_text": "¢ “Data science is an inter-disciplinary field that uses scientific methods,\nprocesses, algorithms and systems to extract knowledge and insights from\nmany structural and unstructured data. Data science is related to data mining,\nmachine learning and big data.” wikipedia\n*  data scientist is someone who knows how to extract meaning from and\ninterpret data, which requires both tools and methods from statistics and\nmachine learning, as well as human review. They spend. lot of time in the\nprocess of collecting and cleaning data, because data is never clean\n* For fun watch The Beauty of Data Visualization\n¢ https://www.youtube.com/watch?v=5Zg-C8AAIGg (18 min)\nCopyright Ellis Horowitz, 2011-2022 15\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "**Notion of Relevance**: Simplest notion of relevance is that the query string appears verbatim in the document."
        ],
        "definitions": [
          "None explicitly stated"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "s15.png",
        "raw_text": "¢ Simplest notion of relevance is that the query string appears\nverbatim in the document.\n— Slightly less strict notion is that the words in the query appear\nfrequently in the document, in any order (this is like viewing the\ndocument as  bag of words).\n* But that may not retrieve relevant documents that include\nsynonymous terms.\n— “restaurant” vs. “café”\n— “PRC” vs. “China”\n* And it may retrieve irrelevant documents that include ambiguous\nterms.\n— “bat” (baseball vs. mammal)\n— “Apple” (company vs. fruit)\n— “bit” (unit of data vs. act of eating)\nCopyright Ellis Horowitz, 2011-2022 16\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Goes beyond using just keyword matching",
          "* Takes into account the meaning of the words used",
          "* Adapts to the user based on direct or indirect feedback"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "* Taking into account the order of words in the query"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "s16.png",
        "raw_text": "¢ Goes beyond using just keyword matching, instead\nit\n— Takes into account the meaning of the words used\n— Takes into account the order of words in the query\n— Adapts to the user based on direct or indirect feedback\n— Takes into account the authority of the source\nCopyright Ellis Horowitz, 2011-2022 17\nrr"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Indexing: The process of creating an index from a text database to enable fast search operations.",
          "*  Search Initiation: Queries are used to initiate a search on the indexed documents."
        ],
        "definitions": [
          "*  Text Database: A collection of unstructured text data that is used as input for information retrieval systems.",
          "*  User Interface: The part of the system that allows users to interact with the search engine, submit queries, and view results."
        ],
        "formulas": [],
        "algorithms": [
          "*  Search Process:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "s17.png",
        "raw_text": "Logical View\n- User needs are part of the\n- User feedback is provided\n- Queries initiate  search 0\nthe index and docs are retrie\n- Aranking furstion orders th\nresults\nnverted\nfile\nStart with  text database; it is indexed;  user interface permits query operations which cause\nsearch on the Index; matched documents are retrieved and ranked\nCopyright Ellis Horowitz 2011-2022\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Parsing forms index words (tokens)",
          "* Stopword removal",
          "* Stemming"
        ],
        "definitions": [
          "* Inverted index",
          "* Query token",
          "* Relevance metric"
        ],
        "formulas": [],
        "algorithms": [
          "2. Stemming"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "s18.png",
        "raw_text": "¢ Parsing forms index words (tokens) and includes:\n— Stopword removal\n* See http://www.ranks.nl/stopwords for google stopwords\n— Stemming: reducing  word to its root\n* More about this later\n¢ Indexing constructs an inverted index of word to\ndocument pointers.\n* Searching retrieves documents that contain  given query\ntoken from the inverted index.\n¢ Ranking scores all retrieved documents according to\nrelevance metric.\nCopyright Ellis Horowitz, 2011-2022 19"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Retrieval model specifies details of document representation, query representation, and retrieval function",
          "* Determines notion of relevance",
          "* Three major types of retrieval models: Boolean, Vector Space, and Probabilistic"
        ],
        "definitions": [
          "* Binary relevance: a binary (yes/no) notion of relevance",
          "* Continuous relevance: ranked retrieval, where documents are ranked in order of relevance",
          "* Set theoretic model: Boolean model (chapter 1 in Manning et al)",
          "* Statistical/algebraic model: Vector Space model (chapter 2 in Manning et al)",
          "* Chapter 11 in Manning et al refers to Probabilistic models"
        ],
        "formulas": [],
        "algorithms": [
          "Note that there is no explicit mention of \"algorithms\" or step-by-step processes, so I did not mark anything as . Similarly, there are no concrete examples or case studies mentioned, so I did not mark any content as [EXAMPLE]."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "s19.png",
        "raw_text": "¢  retrieval model specifies the details of:\n— Document representation\n— Query representation\n— Retrieval function\n* Determines  notion of relevance.\n* Notion of relevance can be binary or continuous (i.e. ranked\nretrieval)\n¢ Three major\n1. Boolean models (set theoretic) (chapter 1 in Manning et al)\n2. Vector space models (statistical/algebraic) (chapter 2 in Manning etal)\n3. Probabilistic models (chapter 11 in Manning et al)\nCopyright Ellis Horowitz, 2011-2022 21\nmmm"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "**Text Preprocessing**: The process of preparing text data for information retrieval.",
          "**Inverted Index**: A data structure that maps keywords to documents they appear in."
        ],
        "definitions": [
          "**Tokenization**: Breaking down text into individual words or tokens.",
          "**Stemming**: Reducing words to their root form (e.g. \"running\" becomes \"run\").",
          "**Stopwords**: Common words that do not carry much meaning, such as \"a\", \"the\", etc."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "s20.png",
        "raw_text": "1. Strip unwanted characters/markup (e.g. HTML tags,\npunctuation, page numbers, etc.).\n2. Break into tokens (keywords) separating out\nwhitespace.\n3. Stem tokens to “root” words\n4. Remove common stopwords (e.g. a, the, it, etc.).\n5. Detect common phrases (possibly using  domain\nspecific dictionary).\n6. Build inverted index (keyword > list of docs\ncontaining it).\nCopyright Ellis Horowitz, 2011-2022 22\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Document representation as a set of keywords",
          "*  Boolean queries as expressions of keywords connected by AND, OR, and NOT operators",
          "*  Scope indication using brackets"
        ],
        "definitions": [
          "*  Keyword: a word representing a document",
          "*  Query: a request for information, expressed as a Boolean expression of keywords",
          "*  AND operator: combines two or more keywords to search for documents containing all specified keywords",
          "*  OR operator: combines two or more keywords to search for documents containing at least one of the specified keywords",
          "*  NOT operator: excludes specific keyword from search results"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Sample Boolean query with explicit AND, OR, NOT operators: [[Rio & Brazil] | [Hilo & Hawaii]] & hotel & !Hilton]",
          "*  Advanced Search example using AND, OR, and NOT operators in Google"
        ],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "s21.png",
        "raw_text": "«  document is represented as  set of keywords.\n* Queries are Boolean expressions of keywords, connected by AND,\nOR, and NOT, including the use of brackets to indicate scope\n* Here is  sample Boolean query with explicit AND, OR, NOT\noperators\n— [[Rio & Brazil] | [Hilo & Hawaii]] & hotel & !Hilton]\nGoogle Advanced es\nSearch; cede = oo\nNote inclusion of ‘Advanced Search\nAND, OR, NOT\noperators\nFind pages with. To do this in the search box.\nEe aatitnese words: | Type the important words: tri-colour rat terrier\nCopyright Ellis Horowitz, 2011-2022 23\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "Popular retrieval model",
          "Boolean models can be extended to include ranking",
          "Reasonably efficient implementations possible for normal queries"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "s22.png",
        "raw_text": "¢ Popular retrieval model because:\n— Easy to understand for simple queries.\n— Clean formalism.\n* Boolean models can be extended to include ranking\n* Reasonably efficient implementations possible for\nnormal queries.\nCopyright Elis Horowitz, 2011-2022 24"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Rigid Boolean query model",
          "*  Difficulty in expressing complex user requests"
        ],
        "definitions": [
          "*  AND: means all; OR: means any (in the context of Boolean queries)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "s23.png",
        "raw_text": "¢ Very rigid: AND means all; OR means any\n¢ Difficult to express complex user requests\n¢ Difficult to control the number of documents retrieved\n— All matched documents will be returned\n¢ Difficult to rank output\n— All matched documents logically satisfy the query\n¢ Difficult to perform relevance feedback\n— Ifa document is identified by the user as relevant or\nirrelevant, how should the query be modified?\nCopyright Ellis Horowitz, 2011-2022 25"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Information retrieval challenges: too many matches for simple queries (e.g., \"Lincoln\")",
          "* Importance of query refinement for accurate results"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Simple query: \"Lincoln\" (too many matches)",
          "* More detailed query: \"President AND Lincoln\" (returns incorrect results)",
          "* Even more detailed query: \"president AND Lincoln AND NOT (automobile OR car)\" (better, but still not ideal)",
          "* Successful query refinement: \"President AND lincoln AND (biography OR life OR birthplace OR gettysburg) AND NOT (automobile OR car)\""
        ],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "s24.png",
        "raw_text": "¢ The simple query “Lincoln”\n— Too many matches including Lincoln cars and places named Lincoln as\nwell as Abraham Lincoln\n* More detailed query “President AND Lincoln”\n— Returns documents that discuss the President of Ford Motor company\nthat makes the Lincoln car\n¢ Even more detailed query “president AND Lincoln AND NOT\n(automobile OR car)”\n— Better, but the use of NOT will remove  document about President\nLincoln that says “Lincoln’  body departs Washington in  nine car\nfuneral train”\n¢ Perhaps try\n— President AND lincoln AND biography AND life AND birthplace AND gettysburg AND\nNOT (automobile OR car), but too many ANDs can lead to nothing, so\n— President AND lincoln AND (biography OR life OR birthplace OR gettysburg) AND NOT\n(automobile OR car)\nCopyright Ellis Horowitz, 2011-2022 26\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  **Vector Space**: A set of \"orthogonal\" terms (index terms or vocabulary) that form a vector space.",
          "*  **Dimension**: The size of the vocabulary, equal to the number of index terms."
        ],
        "definitions": [
          "*  **Index Terms**: Distinct terms remaining after preprocessing.",
          "*  **Vocabulary**: Set of index terms.",
          "*  **Document Vector**: A vector representing a document in the vector space, with each element being the weight of an index term."
        ],
        "formulas": [
          "*  **Dv = (di1, d12, ..., dim)**: Document D represented as a vector of index terms, where di is the weight of the j-th term in the document."
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "s25.png",
        "raw_text": "¢ Assume ¢ distinct terms remain after preprocessing; call\nthem index terms or the vocabulary\n¢ These “orthogonal” terms form  vector space\nsize of the vocabulary = Dimension = ¢ = |vocabulary|\n¢«  document D; is represented by  vector of index terms\nDz = (dip diy -» Aid)\n* Where d, represents the weight of the j-th term in the  doc\n— but how is the weight computed?\n¢ Both documents and queries are expressed as\nt-dimensional vectors\nCopyright Ellis Horowitz, 2011-2022 27\nmmm"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "**Vocabulary**: consists of 3 terms (T)",
          "**Term weights**: coefficients",
          "**Documents**: D1 and D2",
          "**Query**: Q = 0T + 0T + 2T"
        ],
        "definitions": [
          "**Vocabulary**: a set of terms used to represent documents or queries",
          "**Term weights**: numerical values assigned to each term in the vocabulary",
          "**Document**: a unit of information represented as a set of weighted terms",
          "**Query**: a request for information, represented as a set of weighted terms"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**Term weights and document representation**: D1 = 2T + 3T + 5T; Q = 0T + 0T + 2T"
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "s26.png",
        "raw_text": "Example ° Vocabulary consists of 3 terms\n. T; with weights the coefficients\nD, = 2T, + 3T, + 5T; . There are two documents, D, and\nD,; there is one query,\nD,=3T,+7T,+ T; US\nQ=0T, + 0T,+ 2T NX\nD, = 2,437, + ST;\nQ=0T,+ 0T, + 2T,\nD,=3T,+7T,+T; Ko\n2 ! PNA fo eIs D, or D, more similar to Q?\n7  oo *How to measure the degree of\nT> ya ney similarity? Distance? Angle?\n< Projection?\nCopyright Ellis Horowitz, 2011-2022 28\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "s27.png",
        "raw_text": "¢  collection of  documents can be represented in the\nvector space model by  term-document matrix.\n¢ An entry in the matrix corresponds to the “weight” of\nterm in the document; zero means the term has no\nsignificance in the document or it simply doesn’ exist in\nthe document; but we still need  way to compute the\nweight\nT, Tn .... Ty\nDi Wy Wa. Wa\nD2 Wir Wa + Wx\nD, Win Wan ++ Win\nCopyright Ellis Horowitz, 2011-2022 29"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Term frequency in a document (more frequent terms are more important)",
          "* Normalizing term frequency across the entire corpus"
        ],
        "definitions": [
          "* tf: term frequency in a document (number of times a term appears in a document)"
        ],
        "formulas": [],
        "algorithms": [
          "* None explicitly mentioned, but the process of normalizing term frequency is implied"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "s28.png",
        "raw_text": "¢ One way to compute the weight is to use the term’\nfrequency in the document\n¢ Assumption: the more frequent terms in  document are\nmore important, i.e. more indicative of the topic.\nJi = frequency of term 7 in document\n¢ May want to normalize term frequency (tf) across the\nentire corpus:\nff, = /maxtfis\nCopyright Ellis Horowitz, 2011-2022 30"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "Terms that appear in many different documents are less indicative of overall topic."
        ],
        "definitions": [
          "*  df; = document frequency of term",
          "*  = number of documents containing term i",
          "*  N: total number of documents",
          "+  df; = document frequency of term",
          "+  = number of documents containing term i",
          "+  N: total number of documents"
        ],
        "formulas": [
          "idf, = log, (W/ df)",
          "+  idf, = log, (W/ df)"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "s29.png",
        "raw_text": "¢ Terms that appear in many different documents are less\nindicative of overall topic\ndf; = document frequency of term\n= number of documents containing termi\nof course df; is always <=  (total number of documents)\nidf, = inverse document frequency of term i,\n= log, (W/ df)\n(N: total number of documents)\n¢ An indication of  term’ discrimination power\n¢ Log is used to dampen the effect relative to tf\nCopyright Ellis Horowitz, 2011-2022"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Term frequency (df)",
          "* Inverse Document Frequency (idf)"
        ],
        "definitions": [
          "* df: document frequency, number of documents containing a term",
          "* idf: inverse document frequency, measures importance of a term in the collection",
          "Note that I didn't mark any of these examples as [EXAMPLE] since they are not concrete examples, but rather illustrative cases. Instead, I categorized them under ."
        ],
        "formulas": [
          "* idf = log(N/df) , where N is the total number of documents (1,000,000)"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "s30.png",
        "raw_text": "° term df; idf;\n* Calpurnia 1 log(1,000,000/1)=6\n* animal 100 4\n¢« Sunday 1,000 3\n* fly 10,000 2\n* under 100,000 1\n* the 1,000,000 0\n° idf, = log 1 (N/df), N= 1,000,000\n¢ there is one idf value for each term ¢in  collection"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Combined term importance indicator",
          "*  TF-IDF (Term Frequency-Inverse Document Frequency) weight",
          "*  Query-document scoring"
        ],
        "definitions": [
          "*  tf-idf: a method for calculating the importance of a term in a document",
          "*  N/df: the number of documents divided by the frequency of the term (denominator)",
          "*  IDF: Inverse Document Frequency, a measure of how rare a term is across all documents"
        ],
        "formulas": [
          "*  w = (1 + log(tf))*log(N/df)"
        ],
        "algorithms": [
          "*  Query-document scoring:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "s31.png",
        "raw_text": "*  typical combined term importance indicator\nis tf-idf weightin 1g ( note: it is often written with  hyphen, but the\nhyphen is NOT  minus sign; some people replace the hyphen with  dot):\n_ ; = *\nw= Uf, idf, = (1 + log tf, )* log, (N/ df)\n*  term occurring frequently in the document but rarely in the rest\nof the collection is given high weight.\n* Many other ways of determining term weights have been proposed.\n¢ Experimentally, ¢idf has been found to work well\n* Given  query q, then we score the query against  document\nusing the formula\nScore (q, d) =>\\( Uf-idf, q) where tis ing Vd\nCopyright Ellis Horowitz, 2011-2022 33\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* **Term Frequency (tf)**: represents the frequency of a term in a document",
          "* **Inverse Document Frequency (idf)**: measures the rarity of a term across the entire collection"
        ],
        "definitions": [
          "* Term Frequency (tf): ratio of the frequency of a term in a document to the total number of terms in the document",
          "* Inverse Document Frequency (idf): logarithm of the ratio of the total number of documents to the number of documents containing the term"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Calculations for A, B, and C terms with given frequencies and document frequencies"
        ],
        "priority": "MEDIUM",
        "slide_number": 32,
        "slide_file": "s32.png",
        "raw_text": "Given  document containing 3 terms with given\nfrequencies:\nA(3), B(2), C1)\nAssume collection contains 10,000 documents and\ndocument frequencies of these 3 terms are:\nA(50), B(1300), C(250)\nThen:\nA: tf=3/3; idf =1log(10000/50) = 5.3;  tf.idf = 5.3\nB: tf=2/3; idf =1og(10000/1300) = 2.0; tf.idf = 1.3\nC: tf=1/3; idf = 1og(10000/250) = 3.7; | tf.idf = 1.2\nCopyright Ellis Horowitz, 2011-2022 34"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Cosine of the angle between vectors is a similarity measure (not distance measure)"
        ],
        "definitions": [
          "* Distance between vectors d, and d, captured by cosine of the angle between them"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 33,
        "slide_file": "s33.png",
        "raw_text": "¢ Distance between vectors d, and d, is captured by\nthe cosine of the angle  between them.\n* Note — this is  similarity measure, not  distance\nmeasure\nts d,\nLo\nto\nCopyright Ellis Horowitz, 2011-2022 35"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Similarity measure"
        ],
        "definitions": [
          "* A similarity measure is a function that computes the degree of similarity between two vectors"
        ],
        "formulas": [
          "Note that there are no mathematical formulas or step-by-step algorithms in this content, so I did not mark anything as  or [ALGORITHM]. However, the properties and procedures mentioned may be useful to study for the midterm exam."
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 34,
        "slide_file": "s34.png",
        "raw_text": "¢  similarity measure is  function that computes\nthe degree of similarity between two vectors\n— Look back at the previous lecture slides for the\ndefinition of similarity\n¢ Using  similarity measure between the query\nand each document has positive aspects:\n— It is possible to rank the retrieved documents in the order\nof presumed relevance.\n— It is possible to enforce  certain threshold so that the\nsize of the retrieved set can be controlled,\nCopyright Ellis Horowitz, 2011-2022 36"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Normalizing vectors (  )",
          "* Unit circle (  )"
        ],
        "definitions": [],
        "formulas": [
          "* a|~ = √(a_1^2 + ... + a_n^2) (  ) - implied formula for calculating vector length",
          "* cos(θ)=d⋅d' / (√(d^2) \\* √(d'^2)) (  ) - explicit cosine formula"
        ],
        "algorithms": [
          "* Normalizing a vector by dividing each component by its length (  )"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 35,
        "slide_file": "s35.png",
        "raw_text": "¢  vector can be normalized (given  length of 1) by\ndividing each of its components by the vector'\nlength\n¢ This maps vectors onto the unit circle:\n* Then, a\\| =~ Vou Wj =1\n¢ Longer documents don’ get more weight\n¢ For normalized vectors, the cosine is simply the dot\nproduct:  if)\ncos( ,,d,)=d,-d,"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* **Similarity between vectors for the document d; and query q can be computed as the vector inner product** ()",
          "*  Similarity between vectors for documents and queries",
          "*  Vector inner product for computing similarity"
        ],
        "definitions": [
          "* For binary vectors, the inner product is the number of matched query terms in the document (size of intersection) (Hamming distance) ()",
          "* For weighted term vectors, it is the sum of the products of the weights of the matched terms. ()",
          "*  Binary vectors: vectors where each element is either 0 or 1 (representing matched or non-matched terms)",
          "*  Weighted term vectors: vectors where each element represents the weight of a term",
          "*  Hamming distance: the number of matched query terms in the document"
        ],
        "formulas": [
          "+  sim(d,q) = dsq = ∑ w_id \\* w_qi",
          "*  sim(d,q) = dsq = ∑ w_id \\* w_qi",
          "*  (for binary vectors) Hamming distance: size of intersection between query and document terms"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 36,
        "slide_file": "s36.png",
        "raw_text": "* Similarity between vectors for the document d; and query\ncan be computed as the vector inner product:\nsim(d,q) = dsq = yy ‘Wig\ni=]\nwhere w;, is the weight of term iin document  and w,,\nis the weight of term 7 in the query\n¢ For binary vectors, the inner product is the number of\nmatched query terms in the document (size of intersection)\n(Hamming distance)\n¢ For weighted term vectors, it is the sum of the products of\nthe weights of the matched terms.\nCopyright Ellis Horowitz, 2011-2022 38"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* **** Vector space model",
          "* **** Binary vector representation",
          "* **** Weighted vector representation"
        ],
        "definitions": [
          "* **** o: term in the document or query (denoted by 1 if present, 0 if not)",
          "* **** S: size of vocabulary (number of unique terms)"
        ],
        "formulas": [
          "* **** Size of vector = size of vocabulary = 7",
          "* **** similarity(D, Q) = inner product of D and Q vectors",
          "* **** sim(D, , Q) = weighted sum of term frequencies"
        ],
        "algorithms": [],
        "examples": [
          "* **** Binary vector representation: oS Ss ws (example of binary vector with 7 terms)",
          "* **** Weighted vector representations:",
          "* **** Calculating similarity using weighted vectors: sim(D, , Q) = 10 and sim(D, , Q) = 2"
        ],
        "priority": "MEDIUM",
        "slide_number": 37,
        "slide_file": "s37.png",
        "raw_text": ". oe oy\n4 . ce ooh oe\nBinary: oS Ss ws\n-D=-L1L1041 0 Size of vector = size of vocabulary = 7\n0 means corresponding term not found in\n~ Q=1, 0, 1,6, 0, 1, 1 document or query\nsimilarity(D, Q) =3 (the inner product)\nWeighted:\nD, =2T,+3T,+5T,; D,=3T,+7T,+ 1T,\nQ=0T, + 0T, + 2T;\nsim(D, , Q) = 2*0 + 3*0 + 5*2 = 10\nsim(D, , Q) = 3*0 + 7*0 + 1*2 = 2\nCopyright Ellis Horowitz, 2011-2022 40\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "**Cosine Similarity**: measures the cosine of the angle between two vectors",
          "**Vector Lengths**: normalization factor in Cosine Similarity calculation ( Dj = sqrt(Dwj^2) )",
          "**Inner Product**: a measure of similarity between two vectors"
        ],
        "definitions": [
          "None extracted, but note that \"Cosine Similarity\" and \"Vector Lengths\" are key concepts with implications for understanding the definition."
        ],
        "formulas": [
          "CosSim(d, q) = (d · q) / (√(d^2) * √(q^2))",
          "dj = sqrt(Dwj^2)",
          "CosSim(D1, Q) = 10 / V(4+9+25)(0+0+44) = 0.81",
          "CosSim(D2, Q) = 2 / V(9+4941)(040+44) = 0.13"
        ],
        "algorithms": [],
        "examples": [
          "D1 = [3T, 7T, 1T]; CosSim(D1, Q) = 2 / V(9+4941)(040+44) = 0.13",
          "D2 is 6 times better than D1 using cosine similarity but only 5 times better using inner product"
        ],
        "priority": "MEDIUM",
        "slide_number": 38,
        "slide_file": "s38.png",
        "raw_text": "* Cosine similarity measures the cosine of the angle t;\nbetween two vectors\n* We compute the inner product normalized by the 0,\nvector lengths ~\\\n. D,\ndj - Dwi Wig \\2\nCosSim(d,  = T= = oo 0,/\n2-4 Wa 2 Wig\nt, D,\nD, =2T, +3T,+5T; CosSim(D, , Q) = 10/ V(4+9+25)(0+0+44) = 0.81\nD, =3T,+7T,+1T; CosSim(D, , Q)= 2/ V(9+4941)(040+44) = 0.13\nQ=0T, + OT, + 2T;\nD, is 6 times better than D, using cosine similarity but only 5 times better using\ninner product.\nCopyright Ellis Horowitz, 2011-2022 41\nRe"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "**Vector Space Model**: a method for representing documents and queries as vectors in a high-dimensional space."
        ],
        "definitions": [
          "**idf (Inverse Document Frequency)**: a measure of how rare a word is in the document collection.",
          "**cosSim (Cosine Similarity)**: a measure of similarity between two vectors, used to compute the score of each document."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 39,
        "slide_file": "s39.png",
        "raw_text": "1. Convert all documents in collection  to ¢/idf weighted\nvectors, the j” document denoted by d,, for keywords in\nvocabulary\n2. Convert each query to  ¢f/idf weighted vector\n3. For each din  do\nCompute score s;= cosSim(d; q)\n4. Sort documents by decreasing score\n5. Present top ranked documents to the user\nTime complexity: O(/V|:|D|) Bad for large  & D!\n[V| = 10,000; |D| = 100,000; |V|-|D| = 1,000,000,000\nCopyright Ellis Horowitz, 2011-2022 42"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Ranking: computing the documents in the corpus \"nearest\" to the query"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  To do efficient ranking:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 40,
        "slide_file": "s40.png",
        "raw_text": "* Ranking consists of computing the & docs in the corpus\n“nearest” to the query =>  largest query-doc cosines.\n* To do efficient ranking one must:\n— Compute  single cosine efficiently.\n— Choose the  largest cosine values efficiently.\nCopyright Elis Horowitz, 2011-2022 8"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Representing query and documents as weighted vectors:"
        ],
        "definitions": [
          "* **Weighted vector**: A vector where each component represents the importance of a particular feature or term, often calculated using techniques like TF-IDF."
        ],
        "formulas": [],
        "algorithms": [
          "1. Compute cosine similarity score between query and document vectors that contain the query term:",
          "2. Rank documents by score:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 41,
        "slide_file": "s42.png",
        "raw_text": "* Represent the query as  weighted ¢f-idf vector\n* represent each document as  weighted ¢/-idf vector\n* compute the cosine similarity score for the query\nvector and each document vector that contains the\nquery term\n* Rank documents with respect to the query by score\n* Return the top  (e.g. k=10) to the user"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "Preprocessing",
          "Preferred list",
          "Cosine similarity"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "Note that I didn't include \"Search\" as an  since it's more of a process description rather than a step-by-step procedure. If you'd like to treat it as an algorithm, let me know!"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 42,
        "slide_file": "s43.png",
        "raw_text": "¢ Preprocess: Pre-compute, for each term, its\nnearest docs.\n— (Treat each term as  1-term query)\n— lots of preprocessing.\n— Result: “preferred list” for each term.\n¢ Search:\n— For a¢-term query, take the union of their ¢ preferred\nlists — call this set S.\n— Compute cosines from the query to only the docs in S,\nand choose top k.\nCopyright Ellis Horowitz, 2011-2022 48"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "*  Mathematically based approach",
          "*  Combination of local and global word occurrence frequencies"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  Partial matching and ranked results: This implies that the algorithm is capable of returning multiple results, ordered by their relevance or importance."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 43,
        "slide_file": "s44.png",
        "raw_text": "¢ Simple, mathematically based approach.\n* Considers both local (¢) and global (idf) word\noccurrence frequencies.\n* Provides partial matching and ranked results.\n* Tends to work quite well in practice despite\nobvious weaknesses.\n¢ Allows efficient implementation for large\ndocument collections."
      },
      {
        "lecture": "info_retrieval",
        "concepts": [
          "* Missing semantic information (e.g. word sense)",
          "* Missing syntactic information (e.g. phrase structure, word order, proximity information)"
        ],
        "definitions": [
          "* Term independence: the assumption that terms in a query are independent of each other"
        ],
        "formulas": [],
        "algorithms": [
          "* Boolean model limitations (e.g., requiring term to appear in document)"
        ],
        "examples": [
          "* Two-term query \"B\" may prefer document containing frequently but not B, over document that contains both and B, but both less frequently"
        ],
        "priority": "MEDIUM",
        "slide_number": 44,
        "slide_file": "s45.png",
        "raw_text": "¢ Missing semantic information (e.g. word sense).\n¢ Missing syntactic information (e.g. phrase structure, word\norder, proximity information).\n¢ Assumption of term independence\n¢ Lacks the control of  Boolean model (e.g., requiring\nterm to appear in  document).\n— Given  two-term query “ B”, may prefer  document\ncontaining  frequently but not B, over  document that\ncontains both  and B, but both less frequently.\nCopyright Ellis Horowitz, 2011-2022 50"
      },
      {
        "lecture": "info_retrieval",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 45,
        "slide_file": "UAC.png",
        "raw_text": "O:=-2é"
      }
    ],
    "inverted_indexing": [
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "*  Inverted index: a data structure used to improve search efficiency",
          "*  Linked Inverted Index: an extension of inverted indexing that links words with their occurrences",
          "*  Distributed Indexing: a technique for scaling inverted indexes across multiple machines"
        ],
        "definitions": [
          "*  Inverted index: an index data structure used to store the location(s) of each word in a document collection",
          "*  Linked Inverted Index: an extension of inverted indexing that stores pointers to linked words with their occurrences",
          "*  Skip Pointers: pointers that allow for efficient merging of sorted lists"
        ],
        "formulas": [],
        "algorithms": [
          "*  Processing Query on Linked Inverted Index: an algorithmic process to retrieve relevant documents from a linked inverted index",
          "*  Distributed Indexing: a distributed processing algorithm for scaling inverted indexes across multiple machines"
        ],
        "examples": [
          "*  Examples of Inverted Indices: real-world applications or scenarios where inverted indexing is used",
          "*  biwords: an example of using n-grams to improve search results"
        ],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "s1.png",
        "raw_text": "* Definition of an Inverted index\n¢ Examples of Inverted Indices\n* Representing an Inverted Index\n* Processing  Query on  Linked Inverted Index\n¢ Skip Pointers to Improve Merging\n¢ Phrase Queries\n* biwords\n* Grammatical Tagging\n« N-Grams\n¢ Distributed Indexing\nCopyright Ellis Horowitz, 2011-2013 2"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted index is a data structure composed of a vocabulary (vector) containing distinct words in lexicographical order",
          "* Vocabulary includes lists of documents and text positions where each word occurs"
        ],
        "definitions": [
          "* Case folding: converting all uppercase letters to lowercase",
          "* Stemming: reducing words to their morphological roots",
          "* Stop words: words that are so common they provide no information"
        ],
        "formulas": [
          "Note that there are no mathematical formulas or algorithms in the provided content, so I couldn't mark any as  or [ALGORITHM]. The priority ratings are subjective and based on the assumption that a midterm exam would focus on understanding how an inverted index is created and used."
        ],
        "algorithms": [
          "+ For each word, listing all documents and text positions where it occurs"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "s2.png",
        "raw_text": "« An inverted index is typically composed of  vector containing\nall distinct words of the text collection in lexicographical order\n(which is called the vocabulary) and for each word in the\nvocabulary,  list of all documents (and text positions) in which\nthat word occurs\n— This is nothing more than an index that one finds at the back of  book\n¢ Terms in the inverted file index are refined:\n— Case folding: converting all uppercase letters to lower case\n— Stemming: reducing words to their morphological roots\n— Stop words: removing words that are so common they provide no\ninformation\nCopyright Ellis Horowitz, 2011-2013 3"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1. **** Inverted Indexing: A data structure used for efficient text search.",
          "2. **** Dictionary (Index File): The part of an inverted index that stores terms in alphabetical order with pointers to their postings lists.",
          "3. **** Postings List: The part of an inverted index that stores a list of document IDs where a term appears."
        ],
        "definitions": [
          "1. **** Term Frequency: Not explicitly defined, but implied as the frequency of a term in a particular document (e.g., \"ae\" has a certain frequency in the system 1 document).",
          "2. **** Document Frequency (df): The number of documents that contain a particular term.",
          "3. **** Inverted Index: A data structure that stores terms and their corresponding postings lists."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** System 1 document with term \"ae\" and its frequency.",
          "2. **** Postings list for the term \"system 1\", which would contain a list of document IDs where this term appears."
        ],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "s3.png",
        "raw_text": "j-th document, term frequency\nDocument frequency\nIndex terms df\nsystem 1\nae\nIndex file Postings lists\nThe two parts of an inverted index. The dictionary (Index file) is usually kept in memory, with\npointers to each postings list , which is stored on disk: The dictionary has been sorted alphabetically\nand the postings list is sorted by document ID\nCopyright Ellis Horowitz 2011-2012 4\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted indexing",
          "* List of words by position",
          "* List of positions by word"
        ],
        "definitions": [
          "* *Inverted file*: A list of positions by word",
          "* Each entry in the inverted file represents a word and its corresponding positions"
        ],
        "formulas": [],
        "algorithms": [
          "* Creating an inverted index from a file involves creating a list of words by position, where each entry is a word and its position in the original file",
          "* To create an inverted file, create a list of positions by word, where each entry is a word and its corresponding positions in the original file",
          "* [PRIORITY] MEDIUM: Creating an inverted index from a file and creating an inverted file"
        ],
        "examples": [
          "- Last entry: \"°\" at last word"
        ],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "s4.png",
        "raw_text": "\\ ° . ege\n1  file is  list of words by position\nto First entry is the word in position 1 (first word)\n22 Entry A562 is th¢ word in position 4562 (4562\"! word)\n* Last ¢ntry is the last word\n% An ifverted fle is_a list of positions by word! >\nbee  5 Sy : —\na(1, 4, 40) So\nentry (11, 20, 3 OE  )\nlist (5, 41) oe “3 = = =\nposition (9, 76, 26) HN  ) 5  “eS\npositions (44) Lf} | . : -\nword (14,9, 24, 29, 35, 45) yy pV |\nwords (74 oo\n4562 (21, 27)\nCopyright Ellis Horowitz, 2011-2013 5 5"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "s5.png",
        "raw_text": "* The Query\n— Which plays of Shakespeare contain the words Brutus AND Caesar but NOT\nCalpurnia?\n* One Possible Solution\n— One could grep all of Shakespeare’ plays for Brutus and Caesar, then strip out\nlines containing Calpurnia?\n* Too Slow (for large corpora)\n* Requires lots of space\n¢ This method doesn’ allow for other operations (e.g., find the word Romans\nnear countrymen) are not feasible with this approach\nCopyright Ellis Horowitz, 2011-2013 6\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1. ** Inverted Index**: Considered as a sparse matrix where rows represent terms and columns represent documents.",
          "2. ** Sparse Matrix**: Used to represent an inverted index, where most entries are zero."
        ],
        "definitions": [
          "1. ** Term**: A single word or phrase in a document (e.g., \"Antony\", \"Brutus\", etc.)",
          "2. ** Document**: A unit of text that is indexed by the inverted index (e.g., \"Antony and Cleopatra\", \"Julius Caesar\", etc.)"
        ],
        "formulas": [],
        "algorithms": [
          "1. **** The process of creating an inverted index can be represented as a sparse matrix, where each row corresponds to a term and each column corresponds to a document."
        ],
        "examples": [
          "1. **** The slide provides a concrete example of an inverted index represented as a sparse matrix:"
        ],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "s6.png",
        "raw_text": "One way to think about an inverted index is to consider it as  sparse matrix where rows\nRepresent terms and columns represent documents\ndocuments ————> Antony and Cleopatra Julius Caesar The Tempest Hamlet Othello Macbeth\nAntony 1 1 0 0 0 1\nBrutus 1 1 0 1 0 0\nCaesar 1 1 0 1 1 1\nCalpurnia 0 1 0 0 0 0\nCleopatra 1 0 0 0 0 0\nmercy 1 0 1 1 1 1\nterms worser 1 0 1 1 1 0\nBrutus AND Caesar but NOT Calpurnia\nword, 0 otherwise\nCopyright Ellis Horowitz, 2011-2013 7\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted indexing ()",
          "* Bitwise AND operation in inverted indexing ()"
        ],
        "definitions": [
          "+ Complemented vector (implied as \"complemented\" in the text) (): a vector that has been modified to represent the absence of certain terms."
        ],
        "formulas": [
          "* 110100 AND 110111 AND 101111 = 100100 ()"
        ],
        "algorithms": [
          "1. Take the vectors for the relevant documents (e.g., Brutus, Caesar, and Calpurnia) ()",
          "2. Complement the vectors (implied as necessary for the AND operation) ()",
          "3. Perform bitwise AND on the complemented vectors to retrieve matching documents ()"
        ],
        "examples": [
          "* Querying multiple documents simultaneously using bitwise AND ()",
          "+ Example: 110100, 110111, and 101111 (vectors for Brutus, Caesar, and Calpurnia) ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "s7.png",
        "raw_text": "* So we have  0/1 vector for each term.\n* To answer query: take the vectors for Brutus, Caesar\nand Calpurnia (complemented) and do. bitwise AND.\n* 110100 AND 110111 AND 101111 = 100100\n* So the two plays matching the query are: Anthony and\nCleopatra, Hamlet"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "*  Indexing: The process of creating a data structure that allows for efficient retrieval of specific information from a large dataset."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Antony and Cleopatra, Act Ill, Scene ii: This example illustrates a situation where Agrippa observes Antony's emotional response to the death of Julius Caesar.",
          "*  Hamlet, Act Ill, Scene ii: This example shows Lord Polonius misinterpreting Brutus' actions in relation to the death of Julius Caesar."
        ],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "s8.png",
        "raw_text": "¢ Antony and Cleopatra, Act Ill, Scene ii\n* Agrippa [Aside to DOMITIUS ENOBARBUS]:\n° Why, Enobarbus,\n. When Antony found Julius Caesar dead,\n. He cried almost to roaring; and he wept\n. When at Philippi he found Brutus slain.\n* Hamlet, Act Ill, Scene ii\n¢ Lord Polonius: | did enact Julius Caesar | was killed i' the Capitol;\nBrutus killed me.\nCopyright Ellis Horowitz, 2011-2013 9"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted indexing ()",
          "* Sparsity of term-document matrix ()"
        ],
        "definitions": [
          "* Term-document matrix: a matrix representing the presence or absence of terms in documents ()"
        ],
        "formulas": [
          "* The size of the term-document matrix is approximately 500K x 1M = half-trillion elements, but with no more than one billion 1's ()"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "s9.png",
        "raw_text": "* Given | million documents and 500,000 terms\n¢ The term  Document matrix in this case will have size 500K  1M or half-\na-trillion 0’ and 1’s.\n¢ But it has no more than one billion 1’s.\n— So the matrix is extremely sparse, only 0.1% of the elements are 1\n* So instead we use  data structure for an inverted index that exploits sparsity\nand then devise algorithms for query processing\nCopyright Ellis Horowitz, 2011-2013 10\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "Inverted Indexing: Storing a list of all documents that contain a specific term."
        ],
        "definitions": [
          "Linked lists: Data structures that allow for dynamic space allocation and easy insertion of terms into documents.",
          "Inverted indexing: A technique used to efficiently store and retrieve information in search engines and databases.",
          "[PRIORITY] HIGH for Inverted Indexing concept and  Linked lists"
        ],
        "formulas": [],
        "algorithms": [
          "Dynamic space allocation process:",
          "[PRIORITY] MEDIUM for  Dynamic space allocation process"
        ],
        "examples": [
          "Caesar —_—_> (example of a term with associated documents)"
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "s10.png",
        "raw_text": "¢ For each term 7, we must store  list of all documents that contain 7.\n* Linked lists are generally preferred to arrays\n— Dynamic space allocation\n— Insertion of terms into documents easy\n— However, there is space overhead of pointers, though this is not too serious\nBrutus) -—>[2}-4-48\n| Calpurnia |“—> 8)\nCaesar —_—_ >\nCopyright Ellis Horowitz, 2011-2013 ie\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "Inverted Indexing: A data structure used to store words and their corresponding documents."
        ],
        "definitions": [
          "**Document ID**: Unique identifier assigned to each document.",
          "**Modified Token**: Modified form of a word, e.g., \"Caesar\" becomes \"caesar\".",
          "**Sequence of (Modified token, Document ID) pairs**: A list of tuples containing the modified word and its corresponding document ID."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Document parsing: \"Tem Doct and these are saved with the document ces ID i. sequence of (Modified token, peesar : ...)\" is an example of how documents are parsed to extract words.",
          "Example of document ID pairing: \"(brutus 1, Doc 1), (Caesar was killed Caesar. The noble ——— ' 1 , noble) becomes ((brutus, 1), (doc_1)), ((caesar, 2), (doc_2))\""
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "s11.png",
        "raw_text": "* Documents are parsed to extract words Tem Doct\nand these are saved with the document ces\nID i.  sequence of (Modified token, peesar :\n. was 1\nDocument ID) pairs ies '\ncept\nbrutus 1\nDoc 1 Doc 2 ——=> killed ‘\nit 2\n| did enact Julius So let it be with be 2\n. with. 2\nCaesar | was killed Caesar. The noble ———\n' 1 . noble\ni‘ the Capitol; Brutus hath told you brits 2\nBrutus killed me. iti tad 2\nCaesar was ambitious ta 2\nCopyright Ellis Horowitz, 2011-2013 ametons 12 ;"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1. **Inverted Indexing**: An index data structure for efficient information retrieval",
          "2. Understanding of a corpus: Knowledge of the document collection before parsing"
        ],
        "definitions": [
          "1. Inverted File: A sorted list of terms with their corresponding documents"
        ],
        "formulas": [
          "Note that there are no mathematical formulas or equations related to inverted indexing in this content, so I did not mark any as ."
        ],
        "algorithms": [
          "1. **Sorting of Inverted Index**: Sorting the list of terms by frequency or other criteria",
          "2. **Updating of Inverted Index**: Updating the index when new documents are added to the corpus"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "s12.png",
        "raw_text": ". . Term Doc # Term __|Doc #\n¢ Ifthe corpus is known in \\ ' ambitious __#\nadvance, then after all enact ‘ brutus 3\n4 capitol 1\ndocuments have been parsed the “ caesar 1\ninverted file is sorted by terms = *\"** : caesar 2\nthe 4 enact 1\ncapitol 1 hath 1\nres : Refined list of — ! 1\nme 4 terms , 1\nso 2 ,\nInitial capture of terms ————————> et  > i;\nbe 2 killed 1\nith 5 killed 1\n* However, on the WWW, casa 2 lt 2\ndocuments are constantly being = *™*, 2 nebe 3\nvat th 1\nadded and the terms are told 2 the 2\n. . you 2 told 2\nconstantly increasing caesar 2 you 2\nwas 2 was 1\nambitious 2 was 2\nwith 2\nCopyright Ellis Horowitz, 2011-2013 13\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "**KEY CONCEPTS: **",
          "1. **Inverted Index**: A data structure that stores a list of documents containing each unique term ()",
          "2. Importance of efficient indexing for search queries ()"
        ],
        "definitions": [
          "**DEFINITIONS: **",
          "1. **Term**: A word or phrase in a document ()",
          "2. **Document Frequency (DF)**: The number of documents containing a particular term ()",
          "3. **Term Frequency (TF)**: The frequency of a term within a document ()"
        ],
        "formulas": [
          "**FORMULAS/EQUATIONS: **"
        ],
        "algorithms": [
          "**ALGORITHMS: **",
          "1. **Inverted Index Construction Algorithm**: A step-by-step process to create an inverted index from a set of documents ()"
        ],
        "examples": [
          "**EXAMPLES: **"
        ],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "s13.png",
        "raw_text": "Term Doc # Term Doc # Freq\nambitious 2 ambitious 2 1\n. . . be 2 be 2 1\nbrutus 1 brutus 1 1\nMultiple term entries ina =\" 1 bats i{\n: capitol 1 capitol 1 1\nsingle document are cevser 1 caoser\ncaesar 2 caesar 2 2\ncaesar 2 did 41 4\nmerge ° did 1 enact 1 1\n. . . enact 1 hath 2 1\n° Frequency infot mation 1S hath 4 |\n! 1 it 2 1\nadded.  1 —> julius 1 1\nit 2 killed 1 2\njulius 1 let 2 4\nkilled 1 me 1 4\nkilled 1 noble 2 1\nlet 2 so 2 1\nme 4 the 1 1\nnoble 2 the 2 1\nso 2 told 2 1\nthe 1 Fen 2 1\nthe 4 was 1 1\ntold 2 was 2 1\nyou 2 with 2 1\nwas 1\nwas 2\nwith 2\nCopyright Ellis Horowitz, 2011-2013 14\nRs"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "**Inverted Indexing**: a data structure used to index and store terms in a document collection.",
          "**Dictionary file**: contains a list of unique terms (index entries) with their corresponding frequencies and document numbers.",
          "**Postings file**: contains the locations where each term appears in the documents, along with the frequency information."
        ],
        "definitions": [
          "**Term Frequency (TF)**: the number of times a term appears in a single document.",
          "**Document Frequency (DF)**: the number of documents that contain a particular term.",
          "**Inverted Index**: a data structure used to efficiently retrieve the locations and frequencies of terms in a document collection."
        ],
        "formulas": [],
        "algorithms": [
          "**Building an Inverted Index:**"
        ],
        "examples": [
          "The provided example illustrates a simplified inverted index for a small document collection, showing how terms are indexed and their frequencies stored."
        ],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "s14.png",
        "raw_text": "* The file is commonly split into  Dictionary and  Postings file\nTerm Doc# _ Freq v4 Tu\nambitious 2 1 Doc # __|Freq\nbe 2 1 Term Ndocs TotFreq 2 1\nbrutus 1 1 ambitious 4 ji  4\nbrut 2 1 2 _—_——————\ncapitel   brutus 2 2 2 1\ncaesar 1 1 capitol 1 [ 1 1\ncaesar 2 2 caesar 2 3 SI 1 1\ndid 1 1 2 2\ndid ‘ ‘ enact 1 12 1 1\nenact hath 1 12 1 1\nhath 2 1  4 Qe) 2 1\n1 1 2 Le\n!  1 1 1 2\n12 yt ae ee\nLs 2 1 julius 1 1 2 1\njulius 1 1 killed 1 2  1 1\nkilled 1 2 let 4 1 e/ Al 2\nlet 2 1 me 1 [) ae ae 5\nme 1 1 noble 1 1 ZA  1 1\nnoble 2 1 sO 1 1 (2 OT 2 1\nso 2 1 the 2   5 1\nthe 1 1 told 1 1 1 1\nthe 2 1 you 1 1 2 4\ntold 2 1 was 2 2 2 1\nyou 2 1 with 1 1 2 1\nwas 1 1 1 1\nwas 2 1 2 1\nwith 2 1 2 1\nCopyright Ellis Horowitz, 2011-2013 15\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "+ Inverted Indexing",
          "+ Dictionary (inverted index)",
          "+ Postings (document ids)"
        ],
        "definitions": [
          "+ Dictionary: a data structure that stores words and their corresponding postings",
          "+ Postings: the document ids associated with a word in the dictionary"
        ],
        "formulas": [],
        "algorithms": [
          "* Merge the two postings (postings are document ids)"
        ],
        "examples": [
          "+ Merged postings: combined document ids for both words"
        ],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "s15.png",
        "raw_text": "* Consider processing the query:\nBrutus AND Caesar\n— Locate Brutus in the Dictionary;\n* Retrieve its postings.\n— Locate Caesar in the Dictionary;\n* Retrieve its postings.\n— “Merge” the two postings (postings are document ids):\nge 2 HL Brutus\n2/43 )5)48)+ 134/21 +134) Caesar\nCopyright Ellis Horowitz, 2011-2013 16"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted indexing ()"
        ],
        "definitions": [],
        "formulas": [
          "* O(m+n) operations for merge ()"
        ],
        "algorithms": [
          "* Walk through two postings simultaneously in linear time ()"
        ],
        "examples": [
          "* Example of merging postings: \"If the list lengths are m and n, the merge takes O(m+n) operations\" ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "s16.png",
        "raw_text": "¢ Walk through the two postings simultaneously, in\ntime linear in the total number of postings entries\n5) gem) 2 Boe 8) ore\n8 [31 | Caesar\nIf the list lengths are  and n, the merge takes O(m+n)\noperations."
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "**Inverted Indexing**: The process of storing and retrieving documents based on their indices, allowing for efficient querying.",
          "**Query Processing**: The process of evaluating a query to retrieve relevant documents from the index."
        ],
        "definitions": [
          "*  **Postings**: A collection of document IDs that contain a specific term.",
          "*  **AND Operator**: A logical operator that combines two or more terms, returning only documents that contain all specified terms."
        ],
        "formulas": [],
        "algorithms": [
          "**AND Operator Algorithm**:"
        ],
        "examples": [
          "**Query Example**:"
        ],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "s17.png",
        "raw_text": "¢ What is the best order for query processing?\n* Consider  query that is an AND of  terms.\n* For each of the ¢ terms, get its postings, then AND\ntogether.\nm——>[214 7 8] 161 32] 641281 1\no——>[11 27 37578 | 16) 27 34]\nu——>3 eT ToT 1 TT\nQuery: Brutus AND Calpurnia AND Caesar"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "*  **Inverted Indexing**: not mentioned explicitly, but related to the technique described",
          "*  **Skip Pointers**: main idea discussed in this slide"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  **Technique of Skip Pointers for merging postings**:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "s19.png",
        "raw_text": "To speed up the merging of postings we\nuse the technique of Skip Pointers"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "- **Skip pointers**:",
          "- **Posting augmentation**:"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "s20.png",
        "raw_text": "Augment postings with skip pointers (at indexing time)\n¢ Why?\n¢ To skip postings that will not figure in the\nsearch results.\n* How?\n* Where do we place skip pointers?"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "*  Inverted Indexing",
          "*  Posting lists",
          "*  Successor (in the context of postings)",
          "*  Skip successor (of a posting on a lower list)"
        ],
        "definitions": [
          "*  Inverted Indexing: A data structure that stores references to words in documents, allowing for efficient retrieval.",
          "*  Posting lists: Lists of postings that contain specific word or phrase occurrences.",
          "*  Successor (in the context of postings): The next posting after a given posting."
        ],
        "formulas": [],
        "algorithms": [
          "*  Stepping through lists to process postings until a certain point (e.g., processing 8 on each list).",
          "*  Identifying the successor of a posting.",
          "*  Determining the skip successor of a posting based on its position in a lower list."
        ],
        "examples": [
          "*  Processing postings until reaching a specific point, such as getting to 16 on the top list and realizing its successor is 32.",
          "*  Identifying the skip successor of a posting (8 on the lower list) as 31, allowing for skipping ahead."
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "s21.png",
        "raw_text": "Suppose we’ve stepped through the lists until we process 8\non each list.\nWhen we get to 16 on the top list, we see that its\nsuccessor is 32.\nBut the skip successor of 8 on the lower list is 31, so\nwe can skip ahead past the intervening postings. »"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1.  **Skip Pointers**: shortcuts added at indexing time to help with AND queries",
          "2.  **Static Corpus**: when the corpus is relatively static, skip pointers are more useful",
          "3.  **Posting List Length (P)**: the length of a postings list affects the placement of skip pointers"
        ],
        "definitions": [
          "1.  **Skip Pointers**: shortcuts that help for AND queries and are useful when the corpus is relatively static",
          "2.  **Posting List**: a list of documents containing a particular term"
        ],
        "formulas": [],
        "algorithms": [
          "1.  **Simple Heuristic for Placing Skips**: use sqrt(P) evenly-spaced skip pointers for postings list of length P"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "s22.png",
        "raw_text": "+ skip pointers are added at indexing time; they are shortcuts, and they only help for AND queries and they are\nuseful when the corpus is relatively static\n+ there are two questions that must be answered:\n+ 1. where should they be placed?\n+ 2. how do the algorithms change?\n* More skips means shorter skip spans, and that we are more likely to skip. But it also means lots of\ncomparisons to skip pointers, and lots of space storing skip pointers. Fewer skips means few pointer\ncomparisons, but then long skip spans which means that there will be fewer. opportunities to skip.\n+ —  simple heuristic for placing skips, which has been found to work well in practice, is that fora postings list\nof length P, use sgrt{P} evenly-spaced skip pointers. This heuristic possibly can be improved upon as it\nignores any details of the distribution of query terms.\n+ Building effective skip pointers is easy if an index is relatively static; it is harder if  postings list keeps\nchanging because of updates.  malicious deletion strategy can render skip lists ineffective.\n+ See the YouTube video\n+ http:/;www.youtube.com/watch?v=tPsCQOsa7j0\nCopyright Ellis Horowitz, 2011-2013 23\na—x———ooEoEoEoEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEeEe—_e_eeee"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted indexing",
          "* Phrase queries"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "s24.png",
        "raw_text": "¢ We want to answer queries such as “stanford\nuniversity’’ — as  phrase\n¢ Thus the sentence “ went to university at Stanford” is\nnot  match.\n— The concept of phrase queries has proven easily understood\nby users; about 10% of web queries are phrase queries\n* No longer suffices to store only\n<term : docs> entries\nCopyright Ellis Horowitz, 2011-2013 25"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "+  Biword (or 2-gram) concept: consecutive pair of terms in a text",
          "+  Indexing biwords to improve phrase searching"
        ],
        "definitions": [
          "* Biword (or 2-gram):  A consecutive pair of terms in a text",
          "* Dictionary term:  Each bi-word is treated as a single dictionary term"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "+  friends romans",
          "+  romans countrymen"
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "s25.png",
        "raw_text": "¢  biword (or  2-gram) is  consecutive pair of terms\nin some text\n* To improve phrase searching one approach is to index\nevery biword in the text\n¢ For example the text “Friends, Romans, Countrymen”\nwould generate the bi-words\n— friends romans\n— romans countrymen\n* Each of these bi-words is now  dictionary term\nCopyright Ellis Horowitz, 2011-2013 26"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1. Inverted Indexing",
          "2. Biwords"
        ],
        "definitions": [
          "1. Vocabulary database : a collection of words or phrases used in indexing",
          "2. Boolean query on biwords : a search query broken down into smaller segments (biwords)"
        ],
        "formulas": [
          "Note that I did not mark any of the content as  since there are no mathematical formulas presented."
        ],
        "algorithms": [
          "1. Breaking queries longer than 2 words into biword segments",
          "2. Matching the query to terms in the index"
        ],
        "examples": [
          "1. Querying a 4-word phrase (stanford university palo alto) and breaking it down into Boolean queries on biwords"
        ],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "s26.png",
        "raw_text": "¢ Consequences\n— Biwords will cause an explosion in the vocabulary database\n— Queries longer than 2 words will have to be broken into\nbiword segments\n¢ Example: suppose the query is the 4 word phrase\nstanford university palo alto\nThe query can be broken into the Boolean query on biwords:\nstanford university AND university palo AND palo alto\n¢ Matching the query to terms in the index will work, but may\nalso produce false positives (i.e. occurrences of the biwords,\nbut not the full 4 word query)\nCopyright Ellis Horowitz, 2011-2013 27"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "Inverted Indexing: a data structure used to store and retrieve documents based on their terms."
        ],
        "definitions": [
          "*  **Inverted Entry**: an entry in the inverted index containing information about a specific term, including:"
        ],
        "formulas": [],
        "algorithms": [
          "*  Inverted Indexing Algorithm:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "s27.png",
        "raw_text": "¢ Store, for each term, entries of the form:\n<number of docs containing term;\ndocl: position1, position? ... ;\ndoc2: position1, position? ... ;\netc.>\nCopyright Ellis Horowitz, 2011-2013 28"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* ** Inverted Indexing**: Storing postings of the form docID: position1, position2, ..., where each position is a token index in the document.",
          "* ** Positional Index**: Expanding required postings storage significantly, even if we compress position values/offsets."
        ],
        "definitions": [
          "* ** Posting**: A record of the occurrences of a term in a document, including its frequency and positions.",
          "* ** Token index**: The index of a token (word) within a document."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* **** Document 1 has the following postings for term \"be\":",
          "* **** Document 2 has the following postings for term \"be\":"
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "s28.png",
        "raw_text": "for each term in the vocabulary, we store postings of the form\ndocID: position1, position2, ...,\nwhere each position is  token index in the document.\nEach posting will also usually record the term frequency\nAdopting  positional index expands required postings storage significantly,\neven if we compress position values/offsets\nLots of documents\n_— Lots of occurrences\n<be: 993427; 4\n1:7, 18, 33, 72, 86, 231;\n2: 3, 149;\n4:17, 191, 291, 430, 434;\n5: 363, 367, ...>\n* Nevertheless, this expands postings storage substantially\nCopyright Ellis Horowitz, 2011-2013 29\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted indexing: a method for extracting and merging doc:position lists for multiple terms ()",
          "* Proximity searches: searching for adjacent words in a document ()"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*"
        ],
        "examples": [
          "*"
        ],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "s29.png",
        "raw_text": "¢ Extract inverted index entries for each distinct term:\nto, be, or, not.\n* Merge their doc:position lists to enumerate all\npositions with “to be or not to be”.\n— to:\n. 2:1,17,74,222,5514 448, 6,190,429,433; 7:13,23,191; ...\n— be:\n« Same general method for proximity searches\n— In document 4 the word “to” appears in position 16 and the word “be”\nappears in position 17, so they are adjacent\nCopyright Ellis Horowitz, 2011-2013 30"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "s30.png",
        "raw_text": "« Among possible queries, nouns and noun phrases often appear, e.g.\n“abolition of slavery”, “renegotiation of the constitution”\n« But as seen above, related nouns can often be divided from each other by various\nfunction words\n* These needs can be incorporated into the biword indexing model in the following\nway:\n— First, we tokenize the text and perform part-of-speech-tagging. We can then\ngroup terms into nouns, including proper nouns, (N) and function words,\nincluding articles and prepositions, (X), among other classes.\n— Now deem any string of terms of the form NX* to be an extended biword.\nEach such extended biword is made  term in the vocabulary.\n— Eg. “renegotiation of the constitution” is mapped to     (two nouns and\ntwo others), so the others are ignored and the two word phrase “renegotiation\nconstitution” is added to the index\n* Programs that identify  word’ part-of-speech tag are based on statistical or rule-\nbased approaches and are trained using large corpora\nCopyright Ellis Horowitz, 2011-2013 31\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Frequency analysis in patent data*"
        ],
        "definitions": [
          "* POS tagging: Part-of-Speech tagging, a process of automatically assigning grammatical categories to words in a text*"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* TREC Patent Frequency — Phrase Frequency Phrase*"
        ],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "s31.png",
        "raw_text": "TREC Patent\nFrequency — Phrase Frequency Phrase\n65824 united states 975362 present invention\n61327 article type 191625 u.s. pat\n33864 los angeles 147352 preferred embodiment\n18062 Hong kong 95097 carbon atoms\n17788 North korea 87903 group consisting\n17308 New York 81809 room temperature\n15513 San diego 78458 seq id\n15009 Orange county | 75850 brief description\nThe phrases above were identified by POS tagging; The data above shows that common phrases\nare used more frequently in patent data as patents have  very formal style; many of the TREC\nphrases are proper nouns, whereas patent phrases are those that occur in all patents"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1. N-grams are sequences of consecutive words",
          "2. N-grams can be identified at the time of parsing",
          "3. The inverted index will need pointers to all dictionary terms containing an n-gram (postings)"
        ],
        "definitions": [
          "1. N-grams: any sequence of consecutive words",
          "2. Postings: pointers to all dictionary terms containing a given n-gram"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. Common n-grams are usually made up of stop words (e.g. \"and the\", \"there is\")"
        ],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "s32.png",
        "raw_text": "* Generalizing from bi-words, an n-gram is any sequence of  consecutive\nwords\n¢ N-grams can be identified at the time of parsing\n¢ N-grams of all lengths form  Zipf distribution with  few common phrases\noccurring very frequently and  large number occurring with frequency 1\n* Common n-grams are usually made up of stop words (e.g. “and the” “there\nis”)\n* For each n-gram, the inverted index will need pointers to all dictionary\nterms containing it — the “postings”\n* Therefore, the larger the value of n, the larger the amount of space required\nto hold all n-grams\nCopyright Ellis Horowitz, 2011-2013 33\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "*  Inverted Indexing: A technique used to speed up information retrieval by storing an index of words in a document along with their locations.",
          "*  N-grams: A sequence of n items (e.g., words, characters) from a given text."
        ],
        "definitions": [
          "*  Token: An individual item in a dataset (e.g., word, character).",
          "*  Bigram: A sequence of two items (e.g., words).",
          "*  Trigram: A sequence of three items (e.g., words).",
          "*  Four-gram: A sequence of four items (e.g., words)."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  The Google n-gram dataset, which contains 1 trillion tokens, is an example of a large-scale inverted indexing system.",
          "*  The number of bigrams (314,843,401) and trigrams (977,069,902) in the Google n-gram sample are examples of how inverted indexing can be used to analyze text data."
        ],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "s33.png",
        "raw_text": "* Google made available  file of n-grams derived from the web pages it\nindexed\n* http://googleresearch. blogspot.com/2006/08/all-our-n-gram-are-belong-to-\nyou.html\n* Statistics for the Google n-gram sample\n* Number of tokens 1,024,908,267,229 (1 trillion, ...)\n¢ Number of sentences 95,119,665,584\n+ Number of unigrams 13,588,391\n+ Number of bigrams 314,843,401\n* Number of trigrams 977,069,902\n* Number of four grams 1,313,818,354\n* Number of five grams 1,176,470,663\n+ For specific examples of 3-gram and 4-gram data see the web page above\nCopyright Ellis Horowitz, 2011-2013 34\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1.  **Inverted Indexing**: The concept of inverted indexing is not explicitly mentioned in this slide, but it can be inferred from the context.",
          "2.  **N-gram statistics**: The study focuses on analyzing n-grams (sequences of characters or words) in English and Chinese texts."
        ],
        "definitions": [
          "1.  **N-gram**: A sequence of n items (characters, words, etc.) from a text.",
          "2.  **Uni-gram**, **Bi-gram**, **3-gram**, **4-gram**: Specific types of n-grams with different lengths."
        ],
        "formulas": [],
        "algorithms": [
          "1.  **Analyzing N-gram statistics**: The study involved analyzing the frequency distributions of n-grams in English and Chinese texts, but no specific algorithm is mentioned.",
          "2.  **Calculating the ratio of Chinese characters to English words**: Although not explicitly stated as an algorithm, the conclusion that 1.5 Chinese characters correspond to 1 English word can be seen as a result of analyzing n-gram statistics."
        ],
        "examples": [
          "1.  **Study on 200 million randomly sampled English and Chinese Web pages**: The study provides an example of analyzing large text datasets.",
          "2.  **Frequency distributions of uni-grams, bi-grams, 3-grams, and 4-grams in English and Chinese texts**: The study provides examples of frequency distributions for different types of n-grams."
        ],
        "priority": "MEDIUM",
        "slide_number": 32,
        "slide_file": "s34.png",
        "raw_text": "« S. Yang et al, N-gram statistics in English and Chinese: Similarities and differences,\nICSC, 2007, Int’! Conf. on semantic computing, 454-460\n*  http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google\n.com/en/us/pubs/archive/33035.pdf\n* They analyzed 200 million randomly sampled English and Chinese Web pages and\nconcluded:\n1. The distribution of the unique number of n-grams is similar between English and\nChinese, though the Chinese distribution is shifted to larger\n2. The distribution indicates that on average 1.5 Chinese characters correspond to 1\nEnglish word\n3. While frequency distributions of uni-grams and bi-grams are very different, the\nfrequency distribution for 3-grams and 4-grams are strikingly similar\nCopyright Ellis Horowitz, 2011-2013 35\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "*  **Distributed Computing Cluster**: A pool of machines used for web-scale indexing to improve efficiency and reliability.",
          "*  **Fault-tolerance**: The ability of individual machines in a distributed cluster to handle unpredictable slowdowns or failures."
        ],
        "definitions": [
          "*  **Master Machine**: A central machine that directs the indexing job and assigns tasks to idle machines in the pool.",
          "*  **Indexing Job**: The process of creating an inverted index for efficient searching and retrieval of data."
        ],
        "formulas": [],
        "algorithms": [
          "*  **Task Assignment**: Break up indexing into sets of parallel tasks, and assign each task to an idle machine from the pool using a master machine as the coordinator.",
          "*  **Fault-tolerance Mechanism**: Implementing mechanisms to handle unpredictable slowdowns or failures in individual machines."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 33,
        "slide_file": "s35.png",
        "raw_text": "* For web-scale indexing one\nmust use  distributed computing cluster\n¢ Individual machines are fault-prone\n— Can unpredictably slow down or fail\n* How do we exploit such  pool of machines?\n¢ Must we maintain  master machine directing the\nindexing job — considered “safe”.\n¢ Break up indexing into sets of (parallel) tasks.\n¢ Master machine assigns each task to an idle machine\nfrom  pool.\nCopyright Elis Horowitz, 2011-2013 36"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "Inverted Indexing",
          "Parallel tasks (Parsers and Inverters)",
          "Document corpus splitting"
        ],
        "definitions": [
          "Parser: reads documents and emits (term, doc) pairs",
          "Inverter: complements the parser to complete index inversion",
          "Split: a subset of documents assigned to an idle parser machine"
        ],
        "formulas": [
          "Note that the mathematical formulas are minimal in this content, so there is no  category."
        ],
        "algorithms": [],
        "examples": [
          "Document corpus splitting example:"
        ],
        "priority": "MEDIUM",
        "slide_number": 34,
        "slide_file": "s36.png",
        "raw_text": "* One approach is to use two sets of parallel tasks\n— Parsers\n— Inverters\n¢ Break the input document corpus into splits\n— Each split is  subset of documents\n* Master assigns  split to an idle parser machine\n¢ Parser reads  document at  time and emits\n(term, doc) pairs\n* Parser writes pairs into / partitions\n* Each fora range of terms’ first letters\n— (e.g., a- g-p, g-z)— here j=3.\n* Now to complete the index inversion\nCopyright Ellis Horowitz, 2011-2013 37\nee"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "Inverted Indexing",
          "Posting (refer to sign \"assign\")"
        ],
        "definitions": [
          "Posting: a data structure that stores the locations of words in a document",
          "Inverted Index: an index data structure used for fast and efficient searching of text documents"
        ],
        "formulas": [],
        "algorithms": [
          "Assigning postings to a sign (assign...-| Master ]-----...)"
        ],
        "examples": [
          "The use of posting and inverted indexing for fast searching (no specific example given)"
        ],
        "priority": "MEDIUM",
        "slide_number": 35,
        "slide_file": "s37.png",
        "raw_text": "assign..-| Master ]-----...assign ;\nsign assign Postings\nfe =\nParser) —fei]ep]a2]  invener >\n—_)\\GPaser—tetlolez] invertor >fg5\nfo} [e} fo}\nit ° é —\nCParser>—taf| gplqz\nCopyright Ellis Horowitz 2011-2012 38\nrr"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "* Inverted indexing"
        ],
        "definitions": [
          "* Partition: a subset of documents or terms (not explicitly defined, but implied)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 36,
        "slide_file": "s38.png",
        "raw_text": "* Collect all (term, doc) pairs for  partition\n* Sorts and writes to postings list\n* Each partition contains  set of postings\nCopyright Ellis Horowitz, 2011-2013 39"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "*  Inverted indexing: a method for searching across multiple indices",
          "*  Main index vs. auxiliary index: maintaining two separate indices for efficient search and updates",
          "*  Big main index vs. small auxiliary index: trade-off between space and search efficiency"
        ],
        "definitions": [
          "*  Invalidating bit-vector: a data structure used to mark deleted documents in the auxiliary index"
        ],
        "formulas": [],
        "algorithms": [
          "*  Deletion process:",
          "*  Periodic re-indexing into one main index"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 37,
        "slide_file": "s39.png",
        "raw_text": "¢ Maintain “big” main index\n* New docs go into “small” auxiliary index\n¢ Search across both, merge results periodically\n* Deletions\n— Invalidation bit-vector for deleted docs\n— Filter docs output on  search result by this invalidation bit-\nvector\n* Periodically, re-index into one main index\nCopyright Elis Horowitz, 2011-2013 “0"
      },
      {
        "lecture": "inverted_indexing",
        "concepts": [
          "1. **** Inverted Indexing: a data structure used to efficiently store and retrieve information from a document collection."
        ],
        "definitions": [
          "1. **** Inverted Index: a mapping of keywords or phrases in a document collection to the documents that contain them.",
          "2. **** Document Collection: a set of documents that are being indexed and searched."
        ],
        "formulas": [],
        "algorithms": [
          "1. **** Building an inverted index involves the following steps:"
        ],
        "examples": [
          "1. **** Inverted Index for the given document collection:"
        ],
        "priority": "MEDIUM",
        "slide_number": 38,
        "slide_file": "InvIndexEx.png",
        "raw_text": "Draw the inverted index that would be built for the following document collection.\n(See Figure 1.3 for an example.)\nDoc1_ new home sales top forecasts\nDoc2 home sales rise in july\nDoc3_ increase in home sales in july\nDoc4 july new home sales rise\nSOLUTION. Inverted Index: forecast->1 home->1->2->3->4 in->2->3\nincrease->3 july->2->3 new->1->4 rise->2->4 sale->1->2->3->4 top->1"
      }
    ],
    "querying": [
      {
        "lecture": "querying",
        "concepts": [],
        "definitions": [
          "Query box: A text field in a web search engine where users can enter keywords or phrases to initiate a search."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "s1.png",
        "raw_text": "The Power of\nGoogle’ Query Box"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Query language processing",
          "*  Boolean operators (AND, BUT NOT)",
          "*  Search engine functionality (returns unexpected results)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  The query \"apple AND orchard BUT NOT computer\" still returns Apple Computer results",
          "*  Google search result showing unexpected results (image of Google search engine with a red X marked through it)"
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "s2.png",
        "raw_text": "Query: “apple AND orchard BUT NOT computer” still returns Apple Computer results\nGoogle (ws neennraienee «sue | [YAHDO! Semen\nGoogle \\\ngs Yahoo Bing |\nNotice\nRelated\nsearches\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "Boolean query",
          "Advanced Search page",
          "ANDing ( logical operator)",
          "ORing (logical operator)",
          "NOTing (logical operator)"
        ],
        "definitions": [
          "Boolean query: a search method that allows users to enter specific keywords and operators to refine their search results"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Entering a Boolean query in Google's Advanced Search page:"
        ],
        "priority": "HIGH",
        "slide_number": 3,
        "slide_file": "s3.png",
        "raw_text": "SC\neee\n9 oogle Advanced Search 2\n€ >\nTTT -| Search engines typically offer an\n| “Advanced Search” page where\nGoog  Advanced Search Advanced Search Tips | About Google users Can enter, in effect,  Boolean\nquery; e.g. Here is Google’ “advanced\nsite:www.neri.ie search” screen\nFind web pages that have... .\nall these words | ANDing. set of words\nthis exact wording or phrase Exact phrase\none or more of these words: | [ ] ORing  set of words\nBut don’ show pages that have...\nany ofthese unwanted words: | | NOT words\nNeed more tools?\nReading level: no reading level displ: ¥ [\nThis option des not reading level\napply in Google Instant.\nLanguage any language\nSearch within  site or domain: [wow.neti ie file type\nDate, usage rights, region, and more etc\nalso\nh}tps://help.bing.microsoft.com/#apex/bing/en-US/10002/-1\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  **Search Engine Query**: Understanding how search engines process queries is crucial in querying."
        ],
        "definitions": [
          "*  **SOY cose som** ( acronym or term not explicitly defined)",
          "*  **Google apple orchard computer**: Example of a query"
        ],
        "formulas": [],
        "algorithms": [
          "*  **Advanced Search**: Not explicitly defined as an algorithm, but mentioned as a feature in search engines"
        ],
        "examples": [
          "*  **Google apple orchard computer**: Example query",
          "*  **sows wer Apple Orchard -Computer**: Example of a modified query with operators",
          "*  **Whally’ Orchard**: Example query result or entity related to an orchard"
        ],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "s4.png",
        "raw_text": "SOY cose som -& ‘ 6 + °\n> 0 oboGle.comi * 4 go09he.co .\nHt Awe @ PF) Piazza,  Biome iment Es °  @  Ba\nGoogle\nGoogle apple orchard computer  §\nAdvanced Search\nsows wer  Apple Orchard -Computer\naria raging Seo ® —_ NS eae\nGarionalgary eS\n= . Lite Orchard Music\n5.0 kx KK (3) - Orchard °\nWhally’ Orchard\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Implicit AND: when searching for multiple keywords at once, Google/Bing will automatically search for pages that contain ALL of your keywords."
        ],
        "definitions": [
          "*  AND operator: used to combine two or more keywords in a search query (e.g., \"disney AND disneyland AND pirates\")."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Searching for multiple keywords at once: \"disney disneyland pirates\" is the same as searching for \"disney AND disneyland AND pirates\"."
        ],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "s5.png",
        "raw_text": "¢ Ifyou search for more than one keyword at  time, Google/Bing will\nautomatically search for pages that contain ALL of your keywords\n— This is called “implicit AND”\n¢  search for\ndisney disneyland pirates\nis the same as searching for\ndisney AND disneyland AND pirates\n(without the ANDs needed)\n* Google sometimes returns pages that don’ contain all of your query\nterms\nCopyright Ellis Horowitz, 2011-2022 5"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Searching for phrases in Google requires putting the phrase in quotes.",
          "*  Using the exact phrase within quotes will show pages that contain the exact phrase, not just the individual words."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Searching for the phrase \"pirates of the Caribbean\" within quotes will show pages that contain the exact phrase.",
          "*  Using the AND operator, searching for \"disney\" AND \"disneyland\" AND \"pirates of the Caribbean\" will show pages that contain all three words."
        ],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "s6.png",
        "raw_text": "¢ To search for phrases, just put your\nphrase in quotes. Google disney fatasyland “pirates ofthe caribbean” ba\n¢ For example, an Dimes Bites min ith\ndisney disneyland “pirates of the Pra of the Carbbean Ride | Wait Disney World Resort\ncaribbean” ovsatesrebaneenreteuner os miata\n* This would show you all the pages in rc eucaumnis sy soos\nGoogle’ index that contain the word\ndisney AND the word disneyland Pirates ofthe Carbbean traction) - Wikipedia\nAND the exact phrase “pirates of Dry Wes ng tt, bie Deeds Grund Pu Danan Pa rd\nthe Caribbean” (of course, without ee ee vos\nthe quotes) wa tacos naps sana me\nCopyright Ellis Horowitz, 2011-2022 6\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  General principles of electricity supply systems",
          "*  Law of electricity (no clear definition provided)",
          "*  Basic principle of electric generator",
          "*  Theory of electromagnetism (implied through discussion on magnetism and electromotive force)"
        ],
        "definitions": [
          "*  Electromotive force (EMF) - \"The pressure that is put on free electrons that causes them to flow\"",
          "*  Magnetism",
          "*  Electric generator"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "s7.png",
        "raw_text": "GGA GLE “generat princpies of electri\" ea GIA GLE genera pines of lecticty $\n‘° General principles of electricity supply systems\nvei cnine on/ General pens. of. electly_supoy. systems pt + pearl pre snap deem aan near\ntscriaty by pas sec cara (lc) trough  windngeon en rn care wich te mide second (one ampere) flow through  conductor having  resistance of one ohm is\nthree-phase windings on the stator of the machine. The magnetic field is rotated by means of  prime fone: vaaks The cgaaeey OF elacteia charge Is messired in cousarios.\noer otichrmpbe\n” The Principles of Electricity - Energizer\nGeneral principles of electricity supply systems | EEP | Transmission\ntip pnterestcom/pin/25888128254760417/\nWhat  the law of electricity?\narticles - Mentor EBS ‘What is the basic principle of the electric generator?\nvon mertorebs ARTICLES\nTo give the answer to this question, we need knowledge of the general principles of electricity pricing in ‘What is the basic principle of magnetism?\nelectricity is sold to suppliers by producers, not the rate in the end-user market. The latter, naturally, ‘What is the theory of etectiohy!\ncect aso\n‘The Basic Principles of Electricity | Anixter\nElectricity - Guides.turitin.com mes mncaninton/ ou esoucee bea plclenolactyn +\nihttps:/guides.turnitin.com » ..» Prompt Library » Secondary Education + The Vol. The pressure that is put on free electrons that causes them to flow is known as electromotive\nread an anicle ‘The Principles of Electricity - Energizer\n> C:\\Physies Dept History\\1965Curriculum wpd Sonat tt oe eyes how actor The it felt estas an\nWithout quotes\nWith quotes Copyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "Querying: searching for pages that contain words similar to search terms",
          "Word Variations or Automatic Stemming: feature of Google that finds pages with similar words (e.g. \"child\" and \"children\")"
        ],
        "definitions": [
          "Stop Words: common words like \"the,\" \"and,\" etc. that are ignored in searches"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Query [child bicycle helmet]: finds pages with similar words to search terms",
          "Query with only Stop Words, e.g. [the who]: gets treated as significant and returns relevant results (e.g. the Rock Group)"
        ],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "s8.png",
        "raw_text": "School of Engineering and Stop Words\n* The query [child bicycle helmet] finds pages that contain\nwords that are similar to some or all of your search terms,\n— eg., “child,” “children,” or “children's,” “bicycle,” “bicycles,”\n“bicycle's,” “bicycling,” or “bicyclists,” and “helmet” or “helmets.”\n— Google calls this feature word variations or automatic stemming\n* Google will often ignore Stop Words\n— However,  query with only Stop Words, e.g. [the who] gets treated\nas significant, returning pages for the Rock Group, the Who\n¢ Google limits queries to 32 words\n— Google will return nonsense results for this nonsense query\n— aardvark aback abacus abalone abandon abashed abbey abbreviate\nabdicate abdomen abduct aberration abhor abide ability abject able\nabnormal aboard abode abolish abolitionist abort about above abrade\nabridge abroad abrupt abscond absent absinthe\nCopyright Ellis Horowitz, 2011-2022 8"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Google favors results that have search terms near each other",
          "*  Google gives higher priority to pages with terms in the same order as the query",
          "*  Google is NOT case sensitive"
        ],
        "definitions": [
          "*  Case sensitivity: Google's ability to treat uppercase and lowercase letters the same way"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  The query [snake grass] finds pages about plants;",
          "*  The query [snake in the grass] finds pages about sneaky people",
          "*  [Red Cross ], [ red cross ], and [ RED CROSS ] return the same results."
        ],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "s9.png",
        "raw_text": "* Google favors results that have your search terms near each other\n— The query [snake grass] finds pages about plants;\n— The query [snake in the grass] finds pages about sneaky people\n* Google gives higher priority to pages that have the terms in the same\norder as in your query\n* Google is NOT case sensitive; it shows both upper- and lowercase\nresults\n— [Red Cross ], [ red cross ], and [ RED CROSS ] return the same results.\n* Google ignores some punctuation and special characters, including ! ? ,\nss T]@/#<>\n— Exceptions: C++, or math symbols in Google calculator\nCopyright Ellis Horowitz, 2011-2022 9"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Case sensitivity in querying (e.g., \"bush\" vs. \"Bush\")"
        ],
        "definitions": [
          "*  None explicitly stated"
        ],
        "formulas": [],
        "algorithms": [
          "*  None explicitly stated, but the example of querying different variations (e.g., \"yahoo/news\" vs. \"Yahoo!News\") implies a process or procedure for handling queries."
        ],
        "examples": [
          "*  Querying examples:"
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "s10.png",
        "raw_text": "° oe — o} : . =\njoogle tan $ GF Google 2m\neS) encns nal RR\nFox NEWS: ‘yahoo/news THe HHL. ‘ ‘ “— FOXNEWS ‘yahoo!mews. THe Ht. ee es\npsn ett  sna ea no\nbush and Bush yield the same results\nas does “apple” and “Apple”\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "1. **Querying**:  - The process of retrieving data from a database or system.",
          "2. **Database Management**:  - A field that deals with storing, organizing, and managing large amounts of data in databases.",
          "3. **Disney+**:  - A streaming service provided by The Walt Disney Company."
        ],
        "definitions": [
          "1. **Streaming Service**:  - A type of service that provides access to content on-demand over the internet.",
          "2. **Database**:  - A collection of organized data stored in a way that allows for efficient retrieval and manipulation."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **Disney+ Streaming Service**:  - An example of a streaming service provided by The Walt Disney Company.",
          "2. **Database Management**:  - Using Disney's database to manage and retrieve data about their streaming service."
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "s11.png",
        "raw_text": "3 + o| : . °\n6 o0eal com , oe | OCT  [ED > @\n2     & eo\nGoogle disney  Qa Google Disney  4Q\nQAI News ima Mag Vide  Qa lows iat Mops (5) Vide More 7\nDisney+ - Start Streaming The Walt « Disney.com | The official home for all things Disney The Walt ;\nClan ete. New eae. Dany Carnal howbacs xu Oga se. Fem Disney Irons seas ahs tans mane is rants eves xem crac Disney\nEnvetenumienio Sin Fin Sasebete hore pseneyworts isney 9 com\nDisney+, Hulu, and ESPN+\nMarvel Movie Home Disney, is an American dive ‘wecation ad reate memories for « Hetine Disney, is an American div\n‘Stream Disney Throwbacks ‘Burbank California, Wikiped\nDisney.com |The oficial home for allthings Disney Hendquarter: Burbank A= — [Za pearance\nResort, MORE @$ Disney sues for control Disney Sues to Keep Disney Isnt Investigating pears\nothey Marvel chraciors Complete Rights to Handing of Sonal\nWalt Disney World Resort in Orlando, Florida ABC News Boss Urge.\n. ae\n° ‘\ndisney Copyright Ellis Horowitz 2011-2022 Disney\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Boolean OR operator",
          "* Precedence of Boolean operators (OR > AND)"
        ],
        "definitions": [
          "* Boolean OR: an operator used to combine keywords in a search query, allowing for multiple possible matches",
          "* OR always in all caps"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Example 1: disney disneyland OR “pirates of the caribbean”"
        ],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "s12.png",
        "raw_text": "Boolean OR\n* The Boolean OR operator is acceptable in Google\nqueries, placed between keywords, and OR is always in\nall caps\n¢ For example,\ndisney disneyland OR “pirates of the caribbean”\n¢ This would show you pages in Google’ index that\ncontain the word disney AND the word\n(disneyland OR the phrase pirates of the\ncaribbean )\n* OR has higher precedence than AND"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* All query terms are implicitly ANDed",
          "* OR has higher precedence than AND"
        ],
        "definitions": [
          "* Implicit ANDing: The default behavior of search engines to treat all query terms as ANDed, unless specified otherwise",
          "* Precedence: The order in which operations or operators are performed in a query"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. abORc dis treated as  AND ( OR c) AND",
          "2. aORb  OR dis treated as ( OR b) AND ( OR d)",
          "3. aOR “ c” dis treated as ( OR (“ c”)) AND"
        ],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "s13.png",
        "raw_text": "¢ All query terms are implicitly ANDed\n* OR has higher precedence than AND\n¢ Three examples (a, b,  stand for query terms):\n1. abORc dis treated as  AND ( OR c) AND\n2.aORb  OR dis treated as ( OR b) AND ( OR d)\n3. aOR “ c” dis treated as ( OR (“ c”)) AND\n* and see\nhttps://support.google.com/websearch/answer/2466433?hl=en\n¢ https://help.bing.microsoft.com/#apex/bing/en-US/10002/-1"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Full-word wildcard query",
          "*  Google search functionality for exact phrases",
          "*  Use of quotes to search for exact phrases",
          "*  Stop words and their effect on search queries"
        ],
        "definitions": [
          "*  **Wildcard**: a symbol used in search queries to represent one or more characters (e.g. +)",
          "*  **Stop word**: a common word that is ignored by the search engine (e.g. \"the\", \"and\")"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Searching for \"it' small world\" instead of \"it's small world\"",
          "*  Using quotes to search for exact phrases (e.g. \"It'  Wonderful World\")",
          "*  Effect of stop words on search queries:"
        ],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "s14.png",
        "raw_text": ". Bits+\"wontGoogess\n* Google offers full-word wildcard € > & fi [& https//www.google com, e#AR|\nqueries EE Apps [)\n+ For example, if you search Gage EERIE ‘=\nGoogle for 1 Web\n* it’ + * world, “on\n* Google shows you all of the pages rnsipedasunih Nad ted. Moa Won\nin its database that contain the Rose and Kramer ading sional Mads othe We ate pogessed\nphrase, e.g. “it’  small world” \\ It'  Free World... - Wikipedia, the free encyclopedia\n... and “it’  nano world” ... face Worn 2 arate dee en Lh nity Pa\ncae, ; ”\nand “it’  Linux world” ... and Children Song: It ls  Small World ( With lyrics) - YouTube\nso on ] as wor youtube con/nal=Riypta ¥ +\n* The+ before  is required cana hat am wed tra Repent 2) Repent)\nbecause it isa stop word and \\ It'  Wonderful World (1939) - IMDb\nwould otherwise be ignored eee eee\n* This query works the same in isdrackng clase spd by stevbal master Ben Hecht str.\n* There must be  space after the Hofer etl re ney en ot\n+ Quotes. Tinie Parents Guide User Reviews .\nCopyright Ellis Horowitz, 2011-2022 14\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Query modifiers Search",
          "*  Search Operators"
        ],
        "definitions": [
          "*  daterange: Service - no definition provided ( likely a search operator)",
          "*  filetype: allinanchor:, allintext:, allintitle:, allinurl:, cache:, define:, jnanchor: - no clear definition, appears to be list of search operators"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Using advanced search operators (e.g. daterange:, filetype:) to refine search results"
        ],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "s15.png",
        "raw_text": "Query modifiers Search\n. Search Operators\n* daterange: Service\n* filetype: allinanchor:, allintext:, allintitle:, allinurl:, cache:, define:,\n¢ jnanchor: Web Search | filetype:, id:, inanchor:, info:, intext:, intitle:, inurl:, link:,\n+ intext: related:, site:\n¢ inurl: Groups allintext:, allintitle:, author:, group:, insubject:, intext:,\nrou!\n° site: intitle:\nallintext:, allintitle:, allinurl:, ext:, filetype:, intext:,\nirectory\nAlternative query types intities, inurl:\n* cache:\n* link: allintext:, allintitle:, allinurl:, intext:, intitle:, inurl:,\nunk; News\n+ related: location:, source:\n+ info: Product .\nallintext:, allintitle:\nOther information needs Search\n+ stocks:\nhttp://www.googleguide.com/advanced_operators_reference.html\nSee also\nhttps://help.bing.microsoft.com/#apex/bing/en-US/10001/-1"
      },
      {
        "lecture": "querying",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "* ****"
        ],
        "examples": [
          "* ****"
        ],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "s16.png",
        "raw_text": "rr eee iehiee  eee fei ne Btmny erm\n1. Google search for \"html\" 2. click on \"Search Tools\" and 3. click on Any Time and select\neas = \"Any Time\" appears Custom Range\n4. calendar appears, enter dates 5. Final Result"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Restricting search results to specific file types using filetype:",
          "* Understanding that the \"dot\" in file extension is optional"
        ],
        "definitions": [
          "* filetype: restricts search results to files with a specific suffix",
          "* Filetype: should not have spaces between filetype and the file extension",
          "* Optional inclusion of the dot (.) in file extensions"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Example 1: filetype:doc will NOT return .docx files",
          "* Explanation of how to use filetype: to restrict search results to specific file types, e.g. filetype:doc for Microsoft Word documents"
        ],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "s17.png",
        "raw_text": "——— m@ - filetype: restricts your\n(Se eS Se ae oe Le results to files ending in\nGoogle = —  specific file suffix, e.\nPe \".doc\" (or .xls, .ppt. etc.),\nema OTE Ss: and shows you only files\ncreated with the\neon ee corresponding program\nPemetat ¢ There can be no space\netme, heey between filetype: and\nLas Pe tet eH rn per the file extension\nae ¢ The “dot” in the file\nSos Seal  cect extension — .doc — is\noe SES enemas rer res optional\narepae = ¢  filetype:doc will NOT return\nak —_* docx"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* `inanchor` operator restricts results to pages containing specific query terms in anchor text or links",
          "* `allinanchor` operator restricts results to pages containing all specified query terms in anchor text on links"
        ],
        "definitions": [
          "* Anchor text: the text on links to a page"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Example 1: `[restaurants inanchor:gourmet]` returns pages where anchor text contains \"gourmet\" and page contains \"restaurants.\"",
          "* Example 2: `[allinanchor: best museums sydney]` returns only pages where anchor text contains all three words \"best\", \"museums\", and \"sydney\"."
        ],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "s18.png",
        "raw_text": "‘| - inanchor: will restrict the results to\nen — pages containing the query terms you\nGoogle rr aes specify in the anchor text or links to\nrr TTT the page.\neat ng le eons ¢ For example,\noy, Zande [ restaurants inanchor:gourmet |\nsean oe  ¢ will return pages in which the anchor\nPrieta text on links to the pages contain the\ncere ese ee word “gourmet” and the page\nae : contains the word “restaurants.”\n——— ;  ¢ allinanchor: restricts results to pages\ncontaining all query terms you specify\né — <=\" in the anchor text on links to the\nFor example, [ allinanchor: best museums sydney Page.\nwill return only pages in which the anchor text on links to the pages contain the\nwords “best,” “museums,” and “sydney.” 18"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Query types: By text, URLs, and titles",
          "*  Importance of searching body text only"
        ],
        "definitions": [
          "*  Intext: A query type that searches for the exact phrase within the body text (e.g., \"Pirates of the Caribbean\")"
        ],
        "formulas": [],
        "algorithms": [
          "*  Step-by-step process to avoid common query words in URLs and links:"
        ],
        "examples": [
          "*  Example 1: Searching for \"Pirates of the Caribbean\" with intext: \"Disney.com\"",
          "*  Example 2: Understanding the difference between searching by text, URLs, and titles"
        ],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "s19.png",
        "raw_text": "MM -\nMR ES Ne Siuee, * intext: ignores link\n= ‘By text, URLs, and titles, and\nonly searches body text.\n—  intext: helps you avoid\nPirates of the Caribbean | Official Website | Disney query words that are too\nVil te Paes Caitbom eto en st he novo, cht, ey common in URLs and\nMovies The Curse of the Black Pearl links.\nesta oes Ens Te Cpe fe Dish Pe * pirates intext:\"Disney.com”\nainsi oltiscaiieas On lack Spa can te Back requires Disney.com to be\n‘Stranger Tides site to play Pearl and legendary pirate of within the body of the web page\nGames Hector Barbossa\nPirates vs. Mermaids - Disney Games\nPirates of the Caribbean - Pirate' Conquest | Disney Games\n- 19"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Intitle: operator restricts search results to documents containing a particular word in its title."
        ],
        "definitions": [
          "* intitle: - an operator that restricts search results to documents containing a particular word in its title."
        ],
        "formulas": [],
        "algorithms": [
          "+ Example: \"pirates of the caribbean\""
        ],
        "examples": [
          "* Searching with intitle: operator: \"intitle:pirates\" or \"intitle: 'pirates of the caribbean'\""
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "s20.png",
        "raw_text": "gon gunn gunn pane won ‘s2. ¢ intitle: restricts the\nGoogle _ ernest  4a results to documents\n=\" vee oes containing  particular\nOOS oe pete ne eee word in its title.\npares ¢ You can also search for\nEF een\" phrases. Just put your\nFE rors : phrase in quotes\nxs os . intitle:pirates\n__— ¢ Intitle:”pirates of\n—— the caribbean”"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Restricting search results to documents containing a particular word in its URL using `inurl:*` and `inurl:disney` ()"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "s21.png",
        "raw_text": "Bae  as * inurl: restricts the\n| Gooske .  * results to documents\nsie een de Eee ts ala eo Sven nt 29 ar] containing  particular\ncarom word in its URL.\nae Se | Gnurl:disney\naa es * Results include\n= : * Disney.go.com\nSe * www.disney.de"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Restricting search results to a specific site using `site:` operator",
          "*  Importance of exact matching between `site:` and domain"
        ],
        "definitions": [
          "*  `site:` operator - restricts search results to a specific website or domain",
          "*  Domain - the name of a website (e.g. google.com, cs.stanford.edu)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Searching for \"masters site:cs\" to restrict results to Stanford University's Computer Science department",
          "*  Importance of exact matching between `site:` and domain (e.g. \"masters site:cs.stanford.edu\")"
        ],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "s22.png",
        "raw_text": "Google _mastorsstecs.\nos ge ene so oe * site: restricts the\nSee eemectconnte sine results to those\nenn * There can be no space\nenact Bea Beaman compte between site: and\naiacaiaemaneaiaia the domain.\nData Science -\nScares evict oprmionme Query is:\nSTAN TW Sane compte * masters site:cs."
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Using site: in conjunction with another search term or phrase."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  pirates site:disney.com",
          "*  pirates -site:disney.com",
          "*  Pirates -site:com",
          "*  Pirates site:edu"
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "s23.png",
        "raw_text": "Using site:\n¢ You can use site: in conjunction with another\nsearch term or phrase.\npirates site:disney.com\n* You can also use site: and negation to exclude\nsites.\npirates -site:disney.com\n¢ You can use site: to exclude or include entire\ntop level domains (and, like with filetype, the dot is\noptional).\nPirates -site:com\nPirates site:edu"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Deepfaking the Mind",
          "* Brain-Computer Interfaces (BCIs)"
        ],
        "definitions": [
          "* None in this specific content, but it's worth noting that \"web cache\" is a term related to web searching and caching. However, since there are no specific definitions provided, I won't mark it as ."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* \"Could Improve Brain-Computer Interfaces for People with 24 Disabilities\"",
          "* Using Ctrl+ or ⌘- (Mac) and the find bar to quickly search on a webpage"
        ],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "s24.png",
        "raw_text": "SC\nSchool of Engineering cac es\n{ Wot Secure | webeace.gooplevsreontent com **®\nBee  ° 5 . * cache:url shows\nThi Google' che of https://\\ sdu/. It hot of the it  on Feb 15, 2022\n08:63:41 GMT. The curent page could have changed in the meantime. Leam more. the version of  web\nFull version Text-only version View source page that Google has\nTip: To quickly find your search term on this page, press Ctrl+ or 98- (Mac) and use the find bar. in its cache.\nDepartment of\nComputer Science\nVemma is aia | %\ne, me  ya ye\n/  {\n3 ote Re Qe\n\\ \\-_ AAS xy\n“Deepfaking the Mind” Could Improve Brain-Computer Interfaces for People with 24\nDisabilities"
      },
      {
        "lecture": "querying",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "s25.png",
        "raw_text": "Fe woh tk sey ccmn rout internet Clare proved by Cal Site rhea ali]\noo 3osSes Link: restricts the\n| Google ss sass  results to those web\n| server sogesntemnr cacao, pages that have links\ngramianeeroniialon to the specified URL.\nse * link:Disney.com\nrenee ae * Note: apparently the link\nPent esas   ty operator only returns\nRent Suns Yad sample of web pages\nchetelale pointing to the link"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Related lists web pages that are \"similar\" to a specified web page",
          "* Querying  (note: this may not be explicitly stated in the slide, but it's implied as the topic of discussion)"
        ],
        "definitions": [
          "* Qos: aan + There can be no space between related: and the URL.",
          "* \"Related:\" is a search operator that lists web pages similar to a specified page"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Google's related: feature",
          "* Various university computer science department websites listed as examples of \"related:\" results"
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "s26.png",
        "raw_text": "Google  latesswww.cs.\n“ ; . #* related: lists web pages that\nare \"similar\" to  specified web\nCS | Computer Science\nMaren nou nedpenraandonus page.\nDepartment of Computer Science, Columbia University | Home\nQos: aan + There can be no space between\nrelated: and the URL.\nDonald Bren School of Information and Computer Sciences Bre]\nComputer Science - Computer Science ——_————— SSS\nStanford Computer Science  aad ane\nComputer Science Department at Princeton University\n‘Computer Science and Engineering: Welcome to CSE @ UCR aca Pact\nUCSB Computer Science oe :\npoe oo otters a5) Got\nends om"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Google's information retrieval system (implying the concept of a search engine)",
          "*  Specific information about particular web pages (suggesting the idea of searching for specific data)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  \"cere Tec oe rin that Google has\" is not a clear algorithm, but it could be interpreted as an informal description of the process. However, it's likely referring to Google's search algorithm, which is a complex system."
        ],
        "examples": [
          "*  Searching for specific information on a particular web page (illustrating how users interact with the search engine)"
        ],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "s27.png",
        "raw_text": "~ 2 some information\ncere Tec oe rin that Google has\nsnc about  particular\n‘canes web page."
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Querying with specific keywords can trigger special processing in search engines",
          "* Special processing in search engines for specific keyword queries, e.g., \"stocks:\""
        ],
        "definitions": [
          "* Stock ticker symbols"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Google stocks query with \"a0pl\", \"£aQ s#@\", and \"Co\" as stock ticker symbols"
        ],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "s28.png",
        "raw_text": "Google stocks:a0pl  £aQ s#@\nCo\n170.74 uso\n+1.85 (1.10%) # today ‘Apple inc. is an American muitinational technology\n¢ Ifyou begin  query with stocks: Google will treat the\nrest of the query terms as stock ticker symbols, and will\nlink to  finance page showing stock information for\nthose symbols."
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Google operators: special symbols used to refine search results",
          "*  Using math expressions in searches (e.g., 12 + 34+ 10 * (150/ 7))",
          "*  Utilizing dictionary definitions in searches (e.g., define:antidisestablishmentarianism)",
          "*  Tracking numbers and airline flight numbers for specific search results"
        ],
        "definitions": [
          "*  Antidisestablishmentarianism: a term used to demonstrate the limits of the English language",
          "*  Tracking number: a unique code provided by shipping companies like FedEx or UPS"
        ],
        "formulas": [
          "*  12 + 34+ 10 * (150/ 7) = 260.285714 (example math expression)"
        ],
        "algorithms": [],
        "examples": [
          "*  Using @ symbol to search social media: e.g., @twitter",
          "*  Searching within a price range using $ symbol: e.g., camera $50..$100",
          "*  Searching for synonyms using tilde symbol: e.g., ~car repair"
        ],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "s29.png",
        "raw_text": "« Math expressions are evaluated: 12 + 34+ 10 * (150/ 7) = 260.285714\n¢ Dictionary definitions: define:antidisestablishmentarianism\n¢ Put @ in front of  word to search social media. For example: @twitter.\n¢ Put $ in front of  number and search for  price. For example: camera $400.\n* Put .. between two numbers and search in  range.\nFor example, camera $50..$100.\n¢ Puta valid tracking number from FedEx or UPS and it will take you to the\ntracking site\n¢ Puta valid airline and flight number and it will give you its status\n¢ Puta tilde, ~car repair and it queries on ALLisynonyms of car, like auto\n— See the following links to further discussion of Google’ operators\n—  http://www.google.com/support/websearch/bin/answer.py? hl=en&answer=136861\n—_ http://www.googleguide.com/advanced_operators_reference.html\n—_ http://searchengineland.com/google-power-user-tips-query-operators-48126\nCopyright Ellis Horowitz, 2011-2022 29\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "**KEY CONCEPTS **"
        ],
        "definitions": [
          "**DEFINITIONS **"
        ],
        "formulas": [],
        "algorithms": [
          "**ALGORITHMS **"
        ],
        "examples": [
          "**EXAMPLES **"
        ],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "s30.png",
        "raw_text": "° °\nEven More Things One Can Do with Google\nGoogle suaseoss-oin 44 Google mronecaner  ba\nQA Videos) images News @ Maps  More Settings Toots Mortgage calculator\nfifty-three billion four hundred ninety-three scone coun\nmillion four hundred thirty-nine thousand oars payee $473\nfive hundred thirty-one rec\nSpeaking/writing out  number Mortgage calculator\nGoogle what' my ip  $ Google sm zlea\nALC) 80cks News O-SreEDIng Maps| More setings Tooke cm Gimme oereeg Ghee heme rte Senge Tt\nsoot\nWhats my\n172.248.32.30\nYour public IP aderess\n> Leammore about adresses\norn\nWhat is my IP address\nSpinners for Games"
      },
      {
        "lecture": "querying",
        "concepts": [
          "1. **Google Phonebook Operators**:  - Different types of Google phonebook search operators.",
          "2. **Privacy Violations**:  - Concerns raised about Google phonebook feature."
        ],
        "definitions": [
          "1. **Phonebook:**  - Searches the entire Google phonebook.",
          "2. **rphonebook:**  - Searches residential listings only.",
          "3. **bphonebook:**  - Searches business listings only."
        ],
        "formulas": [
          "Note: There are no mathematical formulas or equations in this content, so there's no need to mark any with ."
        ],
        "algorithms": [],
        "examples": [
          "1. **Google Phonebook Operators Example**:  - Different Google phonebook search operators (phonebook, rphonebook, bphonebook) explained."
        ],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "s31.png",
        "raw_text": "sates See eo oe eon ee * There were actually three\nig nea la tank aloe different Google\nass in awaninme east ent phonebook operators.\nae cman. || * phonebook: searches the\naetna oe entire Google phonebook.\nsheen inti aerme * rphonebook: searches\nresidential listings only.\nSheeran * bphonebook: searches\nmanganite business listings only\nAs of 2010, Google' phone book feature has been officially retired.\nBoth the phonebook:and the rphonebook: search operator have both been dropped\ndue to many complaints about privacy violations 3]"
      },
      {
        "lecture": "querying",
        "concepts": [
          "**Statistical models**: The lecture mentions that Google built a statistical model with the help of teachers to classify reading levels.",
          "**Reading level classification**: The feature is based on classifying webpages into different reading levels using statistical models."
        ],
        "definitions": [
          "**Statistical model**: A mathematical representation of a system or process that uses statistical methods to make predictions or classify data.",
          "**Reading level**: The complexity or difficulty of written content, often measured by factors such as vocabulary, sentence structure, and grammar."
        ],
        "formulas": [],
        "algorithms": [
          "**Google's reading level classification algorithm**: While not explicitly described, the lecture mentions that Google used a statistical model to classify webpages into different reading levels."
        ],
        "examples": [
          "**Ovarian cancer information webpage**: The lecture uses this example to demonstrate how the feature classifies content into different reading levels."
        ],
        "priority": "MEDIUM",
        "slide_number": 32,
        "slide_file": "s32.png",
        "raw_text": "Bi ezcean vacations -conge = Wa — Sovtin cancer - Google Sea: » Weg —\nOccawerraree Qc enero cK nahn > chet CS SS sss pcre\nNews Vacation Ideas for 2012 Rame-Florence-Venice from $1345 News\nShopping ie NT aataad ‘Shopping Ovarian Cancer Swmotoms | Ovations orTheCure org\nMore uape Tal ess Ecol com Moe Tea The Ea Sis sk Fats Protect Youse ends & amily\nPas th EEL” esuts by reading lvl or ovarian cancer\nQuery: European vacations Query: ovarian cancer\nThe feature is based primarily on statistical models built with the help of teachers.\nGoogle paid teachers to classify pages for different reading levels, and then took their\nclassifications to build  statistical model. With this model, they can compare the words\non any webpage with the words in the model to classify reading levels.\nGoogle dropped this feature in 2015.\nCopyright Ellis Horowitz 2011-2022\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Google introduced the Wonder Wheel in 2009, a flash-based interface that provided possible interpretations for search queries.",
          "*  The Wonder Wheel was removed in 2011 but restored in 2012 with a renaming of the \"wheel of possible interpretations\".",
          "*  In 2014, Google re-focused the Wonder Wheel to help advertisers choose keywords."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  In 2009, Google introduced the Wonder Wheel as a flash-based interface that provided possible interpretations for search queries.",
          "*  The Wonder Wheel was removed in 2011 but restored in 2012 with a renaming of the \"wheel of possible interpretations\"."
        ],
        "priority": "MEDIUM",
        "slide_number": 33,
        "slide_file": "s33.png",
        "raw_text": "2B eo cong sec 3 7\") gece —_\n€ CS fi\n“Web images Videos Maps News Shopping Grail more ~ Fis Horowitz “Hips News Shopping Gmail BllieMorowitz - 4\njoqvar Sead eouer facts  In 2009 Google\nGoogle nese —— ——- — introduced the Wonder\n83 Eventing we Wheel,  flash-based\naoe ‘anatase saguattacte > dad fasuarendanon suse eeestng _ | interface\nBl News 7 ;  ; In 2011 Google removed\n> More Jaguar é jaguar the Wonder Wheel but\nAny ine — uma  inca 7 scktotass provided no concrete\nPathe pa explanation for why it\nrico susan ssoaccat = source su habia did so:\nCaen toy In 2012 Google restored\nStes with images “ ’ the wonder wheel,\n. 5 “ renaming it the\nGoogle puts up  wheel of possible Pontexteal\ninterpretations including the jaguar »}|, Targeting Tool\ncar and the animal; clicking on the spokes “+ In 2014 it was re-focused\nof the wheel bring up  refined wheel which to help advertisers chose\neventually lead to  modified query their keywords\nIn 2011 Google removed the Wonder Wheel\nCopyright Ellis Horowitz, 2011-2022 33\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Google Code Search was a free beta product that allowed web users to search for open-source code on the internet.",
          "*  It used a regular expression engine to search for code in various formats, including tar.gz, CVS, and Subversion.",
          "*  The service employed a methodology that combined trigram indexing with a custom-built regular expression engine.",
          "*  It supported POSIX extended regular expression syntax."
        ],
        "definitions": [
          "*  Trigram index: A method of indexing data to enable fast searching.",
          "*  Regular expression engine: A software component that searches for patterns in text or code using regular expressions.",
          "*  POSIX extended regular expression syntax: A standard for regular expressions used in Unix-like operating systems."
        ],
        "formulas": [],
        "algorithms": [
          "*  The methodology employed by Google Code Search combined trigram indexing with a custom-built regular expression engine.",
          "*  The service supported fast indexed regular expression searches over local code."
        ],
        "examples": [
          "*  Searching for open-source code using the \"file:\" operator to match common file extensions for a language.",
          "*  Using regular expressions in queries to search for specific patterns in code."
        ],
        "priority": "MEDIUM",
        "slide_number": 34,
        "slide_file": "s34.png",
        "raw_text": "Google Code Search\nFrom Wikipedia, the ree encyclopedia\nNoto be confused with Google Code.\nGoogle Code Search was  free beta product rom Google which debuted in Google Labs on October 5, 2006, allowing web users to Google Code Search\nsearch for open-source code on the ntemet. Features included the ablty to search using operators, namely Lang:, package', =\nLicense: and fie: Google code search\nThe code avaliable or searching was  various formats including targz,tarbz?, ar, and zip, CVS, Subversion, git and Mercutal —pgyeiopare) Google\nsepostionies. Initial release October 5, 2006\nSSS Development statue Dsconinued\nOperating system — Ay (wot besed\njaton)\n3 See also Ww) ° \\ &\nWebsite se goog com\n4 Relerences: ‘codesearch?\n5 External inks\nRegular expression engine «t)\n‘The site allowed the use of regular expressions in queries, which at the time was not offered by any other search engine for code.|“'tion reece} Thig makes it resemble grep, but\n‘over the world' public code. The methodology employed combines  trigram index with  custom-built, denial-of-service resistant regular expression engine.!\"!\nIn March 2010, the code of RE2, the regular expression engine used in Google Code Search, was made open source !2]\nGoogle Code Search supported POSIX extended regular expression syntax, excluding back-references, colting elements, and colaton classes.\nLanguages not officialy supported coud be searched for using the fie: operator 1o match the common fle extensions for the language.\nDiscontinuation {ext}\nIn October 2011, Google announced that Cade Search was to be shut down along with the Code Search API.) The service remained online until March 2013,!*] and it now returns\nIn January 2012, Fuss Cox publshed an overview of history and he technical aspects ofthe tool, and open- sourced abasic implementation of  simlar functionality asa st of\nstandalone programs that can run fast indexed regular expression searches over local code.51"
      },
      {
        "lecture": "querying",
        "concepts": [
          "**Patent search**: The process of searching for patents using various parameters."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  **Patent search algorithm**: No specific steps mentioned, but it's implied that there is a process for searching patents."
        ],
        "examples": [
          "**Google Patents Page example**: The slide shows an image of a Google Patents page with various patent results. This could be considered an example of how to use the patent search system."
        ],
        "priority": "MEDIUM",
        "slide_number": 35,
        "slide_file": "s35.png",
        "raw_text": "SC\noo See rn Ca Oe vena € >\nGoogle _us701748082 BY patent query\nusing patent\nGoogle number\nDocument compression system and method for use with\na5 ax -\nInitial Google Patents Page OMEN  tection gooraoon Aas\nGoogle us 701748082 fo | 2 0@\nSample patent results\nnr page;\nforuse with tokenspace repository akon” Se Note download\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Google Books: a digital database of scanned books and magazines",
          "*  Optical Character Recognition (OCR): process of converting scanned texts to digital text",
          "*  Copyright violations: potential issue with scanning and storing copyrighted materials",
          "*  Editing errors: OCR process may introduce many errors into scanned texts"
        ],
        "definitions": [
          "*  OCR: Optical Character Recognition, the process of recognizing and extracting text from images or scans"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 36,
        "slide_file": "s36.png",
        "raw_text": "C88) 6 conve =| ; ; —\n‘ 2\nHapps 0 0801872 Home Page 0 CSC1871 Home Page 0) CSC1361 Home Page Ill Computer Science omen! the full text of books and magazines that Google\n0@ | has scanned, converted to text using optical character\nrecognition (OCR), and stored in its digital database\nBooks are provided either by\n| publishers and authors, through the Google\n00g le Books Partner Program, or by\n*Google' library partners, through the Library Project.\nz=\nSearch he works most comprehensive index of uxt books. Controversey:\n“een Google has been criticized for potential copyright\nviolations, and lack of editing to correct the many errors\nintroduced into the scanned texts by the OCR process.\nAs of October 2015, the number of scanned book\ntitles was over 25 million.\n' Google estimated in 2010 that there were about\n130 million distinct titles in the world,\nand stated that it intended to scan all of them"
      },
      {
        "lecture": "querying",
        "concepts": [
          "Querying",
          "Full View",
          "Snippet View",
          "Limited View",
          "Querying: HIGH (Understanding querying concepts is crucial for exam)"
        ],
        "definitions": [
          "Querying: The process of searching for specific information in a database or repository.",
          "Full View: A mode of viewing search results that displays the entire content of each result.",
          "Snippet View: A mode of viewing search results that displays a brief summary or excerpt of each result.",
          "Limited View: A mode of viewing search results that restricts the amount of information displayed for each result.",
          "Full View, Snippet View, and Limited View: MEDIUM (Knowledge of these modes is important but not as critical as querying concepts)"
        ],
        "formulas": [],
        "algorithms": [
          "The process of querying is not explicitly described as an algorithm in this slide. However, based on the context:",
          "Search process:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 37,
        "slide_file": "s37.png",
        "raw_text": "ce — —\nFull View , .\nSnippet View\né\nLimited View https://www.google.com/googlebooks/library/screenshots.html#books-fullview\nCopyright Ellis Horowitz 2011-2022\nCO ee——"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Dynamic character grouping",
          "*  Consistency contains toporapic moos (Note: This appears to be a typo or unclear concept, but I'll keep it as is)"
        ],
        "definitions": [
          "*  Google Scholar: freely accessible search engine that indexes the full text or metadata of scholarly literature"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  0 & Secure itps:/fchola google.com #2 (Note: This appears to be an example URL, but its relevance and importance are unclear without further context)"
        ],
        "priority": "MEDIUM",
        "slide_number": 38,
        "slide_file": "s38.png",
        "raw_text": "0 & Secure itps:/fchola google.com #2\nAspe 1 C5C1672 Home Pope 1 CSCI571 Home Pge_D CSC1361 HomePage Ii Corputr Since. eres\nkeMy library  My Citations = My updates Alerts More ~\n* Google Scholar is  freely accessible search\n| engine that indexes the full text or metadata of\n00g  scholarly literature across an array of publishing\nSchol . rae\nsneer formats and disciplines.\nreviewed online academic journals and books,\ndar perce vam onion apt papal map conference papers, theses, dissertations, etc\ncovsour reconstruction\nDynamic character grouping bese on four consistency contains\ntoporapic moos\nSee  ves\nStand on the shoulders of giants"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Relevance Feedback",
          "* Query Expansion"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 39,
        "slide_file": "s39.png",
        "raw_text": "Relevance Feedback &\nQuery Expansion\nCopyright Ellis Horowitz 2011-2022"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Peer-to-peer processing ()",
          "* Feedback mechanism for relevance of retrieved documents ()",
          "* Query processing on peer-to-peer networks ()"
        ],
        "definitions": [
          "* Peer-to-peer processing: \"a way to utilize peer-to-peer networking for distributed computing\" ()",
          "* Distributed computing: not explicitly defined, but implied as a concept related to peer-to-peer processing ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Google's use of peer-to-peer processing (mentioned as an example) ()",
          "+ Storage Device Evolution: General-Purpose Peer Processing ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 40,
        "slide_file": "s40.png",
        "raw_text": "+ After initial retrieval results are presented, allow the user to provide feedback on the relevance of\none or more of the retrieved documents\n— Google offers this but not very prominently (see 10 related searches below)\n‘Bi veer-to-pererocessing -<oo  am 7 ‘Bi veer-to-pererocessing -<oo  am 7\n€ >\nGoogle peer-to-peer processing Google peer-to-peer processing\nSearch Search\nEo Izoduton to Windows Peer-to-Peer Neworking  Enttng Related searches\nImages ep 27, 2008 — Another way to ulize peer-io-peer networking for distributed Images Related searches for peer-to-peer processing\nVideo  Peer-To-Peer Work Processing Apo Vth WCF Veo SSerinccersmceaing alien sasriosue developing\nisda microa confer utragocnsR25T4 ax Seloseergucesangodes,  parloseermonufoctag\nlroduton to Windows Peer-to-Peer Neworking\nMore What is peer-to-peer process More technet. microsoft. conven-us/library/bb457079. aspx\nvekLanewer com> 2 Tecnoagy> Compe» Computer Nebworing ep 27, 2006 — Another way to utlize peer-to-peer networking for distributed\nLos Angeles, CA thine abcess, Network ables comecte to ch machine Prog oa Any tine Drocessing foun provams on each peer alan Gunna le prosessor tes\nChange location = pee  Peer-To-Peer Work Processing Apo With WCF\nStorage Device Evolution: General-Purpose Peer Processing Past 24 hours msdn microsoR comfer-usimagazineld@IE2514 aspx\nMore search tools eters: What is peer-to-peer process\nAn Efficient Scheme for Query Processing on Peer-to-Peer Networks ‘ki answers com>» Technology > Computers » Computer Networking\n‘ »] Ie 5\nCopyright Ellis Horowitz, 2011-2022 40\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "**Enhanced Related Searches**: Google has improved its related searches feature to provide more relevant results."
        ],
        "definitions": [
          "**Category-based search**: Searching for a category, such as \"rock bands\" or \"german cars\", to get related searches and top members."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**Related Searches for German Cars**: If you search for \"german cars\", Google will show the most popular members of that category, e.g. Audi, Volkswagen, BMW, etc.",
          "**Related Searches for Rock Bands**: If you search for \"rock bands\", related searches will include top rock bands such as Metallica and Led Zeppelin."
        ],
        "priority": "MEDIUM",
        "slide_number": 41,
        "slide_file": "s41.png",
        "raw_text": "o/) San)\nae * * Google has enhanced their related searches,\n€ CS tt\neg.\nTop references for german cars - Feedback  ° If you search for the name of  category,\nAudi Volkswagen Smart Google will show the most popular members,\nBMW Porsche Mini “ ”\nMercedes Benz Opel Bentley 3 e.g. “german cars\nS]-nesa)\n€\nRelated searches:\nRelated searches for rock bands  ¢ If you search for  category like “rock bands”\nalec maa meicoaes — hilhaatd You get related searches including  few top\ntimactbas asueanetends fuekartie  [eceppatia rock bands such as metallica and led zeppelin\nIa.coogos tesa nsetedsen) £9 pte oce0-coogese> WOR —_— 2\nAyo sores fo Pablo Pleasso 1 + If you search for famous people you get photos\nBe e)  ea r4 <————___ of their work, e.g. Pablo Picasso returns photos\nan oe of his famous painting\nlecaneeeReEEEEEEEE  Copyright Ellis Horowitz, 2011-2022 41\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Querying",
          "*  Search engines (Yahoo!, Bing)",
          "*  Related searches"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Yahoo!'s 3rd result for query \"jaguar\" is Bing, indicating a ranking or algorithmic decision.",
          "*  Extensive ads for the car at the top and side of search results indicate that searches for the animal are rarer."
        ],
        "priority": "MEDIUM",
        "slide_number": 42,
        "slide_file": "s42.png",
        "raw_text": "Soe eo as —— ® japuer - Yahoo! Search Re,\nEO weetrocon wa €\nccosrevenetooe () cisvivenaraps () CSc tenarae [)easterwesiemen.—_ (Comte samc De = 1 wierd, CSCLS72 Heme Page [) CSCIS71 HomePage [°} CSC1951 Home Page) Eke Horvat’ Home P. C5 other bookmarks\nRi aes Oe Ei\nBING m0\" 2] Woo imasesViseo Lora! Shopping) News More\nOn the query “jaguar” yahoo’ 3rd result is the Bing and Yahoo clearly mark\nanimal; Bing’ 2\" result is the animal; all have Related Searches\nextensive ads for the car at the top and side indicating that\nthe query for the animal is far rarer; Bing, on the left, puts up\nalternatives, e.g. Jaguar Luggage, Jacksonville Jaguars;\nYahoo also provides related searches but only for the car: bmw, lexus, ete\nCopyright Ellis Horowitz, 2011-2022 42\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "**Query Optimization**: The process of improving the performance of a query by optimizing its execution plan."
        ],
        "definitions": [
          "**Index**: A data structure that enables fast lookup, insertion, and deletion of data in a database."
        ],
        "formulas": [
          "None"
        ],
        "algorithms": [
          "**B-tree Indexing**:"
        ],
        "examples": [
          "**Query Example**: A user searches for all customers in a database who live in a specific city. The query optimizer uses an index on the customer's address to quickly locate the relevant data."
        ],
        "priority": "MEDIUM",
        "slide_number": 43,
        "slide_file": "s43.png",
        "raw_text": "Auto-Completion\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Auto-completion is a form of relevance feedback",
          "* Predicting word or phrase that the user wants to type in without actually typing it in completely"
        ],
        "definitions": [
          "* Auto-completion: predicting word or phrase that the user wants to type in without actually typing it in completely",
          "* Relevance feedback: a form of auto-completion"
        ],
        "formulas": [],
        "algorithms": [
          "+ Spelling corrections algorithms to assist in making guesses",
          "* The challenge is to search large index or long list of popular queries in very short amount of time so the results pop up while the user is typing"
        ],
        "examples": [
          "* Browser filling in your name, address and/or email in a form",
          "* Search engines using past history, phonetic Soundex algorithms, and spelling corrections algorithms to assist in making guesses"
        ],
        "priority": "MEDIUM",
        "slide_number": 44,
        "slide_file": "s44.png",
        "raw_text": "* Auto-completion is the process of predicting  word or phrase that the user\nwants to type in without the user actually typing it in completely\n« Auto-completion is  form of relevance feedback\n— This feature is effective when it is easy to predict the word being typed, e.g.\nwhen  browser fills in your name, address and/or email in  form;\n— Search engines may use past history, phonetic Soundex algorithms, and\nspelling corrections algorithms to assist in making guesses;\n— The challenge is to search  large index or  long list of popular queries in\nvery short amount of time so the results pop up while the useris typing\nCopyright Ellis Horowitz, 2011-2022 44\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  **Autocomplete**: Google's feature that provides suggestions based on user input",
          "*  **Experimental feature**: Auto-complete was initially an experimental feature in 2004"
        ],
        "definitions": [
          "*  **Autocomplete**: A feature that predicts the next character or word a user is likely to type"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Google's auto-complete suggesting \"Anniversary Gifts\" when typing \"A\"",
          "*  Auto-complete offering different possibilities after entering the second character",
          "*  Examples of websites with autocomplete features (e.g., Google Maps, Amazon)"
        ],
        "priority": "MEDIUM",
        "slide_number": 45,
        "slide_file": "s45.png",
        "raw_text": "SC\nGoogle [razon  * Google has been offering auto-\nse . . .\nSearch aa completion since 2008, though it\nemerican alrines .\nceo —_ was an experimental feature as\nvam AtbongoMBOTEAL Sn far back as 2004.\nMaps FlgeSelochon ard Araang Pies. Foe Shpplng on Ordre Ova 825 .\nMos 12179 peopl + page * Google does automatic\nNews Amazon com: Online Steping fr tectonics, Apparel, Commu. completion even after the user\n‘Shopping Online retailer of books, movies, music and games along with electronics, toys, apparel, .\nwoe \"Spewack AEN essen fame: Regn t= rt enters just the first character\nOG ¢« When the second character is\nentered  totally different set of\nGoogle fmicsyom ——~— possibilities may be offered\nanthropologie\nSearch angry birds\nandroid market\nAnniversary Gifts 1 (877) 5309512\nImages www.bluenile.com - aokcinio 3,437 seller reviews\n‘oe Tin he pret tor her Free FedEx & 9-Day Retus\nVideos Anniversary Gifts | PersonalizationMall.com\n‘en poraateatenmalcon- S80 2167 sla rovews\nNews Unique Romantic Anniversary Gifts Engraved Free & Ship in 1-2 Days!\nsvoping Bresmad Gis -Weddng Favors Weng Gis Groomsmen Gis\nMore MY M&W'S® - Anniversary | MyMMs.com\nvan nymnmnscom :\nTp RBA Tlorowitz, 2011-2022 45\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Autocomplete feature in Google search",
          "* Intellisense or suggestion feature in Google search"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "* Google's autocomplete algorithm",
          "* Correction and suggestion algorithm for misspelled words"
        ],
        "examples": [
          "+ \"amazon kindle\"",
          "+ Squiggly red line under \"anaheim duks\" suggesting the correct spelling \"Anaheim Ducks\""
        ],
        "priority": "MEDIUM",
        "slide_number": 46,
        "slide_file": "s46.png",
        "raw_text": "‘Search Images Videos Mape News Shopping Gmail More Eile Horowitz | fil e# |\nGoogle [aa * By the time the fourth\nanaheim ducks\nsean [anaes character is entered Google\nos — has already guessed that the\n|  Arabs Duck Tks -Bu Ducks Hockey Tk Toy, ; word has been misspelled,\nvoce ——_ TiAl Ducks Ts Ars tom Honda Contr and shows this by the\n‘ wheter es squiggly red line; so auto-\nae ‘Anaheim Ducks 9\n‘ Ocal oe Neve sata, players, coaches, sched, and aren oration < completion and spelling\nSSS Ooo correction are intertwined\n‘Elie Search Images Videos Maps News Shopping Gma¥. ore Elie Horowitz | fil  *\nGoogle | exe\ntmazon promo code\nSearch amazon kindle\namazon prime\nNews ‘Showing results for amazon\n‘More Amazon.com: Online Shopping for Electronics, Apparel, Computers ... 5\nPAA orowitz, 2011-2022 46\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "1. **Autocomplete feature**  - The ability of a search engine to display results for a partially typed query, including links to related searches."
        ],
        "definitions": [
          "1. **Related search**: A search that is suggested by the autocomplete feature based on the user's input."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **Autocomplete example**: When searching for \"Mikhail Gorbachev\", the search engine also suggests related searches for \"Mike Gorbachev\" and displays links to relevant profiles on social media platforms."
        ],
        "priority": "MEDIUM",
        "slide_number": 47,
        "slide_file": "s47.png",
        "raw_text": "‘Nike garbachev- Google Sear» We\nTS | Will display results for\n“ Hi Hi 99\nGoogle mic serene ay | “Mikhail Gorbachev”, but\nalso provide  link if instead\nSearch\nthe user actually wanted to\ncvernhing Showing resus or mike aorbachey search for “mike garbachev”\nMaps Mikhail Gorbachev - Wikipedia, the free encyclopedia\nMore Mike Gorbachev | Facebook\nMike Gorbachev profiles | Linkedin\nMike Gorbachev (@HeyMikeF antasy} on Twitter ¥\n‘ :\nCopyright Ellis Horowitz, 2011-2022 47\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Auto-complete suggestions",
          "*  Limitations of auto-complete systems (e.g., running out of alternatives)"
        ],
        "definitions": [
          "*  None explicitly stated, but implied concepts include:"
        ],
        "formulas": [],
        "algorithms": [
          "*  None explicitly stated, but implied concept is the process of generating auto-complete suggestions based on user input."
        ],
        "examples": [
          "*  Yahoo's auto-complete system running out of alternatives after a certain number of characters are typed.",
          "*  Google's auto-complete system starting to provide suggestions from the first character typed.",
          "*  Examples of websites (e.g., \"mad mike garbage pail kids\") that have implemented auto-complete systems."
        ],
        "priority": "MEDIUM",
        "slide_number": 48,
        "slide_file": "s48.png",
        "raw_text": "€ >\n‘ though it starts after typing the\n(loo! | mike garb .\nas = third character, whereas\neaee mice Google starts on the first\nBaus ‘mad mike garbage pail kids\nGow, peatapcleaan character\nBr reo ques ¢ As the user types more\n= 3, anit! seu characters in this example\n- - Yahoo runs out of alternatives\nor - — and when one more character\n€\neS ey ees OTE aes is added there will be no more\nYAHOO! nies SSS auto-complete suggestions\nFriday, February 3, 2012 Hie\nace =   fit BAA 05 Tey Moran\nBBitrescopee  as\n< 2 lis Horowitz, 2011-2022 48\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "* Querying and search results (mark with )",
          "* Search engine algorithms and ranking systems (mark with )",
          "+ Query: A request for information submitted to a search engine (implied by )",
          "+ Search result: The output provided by a search engine in response to a query (implied by )"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Yahoo search results for \"Gorbachev\" vs. \"gorbachev\" (mark with )",
          "* Search results for \"Mike Gorbachev\" (mark with )"
        ],
        "priority": "MEDIUM",
        "slide_number": 49,
        "slide_file": "s49.png",
        "raw_text": "e008 mike garbachev - Yahoo! Search Results ]\nOn Cee 7 aiejeq.) After hitting Enter, Yahoo does offer\n(omnia Cpsenaeted Stes ional” (teed + Ces name > Rd sokmae = the correct spelling of “Gorbachev”\n[Goma Roe EL eee eee oo Tn SSeS ,\ncont | Sonn  returns many results; in case the user\nYAHOO, ike sorbachev actually wanted to search for “garbachev”\nwes | omces | woeo | miceona | wes | -acSE wore it provi  produce those\nral We have included michae! gorbachev ‘ests - Show only mike garbachev results, but after clicking Yahoo delivers\npol Belin foenien ean more results for Gorbachev and none for\npana Nile in\" cin Re Boece Garbachev\nMikhai Gorbachev: Bioarohy from Answers com\nMike Gorbachev - Image Results\nMichael Gorbachev | Facebook\n‘My County and the Worl, by Michael Gorbachev\nMikhail Gorbachev - Biography - Nobelpize.org\nmichael gorbachey | crazymonk.org ,\nCopyright Ellis Horowitz, 2011-2022 49\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "1. **Auto-Completion**: Bing's use of previous queries to make suggestions before user enters a single character",
          "2. **Querying**: Process of searching for information on a search engine like Bing"
        ],
        "definitions": [
          "1. **Bing Auto-Completion**: Bing's feature that suggests possible queries or keywords based on previous searches and the user's input",
          "2. **Previous Queries**: Searches made by users that are stored and used to make suggestions for future queries"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **Example of Bing Auto-Completion**: After entering \"mike garbac\", Bing finally comes up with the correct spelling, showing how it uses previous queries to improve search results"
        ],
        "priority": "MEDIUM",
        "slide_number": 50,
        "slide_file": "s50.png",
        "raw_text": "fe\nSpot Bing Auto-Completion\nes Ns SS » — * On the other hand, Bing does not\naaiil ~. ; even wait for the first character to\nail ma be entered, as seen on the left they\nSing  bogs S| make use of previous queries and\ncoma ie enter some possibilities before the\n< user even types  single character\n1 — eo * After entering “mike garbac” bing\nSE  finally comes up with the correct\n= spelling, see below\nboing  - as}\n= pe ee EE  ol E2252 | yyright Ellis Horowitz, 2011-2022 50\nRe"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  **Querying**: Bing will offer results using corrected spelling and include a link for the user to correct their query",
          "*  **Search Engine Results**: Displaying multiple sources, including images, videos, and links"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  **Query Correction Algorithm**: Bing's algorithm for correcting spelling errors in user queries"
        ],
        "examples": [
          "*  **Bing Search Results**:",
          "*  **Corrected Spelling**: Bing offers corrected spelling suggestions for user queries"
        ],
        "priority": "MEDIUM",
        "slide_number": 51,
        "slide_file": "s51.png",
        "raw_text": "ane mike garbachev - Bing —\n_ : ; : ;\n(Ors|icemm=  2isii=hiQl) As with Google and Yahoo, Bing will\nTonys CV Sep ses Goeth > CVn Manes\n(Dome Rene cs fom Sime Grom Gitaien ,oum-geee-Su-| Offer results using the corrected spelling\nDING mesures ro] and include  link for the user spelling\nWeb Web | images Morey\nMararSoaany Mia Gorbachev - ped, te ee enelpeda\nrae facta Brown ne Gorbacher Facor ues be reois of many pool run\nMoa Gorbchey\nvi Mil Gorbachey:Blonrachy fom Answers.com\nanal Bangshoikoy Images of mike garbachev\npol Tum off ‘bing conm/images/search?q=mike garbachev\nMichael Gorbachev | Facebook\nEB ich! Gert on Pacsook Jon Pattee te comoc wi Mchaa\nWalaccoockeonecpaMicnecGomeche\nMy County andthe Word, by Michael Gorbachev\nonlenibaaonbedonbgihe\nMin Gorbachev -Boaraphy -Nobepizeora |\nmichael gorbachev | crazymonk.org pene\n= Cor\nCopyright Ellis Horowitz, 2011-2022 51\nee"
      },
      {
        "lecture": "querying",
        "concepts": [
          "*  Statistical measure for evaluating processes that produce lists of possible responses to samples of queries",
          "*  Mean Reciprocal Rank (MRR) is a statistical measure"
        ],
        "definitions": [
          "*  Reciprocal rank: the multiplicative inverse of the rank of the first correct answer"
        ],
        "formulas": [
          "*  MRR = ∑(1/rank) / n, where n is the number of queries and rank is the reciprocal rank of each query response"
        ],
        "algorithms": [],
        "examples": [
          "*  Example 1: A system tries to translate English words to their plurals, with three sample queries and their corresponding results (cats, tori, virus)"
        ],
        "priority": "MEDIUM",
        "slide_number": 52,
        "slide_file": "s52.png",
        "raw_text": "* The mean reciprocal rank is  statistical measure for evaluating any process that produces  list\nof possible responses to  sample of queries, ordered by probability of correctness.\nThe reciprocal rank of  query response is the multiplicative inverse of the rank of the first\ncorrect answer\n* The mean reciprocal rank is the average of the reciprocal ranks of results for  sample of\njueries 1 1\nMRR = — 5° —_.\nQ| rank;\n+ For example, suppose we have the following three sample queries for  system that tries to\ntranslate English words to their plurals. In each case, the system makes three guesses, with the\nfirst one being the one it thinks is most likely correct:\nQuery Results Correct response Rank Reciprocal rank\ncat catten, cati, cats cats 3 3\ntori torii, tori, toruses _ tori 2 1/2\nvirus viruses, Virii, viri viruses 1 1\nthe mean reciprocal rank as (1/3 + 1/2 + 1)/3 = 11/18 or about 0.61.\nCopyright Ellis Horowitz, 2011-2022 52\nee"
      }
    ],
    "se-basics": [
      {
        "lecture": "se-basics",
        "concepts": [
          "1. KEY CONCEPTS:"
        ],
        "definitions": [
          "2. DEFINITIONS:"
        ],
        "formulas": [
          "3. FORMULAS/EQUATIONS:"
        ],
        "algorithms": [
          "4. ALGORITHMS:"
        ],
        "examples": [
          "5. EXAMPLES:"
        ],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "slide_001.png",
        "raw_text": "Search Engine History and Basics\nCopyright Ellis Horowitz 2013-2022"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  **Evolution of Search Engines**: The development of search engines from 1991 to present day",
          "*  **Non-Web Search Engines**: Early search engines that were not web-based (Gopher, Archie, Veronica)",
          "*  **Web-Based Search Engines**: Transition to web-based search engines in the mid-to-late 1990s"
        ],
        "definitions": [
          "*  **Indexing**: The process of creating a database of web pages for searching",
          "*  **Query Matching**: The process of matching user queries with relevant web pages"
        ],
        "formulas": [],
        "algorithms": [
          "*  **Indexing and Query Processing**: Not explicitly described, but implied to be a key aspect of search engine development",
          "*  **Link-Based Ranking**: Mentioned as a feature of Google's ranking algorithm"
        ],
        "examples": [
          "*  **Early Search Engines (1991-1994)**: List of specific early search engines (Gopher, Archie, Veronica, Wanderer, ALIWeb, Excite, etc.)",
          "*  **Successful Search Engines (1995-2000s)**: List of successful search engines that emerged in the late 1990s and early 2000s (Infoseek, Metacrawler, SavvySearch, LookSmart, Inktomi, HotBot, AskJeeves, Goto)"
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "slide_002.png",
        "raw_text": "+ 1991\n— Gopher, Archie, Veronica early search engines, non-web\n+ 1993\n— Wanderer,\n—  ALIWeb\n— Excite http://www.excite.com/ powerful indexing\n+ 1994\n— Galaxy http://www.galaxy.com/ Early searchable directory\n— Yahoo http://www.yahoo.com/ Sophisticated searchable directory\n— Lycos http://www.lycos.com/ Improved query matching\n—  WebCrawler http://www.webcrawler.com/ Includes full text of pages\n— Alta Vista http://www.altavista.com/  large index\n+ 1995\n— Infoseek http://www.infoseek.com/ included in Netscape Navigator\n—  Metacrawler http://www.metacrawler.com/ combines results from other engines\n—  SavvySearch http://www.savvysearch.com/ combines results from other engines\n— LookSmart http://www.looksmart.com convenient organization\n+ 1996\n— Inktomi http://www.inktomi.com /  large index using commodity hardware\n— HotBot, http://www.hotbot.com/  large index\n+ 1997\n— AskJeeves http://www.askjeeves.com/ fancy query processing\n+ 1998\n— Goto http://www.goto.com/ introduces auctioning of positions\n— Google http://www.google.com ranking using content and links\n+ Today there are hundreds of search engines, many are specialized\n+ See < href=http://www.searchenginehistory.com/>Search Engine History</a><br>\n+ Avery long web page describing the history of search Grgineighith llitsdbgoudthinR@ | 1-2022 2\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* The Internet's early search tools were developed in the late 1980s and early 1990s",
          "* Archie, Veronica, and Jughead were three major search tools that emerged during this period",
          "* The Gopher protocol was a TCP/IP application layer protocol designed for distributing, searching, and retrieving documents over the Internet"
        ],
        "definitions": [
          "* FTP (File Transfer Protocol) - an anonymous method of transferring files over the internet",
          "* Archie - a tool that assembled lists of files available on many FTP servers",
          "* Veronica - a search tool for text files available through Gopher servers",
          "* Jughead - a search tool for text files available through Gopher servers (not explicitly defined in the slide, but implied)",
          "* The Gopher protocol - a TCP/IP application layer protocol designed for distributing, searching, and retrieving documents over the Internet"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* In 1990, Alan Emtage, P. Deutsch, et al of McGill Univ. developed Archie",
          "* Veronica and Jughead were developed in 1993 to search names of text files available through Gopher servers"
        ],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "slide_003.png",
        "raw_text": "School of Engineering Archie, Veronica, Gopher\n¢ By late 1980’ many files were available by anonymous\nFTP.\n* In 1990, Alan Emtage, P. Deutsch, et al of McGill Univ.\ndeveloped Archie (short for “archives”\n— Assembled lists of files available on many FTP servers.\n— Allowed regex search of these file names.\n* In 1993, Veronica and Jughead were developed to search\nnames of text files available through Gopher servers\n— The Gopher protocol is  TCP/IP application layer protocol designed\nfor distributing, searching, and retrieving documents over the Internet.\nStrongly oriented towards  menu-document design\n— The Gopher ecosystem is often regarded as the effective predecessor of\nthe World Wide Web\nCopyright Ellis Horowitz, 2011-2022 3"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Statistical analysis of word relationships to make searching more efficient",
          "*  Use of statistical analysis in search software development"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Excite came from the project Architext, which was started in February 1993 by six Stanford undergrad students",
          "*  Excite was bought by @Home for $6.5 billion and later filed for bankruptcy"
        ],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "slide_004.png",
        "raw_text": "\\/\nexcite\n¢ Excite came from the project Architext, which was started in\nFebruary, 1993 by six Stanford undergrad students.\n— They had the idea of using statistical analysis of word relationships to\nmake searching more efficient.\n— They were soon funded, and in mid 1993 they released copies of their\nsearch software for use on web sites.\n* Later developments\n— Excite was bought by  broadband provider named @Home in January,\n1999 for $6.5 billion, and was named Excite@Home. In October,\n2001 Excite@Home filed for bankruptcy. InfoSpace bought Excite from\nbankruptcy court for $10 million\n— www.excite.com still exists as  portal\nCopyright Ellis Horowitz, 2011-2022 4"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* World Wide Web Wanderer (marked as )",
          "* Potential for general-purpose WWW search engine (marked as )"
        ],
        "definitions": [
          "* Web crawler: A Perl-based program that crawled the web and collected URLs (marked as )"
        ],
        "formulas": [],
        "algorithms": [
          "* None explicitly mentioned, but the process of crawling the web and collecting URLs can be considered an algorithmic process (marked as )"
        ],
        "examples": [
          "* World Wide Web Wanderer as a concrete example of an early web robot (marked as )"
        ],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "slide_005.png",
        "raw_text": "¢ In June 1993 Matthew Gray while at MIT introduced the World Wide Web\nWanderer.\n— Initial goal was to measure the growth of the web by counting active web-servers. He\nsoon upgraded the software to capture actual URL's. His database became known.as the\nWandex.\n¢ The World Wide Web Wanderer was  Perl-based web crawler that was first\ndeployed in June 1993\n¢ Matthew Gray now works for Google.\n¢ While the Wanderer was probably the first web robot, and, with its index,\nclearly had the potential to become  general-purpose WWW search engine\nit never went that far\n¢ The Wanderer charted the growth of the web until late 1995.\nCopyright Ellis Horowitz, 2011-2022 5\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  ALIWEB (Archie-Like Indexing of the Web) - a Web search engine created in 1993",
          "*  Importance of meta information in indexing web pages"
        ],
        "definitions": [
          "*  Crawling: collecting data from web pages",
          "*  Bandwidth: the amount of data transferred over a network",
          "*  ALIWEB: an alternative to traditional Web search engines that uses user-submitted meta information for indexing"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Submission of web pages by users with their own page descriptions using ALIWEB",
          "*  Use case where people did not know how to submit their site, highlighting a limitation of ALIWEB"
        ],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "slide_006.png",
        "raw_text": "¢ In November of 1993 Martijn Koster created “Archie-Like Indexing of the\nWeb”, or ALIWEB in response to the Wanderer.\n— Some consider it to be the first Web search engine\n¢ ALIWEB crawled meta information and allowed. users to submit their pages\nthey wanted indexed with their own page description.\n¢ This meant it needed no bot to collect data and was not using excessive\nbandwidth.\n* One downside of ALIWEB was that people did not know how to submit\ntheir site\nCopyright Ellis Horowitz, 2011-2022 6\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "**AltaVista**: A significant search engine in the early days of the web.",
          "**Natural Language Queries**: Allowing users to search using everyday language, rather than specific keywords or syntax.",
          "**Advanced Searching Techniques**: Features that enable more precise and effective searching, such as filtering and sorting results."
        ],
        "definitions": [
          "**Overture**: A company that owned AltaVista and was later acquired by Yahoo!.",
          "**Yahoo! Search**: The search engine developed by Yahoo! using some of the technology from AltaVista."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**AltaVista's features**: The slide describes several examples of AltaVista's innovative features, such as natural language queries, advanced searching techniques, and inbound link checking."
        ],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "slide_007.png",
        "raw_text": "V/  taro\n« AltaVista debut online came during December, 1995. AltaVista brought many\nimportant features to the web scene.\n— They were the first to allow natural language queries\n— They offered advanced searching techniques\n— They allowed users to add or delete their own URL within 24 hours.\n— They even allowed inbound link checking. AltaVista also provided numerous\nsearch tips and advanced search features.\n* Later developments\n— On February 18, 2003, Overture signed  letter of intent to buy AltaVista for\n$80 million in stock and $60 million cash. After Yahoo! bought out Overture\nthey rolled some of the AltaVista technology into Yahoo! Search, and\noccasionally used AltaVista as  testing platform.\nCopyright Ellis Horowitz, 2011-2022 7\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "Lycos search engine",
          "Ranked relevance retrieval",
          "Prefix matching",
          "Word proximity bonuses"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "Lycos' indexing process (mentioned as identifying and cataloging documents)"
        ],
        "examples": [
          "Lycos going public with a catalog of 54,000 documents on July 20, 1994",
          "Lycos reaching 394,000 documents in August 1994",
          "Lycos indexing over 60 million documents by November 1996"
        ],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "slide_008.png",
        "raw_text": "¢ Lycos was designed at Carnegie Mellon University around July of 1994.\nMichael Loren Mauldin was responsible for this search engine and was the\nchief scientist at Lycos Inc in the early years.\n* On July 20, 1994, Lycos went public with  catalog of 54,000 documents.\n— In addition to providing ranked relevance retrieval, Lycos provided prefix matching and\nword proximity bonuses.\n— Lycos' main difference was the sheer size of its catalog: by August 1994, Lycos had\nidentified 394,000 documents; by January 1995, the catalog had reached 1.5 million\ndocuments; and by November 1996, Lycos had indexed over 60 million documents --\nmore than any other Web search engine.\n¢ In October 1994, Lycos ranked first on Netscape' list of search engines\n¢ Lycos has gone through  series of owners, and it still exists as\nwww.lycos.com\nCopyright Ellis Horowitz, 2011-2022 8\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "slide_009.png",
        "raw_text": "(0) infoseek”\n* Infoseek also started out in 1994, founded by Steve Kirsch\n* In December 1995 they convinced Netscape to use them as\ntheir default search engine, which gave them major exposure.\n* One popular feature of Infoseek was allowing webmasters to\nsubmit  page to the search index in real time, which was\nsearch spammer' paradise\n¢ They were the first search engine to sell advertising on  CPM\n(Cost per Thousand) impressions basis\n* Infoseek was bought by Walt Disney Company in 1998\nCopyright Ellis Horowitz, 2011-2022 9"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  **Hierarchical listing**: A way to organize links into a topical hierarchy",
          "*  **Portal**: A website that acts as an entry point for other websites or services (e.g. Email, Finance, Groups)",
          "*  **Search feature**: A way to search through all of the links on the Yahoo homepage"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  **Yahoo's early days**: The story of David Filo and Jerry Yang posting web pages with links on them in 1994.",
          "*  **Yahoo's decline and acquisition**: The fact that Yahoo was purchased by Verizon in 2017 for $4.48 billion after years of decline."
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "slide_010.png",
        "raw_text": "| “YAHOO!\n* In 1994, two Stanford Ph.D. students David Filo and Jerry Yang\nposted web pages with links on them, organized into  topical\nhierarchy.\n* As the number of links began to grow, they developed  hierarchical\nlisting. As the pages become more popular, they developed  way to\nsearch through all of the links.\n¢ Early on all the links on the pages were updated manually rather than\nautomatically by spider or robot and the search feature searched only\nthose links\n* Yahoo home page acted as  portal with Email, Finance, and Groups\nbeing very successful; however after 2000 usage declined\n¢ After many years of decline Yahoo was purchased by Verizon in 2017\nfor $4.48 billion, and it lives on\nCopyright Ellis Horowitz, 2011-2022 10"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "LookSmart was a search engine that competed with Yahoo! Directory in terms of inclusion rates.",
          "Pay-per-click (PPC) business model, where listed sites pay a flat fee per click."
        ],
        "definitions": [
          "PPC (Pay-Per-Click): A business model where advertisers pay for each ad click on their site.",
          "Syndication: The practice of distributing content, such as paid listings, to multiple websites or portals."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "WiseNut search engine was bought by LookSmart in March 2002, but failed to gain traction."
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "slide_011.png",
        "raw_text": "AA\n\\ LookSmart\nWhere To Look For What You Need:\n¢ Looksmart was founded in 1995 in Australia. They competed with\nthe Yahoo! Directory by frequently increasing their inclusion rates\n¢ Later developments\n— In 2002 Looksmart transitioned into  pay per click provider,\nwhich charged listed sites  flat fee per click. They syndicated\nthose paid listings to some major portals like MSN.\n— The problem was that Looksmart became too dependant on\nMSN, and in 2003, when Microsoft announced they were\ndumping Looksmart that basically killed their business model.\n— In March of 2002, Looksmart bought  search engine by the\nname of WiseNut, but it never gained traction\n* See https://en.wikipedia.org/wiki/LookSmart\nCopyright Ellis Horowitz, 2011-2022 11"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Inktomi Corporation: A search engine company that came into existence in 1996",
          "*  Hotbot: The search engine developed by Inktomi",
          "*  Paid inclusion model: A business model where websites pay a fee to guarantee display on certain search terms"
        ],
        "definitions": [
          "*  Spam sites: Websites that are considered irrelevant or unwanted content"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Inktomi's database of spam sites was accidentally made public in 2001, containing over 1 million URLs",
          "*  Inktomi sold out to Yahoo! for approximately $235 million in December 2003"
        ],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "slide_012.png",
        "raw_text": "* The Inktomi Corporation came about on May 20, 1996 with its search engine\nHotbot. Two Cal Berkeley cohorts created Inktomi from the improved technology\ngained from their research\n« Later developments\n— In October of 2001 Inktomi accidentally allowed the public to access their\ndatabase of spam sites, which listed over 1 million URLs at that-time.\n— Inktomi pioneered the paid inclusion model in which  website pays  fee to the\nsearch engine that guarantees the site will be displayed when certain search\nterms are entered\n— The model was nowhere near as efficient as the pay-per-click auction model\ndeveloped by Overture. Licensing their search results also was not profitable\nenough to pay for their scaling costs. They failed to develop  profitable\nbusiness model, and sold out to Yahoo! for approximately $235 million, or\n$1.65  share, in December of 2003.\n*http://searchenginewatch.com/article/2066745/Inktomi-Spam-Database-Left-Open-To-Public\nCopyright Ellis Horowitz, 2011-2022 12\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* Natural language search engine",
          "* Human editors matching search queries",
          "* Subject Specific Popularity"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* The launch of Ask Jeeves in 1997 as a natural language search engine",
          "* The use of DirectHit technology by Ask Jeeves, which proved vulnerable to spam",
          "* The release of Teoma search engine and its clustering technology"
        ],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "slide_013.png",
        "raw_text": "\\ Bocves: 2\n* In April of 1997 Ask Jeeves was launched as  natural language\nsearch engine.\n— Ask Jeeves used human editors to try to match search queries.\n— Ask was powered by DirectHit for  while, which aimed to rank\nresults based on their popularity, but that technology proved too\neasy to spam.\n— In 2000 the Teoma search engine was released, which uses\nclustering to organize sites by Subject Specific Popularity,\nwhich is another way of saying they tried to find local web\ncommunities. In 2001 Ask Jeeves bought Teoma to replace the\nDirectHit search technology.\n— On March 21, 2005 Barry Diller' IAC agreed to acquire Ask\nJeeves for 1.85 billion dollars. [AC owns many popular websites\nlike Match.com, Ticketmaster.com, and Citysearch.com, and is\npromoting Ask across their other properties.\n— In 2006 Ask Jeeves was renamed to Ask.\nCopyright Ellis Horowitz, 2011-2022 13\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* Google is a play on the word Googol, which refers to 1 followed by 100 zeros ()",
          "* A googol is bigger than the number of atoms in the universe ()"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "slide_014.png",
        "raw_text": "Vv Google\n* Google is  play on the word Googol, coined by Milton Sirotta; it refers to\n1 followed by 100 zeros, 10000000.....0\n¢  googol is bigger than the number of atoms in the universe\n* Google was founded by Larry Page and Sergey Brin, two Stanford Univ.\nComputer Science graduate students\n¢ In 1998 they built  prototype system called BackRub, dropped out of\nschool, and tried to attract investors for their new company\n* Google Inc. released  beta version on Sept. 7, 1998\n* www.google.com was officially released on Sept. 21, 1999\nCopyright Ellis Horowitz, 2011-2022 14"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* Algorithmic Yahoo",
          "* Search Era Lycos",
          "* Paid Search Era"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Google",
          "* Overture (goto.com)"
        ],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "slide_015.png",
        "raw_text": "Brief Chronology of Search Engines\nAlgorithmic Yahoo\nSearch Era Lycos\nExcite |\nGopher/Archie/ Alta Visiay\nopher/Archie .\nVeronica - Early Inktomi, Google\nInternet Search begins\nEngines Hotbot Adword and\nAsk Jeeves) Pay-Per-Click\n| Google,\n1991 1993\" 1995) 1996199701998) 1999° 12000\" | 2001 2002 2003.\nOverture (goto.com) is the first - Fi - : —— as\nto combine sponsored (paid) way yeric cack\nsearch results with search results search results\nconventional search results ;\nPaid Search Era\n(paid search became pervasive)"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Search Engine Basic Behavior"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "slide_016.png",
        "raw_text": "Search Engine Basic Behavior\nCopyright Ellis Horowitz 2013-2022"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Providing access to heterogeneous, distributed information that is publicly available on the World Wide Web",
          "*  Information comes in many different formats",
          "*  Most of the information has not been screened for accuracy",
          "*  The World Wide Web as a source of new opportunities in marketing"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Multi-billion dollar business",
          "*  Strains the boundaries of trademark and intellectual property laws"
        ],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "slide_017.png",
        "raw_text": "¢ Providing access to heterogeneous, distributed information\nthat is publicly available on the World Wide Web\n— Information comes in many different formats\n— Most of the information has not been screened for accuracy\n* Miulti-billion dollar business\n¢ Source of new opportunities in marketing\n¢ Strains the boundaries of trademark and intellectual\nproperty laws\n*  source of unending technical challenges\nCopyright Elis Horowitz, 2011-2022 \""
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "Search engine: program designed to help find information stored on computer systems."
        ],
        "definitions": [
          "Web search engine: searches for information on the public Web.",
          "Enterprise search engines: searches on intranets (internal corporate networks).",
          "Personal search engines: searches individual personal computers."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "slide_018.png",
        "raw_text": "¢ “ search engine is  program designed to help find information stored\non  computer system such as the World Wide Web, inside  corporate\nor proprietary network or  personal computer” wikipedia\n— search engine usually refers to  Web search engine, which searches for\ninformation on the public Web.\n— Other kinds of search engine are enterprise search engines, which\nsearch on intranets,\n— personal search engines, which search individual personal computers\nCopyright Ellis Horowitz, 2011-2022 18"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* The User  (element 1)",
          "* The Web  (element 2)",
          "* The Crawler/Spider  (element 3)",
          "* The Indexer  (element 4)",
          "* The Query Processor  (element 6)"
        ],
        "definitions": [
          "* Crawler/Spider : Software program that traverses the web, discovers new pages, and updates existing indexes",
          "* Indexer : Component responsible for creating and maintaining search indexes",
          "* Query Processor : Module that processes user queries, retrieves relevant documents, and generates ranked results"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "slide_019.png",
        "raw_text": "schoot of engineering Basic Web Search Internals\nMajor Elements\nUser  1. The User\n2. The Web\n“ _  3. The Crawler/spider\nva — 4. The Indexer\nNN 5. The Ads\n/ Web spider. 6. The Query Processor\n| | / NAVA Query processor _~~--------..\n; _ Indexes Ad indexes\nCopyright Ellis Horowitz, 2011-2022 19"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  **Spider (a.k.a. crawler/robot)**: builds corpus",
          "*  **Indexer**: creates inverted indexes",
          "*  **Query processor**: serves query results"
        ],
        "definitions": [
          "*  **Corpus**: a collection of web pages built by the spider",
          "*  **Inverted index**: an index created by the indexer to speed up queries"
        ],
        "formulas": [],
        "algorithms": [
          "*  **Spider's process**:",
          "*  **Indexer's process**: creates inverted indexes with various policies (e.g., word stemming, capitalization, support for Unicode)"
        ],
        "examples": [
          "*  N/A (no concrete examples provided on this slide)"
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "slide_020.png",
        "raw_text": "¢ Spider (a.k.a. crawler/robot) — builds corpus\n— Collects web pages recursively\n* For each known URL, fetch the page, parse it, and extract new\nURLs\n* Repeat\n— Additional pages come from direct submissions & other sources\n¢ The indexer — creates inverted indexes\n— Various policies wrt which words are indexed, capitalization, support\nfor Unicode, stemming, support for phrases, etc.\n* Query processor — serves query results\n— Front end — query reformulation, word stemming, capitalization,\noptimization of Booleans, etc.\n— Back end — finds matching documents and ranks them\nCopyright Ellis Horowitz, 2011-2022 20\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* Distributed content creation and linking",
          "* Truth, lies, obsolete information, contradictions on the web"
        ],
        "definitions": [
          "* Semi-structured data storage in tables",
          "* Structured data storage in databases (NS)",
          "* Scale of larger than previous text corpora"
        ],
        "formulas": [
          "Note: Since there are no mathematical formulas, I didn't mark any as ."
        ],
        "algorithms": [],
        "examples": [
          "* Dynamic generation of content on the web",
          "* Growing and expanding nature of web content"
        ],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "slide_021.png",
        "raw_text": "* No design/co-ordination\n| | ¢ Distributed content creation, linking\n¢ Content includes truth, lies, obsolete\ninformation, contradictions...\n* Data is stored in structured\nNS / (databases), semi-structured (tables)...\n| | ¢ Scale larger than previous text corpora\n|  * Growth - still expanding\nIF * Content can be dynamically generated\nThe Web\nCopyright Horowitz 2006-2022 21"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* Diverse user backgrounds and training methods",
          "* Users' difficulty in distinguishing between search bar and URL address field",
          "* Importance of key results being at or near the top due to users rarely using scroll bars",
          "* Diverse access methodologies (high bandwidth connectivity, mobile limitations)",
          "* Poor comprehension of syntax in current search engines",
          "Note that some content might overlap between categories. For instance, \"Diverse user backgrounds and training methods\" could be both a  and a [PRIORITY] item. However, I've tried to categorize each point according to its primary significance for studying."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Chrome conflating search bar with URL address field"
        ],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "slide_022.png",
        "raw_text": "* Diverse in background/training\n— Users sometimes cannot tell the difference between  search bar from the URL\naddress field (Chrome conflates the two)\n— Users rarely use the scroll bar, so key results must be at or near the top\n* Diverse in access methodology\n— Increasingly, high bandwidth connectivity\n— Growing segment of mobile users: limitations of form factor — keyboard,\ndisplay\n¢ Diverse in search methodology\n— Search, search + browse,\n— Average query length ~ 2.5 terms\n— Has to do with what they’re searching for\n* Poor comprehension of syntax\n— Early engines offered rich syntax for queries — Boolean, phrase, etc.\n— Current engines hide these\nCopyright Ellis Horowitz, 2011-2022 22\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  **Types of user intentions** (Informational, Navigational, Transactional)"
        ],
        "definitions": [
          "*  **Informational intent**: want to learn about something (~40%)",
          "*  **Navigational intent**: want to go to that page (~25%)",
          "*  **Transactional intent**: want to do something (web-mediated) (~35%)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  **Informational intent example**: Low hemoglobin",
          "*  **Navigational intent example**: United Airlines",
          "*  **Transactional intent examples**:"
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "slide_023.png",
        "raw_text": "fe\nSchool of Engineering Are Diverse\n¢ Informational — want to learn about something (~40%)\ne.g. Low hemoglobin\n¢ Navigational — want to go to that page (~25%)\ne.g. United Airlines\n¢ Transactional — want to do something (web-mediated) (~35%)\n— Access  service Los Angeles weather\n— Downloads Mars surface images\n— Shop Nikon CoolPix Camera\n¢ Gray areas\n— Finda good hub Car rental in Finland\n— Exploratory search “see what’ there”\nCopyright Ellis Horowitz, 2011-2022 23\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "1. Query processing involves more than just matching query terms with document terms",
          "2. Semantic analysis of queries is a crucial step in search engine functionality"
        ],
        "definitions": [
          "1. Stop words: unnecessary words filtered from the query (e.g. \"the\", \"and\")"
        ],
        "formulas": [],
        "algorithms": [
          "6. Maintaining a user profile"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "slide_024.png",
        "raw_text": "* Query processing involves much more than just matching query terms with document\nterms\n¢ Semantic analysis of the query includes:\n1. Determining the language of the query\n2. Filtering of unnecessary words from the query (stop words)\n3. Looking for specific types of queries, e.g.\n* Personalities (triggered on names)\n* Cities (travel info, maps)\n* Medical info (triggered on names and/or results)\n¢ Stock quotes, news (triggered on stock symbol)\n* Company info...\n4. Determining the user’ location or the target location of the query\n5. Remembering previous queries\n6. Maintaining  user profile\nCopyright Ellis Horowitz, 2011-2022 24\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  - User interface customization (e.g., \"moar Nsagauavesane\")"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  - Customizing bookmarks (e.g., \"weather los angeles ca- 6\")"
        ],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "slide_025.png",
        "raw_text": "ect)\nBB weather los angeles ca- 6\n€\nEE Apps [ CSCIS72HomePage [*) CSCI57 HomePage [\") CSCI351 HomePage [) Ellis Horowitz’ Hom...  Computer Science D. » Cy Other bookmarks\nGoogle | weal 3 | “sis HO sha @\nweather los angeles ca Remove\nweather san francisco Remove\n2 2\nweather  .  . : .\nweather san diego aintain previous queries\nmoar Nsagauavesane : Helps to minimize typing\nAllow users to remove old ones\n69\"\nTemperature | Preciptatn | wind\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Search Engine Basics (SEB) - a fundamental concept in understanding how search engines work",
          "*  Google - one of the most popular search engines"
        ],
        "definitions": [
          "*  Actor - a person who performs in plays, films, or television shows",
          "*  Filmmaker - a person who creates and produces films",
          "*  Golden Globe Awards - awards given to recognize excellence in film and television"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  George Clooney's career as an actor and filmmaker",
          "*  The search engine results page (SERP) for \"George Clooney\" showcasing various news articles, images, and videos"
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "slide_026.png",
        "raw_text": "SC\nFile Edt View History Bookmarks Tool Hep =\nBB george clooney - Googles.  \\\n}  htt google.com/#q=george~cloone  || BB~ Coogie Pweatnrer BA\nGoogle george clooney | 2 | +elis HE QQ. Shae 7) |\nWeb News mages Videos’ -—-Shopoing Mowe Seach tools 2 Fe) .\n— Includes the following:\nLatest news\nin the news : me\nSr }. ae Biography\nGeorge Clooney, Amal Alamuddin Honeymoon in Swe ge\n< New British Home Me Photos\n‘ Us Magazine - 4 hi =¥ ’ .\neyes George Clooney and Arma lamin ave kp the > he sf Basic facts\n>) S)\nPeople: George Clooney' Wedding Cost About $1.6 Million bay born\nYahoo! Voices - 2 0 7 More images: ial\nGeorge Clooney & Amal Alamuddin Could Nab His & Hers Nobel Peace Prizes, Friend matrie\nPredicts\nPeople Mapai -23 hos 2 George Clooney parents\nMore news for george clooney\nGeorge Timothy Clooney is an American career\nactor and flmmaker He has recetved three\n*ineeth Golden Globe Awards for his work as an\nGeorge Clooney - Wikipedia, the free encyclopedia seers eee Anclea mae oscien\nen ikipedia.egwik/George Clooney ~ Wikipedia se ae eee pode\nGeorge Timothy Clooney (bom May 6, 1961) is an American actor and filmmaker. He ties\nFilmography - Talia Balsam - Amal Alamuddin - Nick Clooney Born: May 6, 1961 (age 53), Lexington,\nKy\nGeorge Clooney & Amal Alamuddin Could Nab His & Hers ... Height: 5 11° (1.80 m)\nwn people.com.george-clooney-amalalamuddinaobet-peace-p.... People Spouse: Amal Alamuddin (m. 2014), Talia\n3 hours ago - SEE 26 MORE CLOONEY WEDDING PHOTOS! Subscribe now to Balsam (m, 1989-1993)\nPEOPLES digital edition and get 26 BONUS photos from inside the Siblings: Adeia Clooney\nParents: Nina Bruce Warren, Nick\nGeorge Clooney - IMDb Clooney\n' www.imdb.com/name/nm0000123/ ¥_Internet Movie Database +\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Las Vegas is the most populous city in the U.S. state of Nevada.",
          "*  Las Vegas is officially known as the City of Las Vegas and has a population.",
          "*  The official website for Las Vegas travel information is VEGAS.com.",
          "*  Las Vegas has various hotels, shows, casinos, restaurants, maps, and attractions."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "slide_027.png",
        "raw_text": "Sey =<=)\nBi es vegas Google Search\n€ C@ fi & https:/www.google.com/search?q=Ias + vegas&oq=las + vegas&aqs=chrome..69i57j015.1920j0j88source #AG=\nEi Apps [1 CSCIST2HomePage [°) CSCISTL HomePage [) CSCI351 HomePage [Elis Hrowits' Hom... fi] Computer Science D. » Ci Other bookmarks\nGoogle las vegas  Kw +lis GH Share ())\nIncludes the following:\nWeb Ima Vid Ac ch  2 2 ws .\n— Official site\n| Map\nLas Vegas - VEGAS.com™ 1\ntas Vegas - VES = Essential facts\nLas Vegas Shows, Hotels and more. The Oficial VEGAS Travel Site™ Las\\Ve9s <nrise\n: : LAS cps scumis founded\nLas Vegas Air + Hotel -Las Vegas Hotels - Headliners and Concerts A. Rig? fo Raradise\nHenderson\nAttractions In Las Vegas - TravelNevada.com\n‘www travelnevada.com/ * haga ecors hag weather\nOrder Your Free Visitor' Guide & Plan Your Vacation To Las Vegas! Mencia oie .\nravel Neva 23 Google time\nLas Vegas\nLas Vegas 5 Day Room Sale - SouthPointCasino.com ; : population\n35) ww, southpointcasino,com ~\nSouth Point Hotel Casino Spa Rates From $35 Sun-Thurs $60 Fri. & Sat Follow\nRestaurants - Hotel Packages & Promos - Nightlife & Entertainment - Casino\nLas Vegas, oficially the City of Las Vegas\nand often known as simply Vegas, is the\n(Official City of Las Vegas Web Site) most populous city in the U.S. state of\nww lasvegasnevada gov! ~ Las Vega Nevada and the county seat of Clark\nThe City of Las Vegas (Oficial Government Site) .. City of Las Vegas: Serving You County. Wikipedia\nOnline Rather Than In Line, Photo: Downtown Las Vegas\nFounded: May 15, 1905\nLas Vegas Hotels, Shows, Casinos, Restaurants, Maps and ... Area: 135.9 sq miles (362 km’)\nwww.lasvegas.com/ + Weather: 87° (31°C), Wind  at 5 mph\nYour official What happens in Vegas, stays in Vegas resource. Plan hotels and things te (8 km/h), 5% Humidity\ndo for your trip on the only oficial website of Las Vegas Local time: Thursday 9:32 AM\nShows & Events - Las Vegas Hotels - Air + Hotel Packages - Special Offers & Deals penuteateneroe ana snnan,\n__——"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Hotel website structure: main pages include map, address, phone number, price of room, photos, features & amenities, directions, make reservation, special offers.",
          "*  Online booking process: includes searching hotel website, selecting dates, and making a reservation."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Sheraton Times Square Hotel website example: illustrating main pages, features, and amenities.",
          "*  Sofitel New York Hotel in Manhattan near Times Square: another hotel website example."
        ],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "slide_028.png",
        "raw_text": "Se)\nsheraton times square ho!\n€ 2 & fi | https://www.google.com eg 7 jols # A&\nEE Apps\nGoogle | steratontimes square hotel nyc Mo | seis HO sae @  .\n- ncludes the following:\nWeb sa ing ach 2 2 Main hotel website\n— Map\nAddress\nSheraton™ New York Hotel - Official Site - Our Best Rates Be Gan ee 4\nvine sheraton convTimesSquare ~ aus Phone number\nGuaranteed. Book Now! KW\nRoseland\n: ' 5s PEON, DLO Price of  room\n9 811 7th Avenue, New York, NY 50 Stay We. . .\nPhotos Features & Amenities ste nn ck Son oe Directions\nMake  Reservation Special Offers\n; Sheraton New\nNYC Times Square Hotel - sofitel-new-york.com .\nwww sofitel-new-york.com! +\nSofitel New York Hotel in Manhattan near Times Square. Book Direct Now! York Times\nMake Your Reservation - Enjoy Business @ Sofitel - Sumptuous Terrace Suites Square Hotel\nSheraton New York Times Square Hotel: Hotel Near Time ae\nwww. sheratonnewyork.com! +\nThe Sheraton New York Times Square Hotel is located between Central Park and\nTimes Square, and offers newly renovated spaces and sophisticated\n3.2 KAA 141 Google reviews - Write  review - $2390\n@) 811 7th Ave, New York, NY 10019 LLNS -\n(212) 581-1000\nPhotos & Videos - Sheraton New York Times\nSheraton Times Square Hotel\nSheraton New York Times Square Hotel - Starwood Hotel... sheratonnewyork reservations.com/ +\nwww.stanwoodhotels.com/sherato... ~  Hotels & ide Book Sheraton Times Square\nkt 8 For Limited Offers - Reserve Today!\nOO"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "*  Google Web History: a feature that allows users to view their search history",
          "*  Personalization of search results: only the user can see their own search history"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Viewing search history on Google Web History page",
          "*  Filtering search history by category (e.g., Today's searches, Yesterday's searches)"
        ],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "slide_029.png",
        "raw_text": "Sr)\nGoogle-Web Histoy\n€  fi & https://history.google.com/history/lookup?hl=en #AHE\nEE Apps [) CSCIS72 HomePage [\") CSCIS7L HomePage [\") CSCI351 Home Page [\") Ellis Horowitz’ Hom... J} Computer Science D. » (5 Other bookmarks\nGoogle [Beacweotisoy RR ome @ They claim that only  can see\nmy history;\nWeb History 1 Only you can see your history om  have issued  total of 18,960\nSS ,\nqueries;\nAll History Hourly search activity Daily search activity Web Activity\nWk SMTWTES\n39 10111213 4 :\nNow ——_——_——__— Graphs show my queries by\nShoopina 22 23 24 26 26 27 28 .\nShopei tt ks hour and by week;\nAds 122 4 6 8 10122 4 6 8 10 Sun Mon Tue Wed Thu Fri Sat =\nVideos Total Google searches: 18960\nap\nBlog  can view my Web queries as\nBooks = = distinct from my Image queries\n“ Today or my News queries, etc\nFinance Searched for my entire google search history 7:28am\n“= Searched for dentists beverly hls 79am  great deal about us!\nSearched for weather los angeles c3 6:27am\nYesterday\nSearched for google filetype:ppt 10:31pm\nGoogle - msu.edu 41pm\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* Google's dominance in the search engine revenue market",
          "* Baidu, Yahoo, and Bing's revenue trends over the years"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Yahoo's revenue fluctuations from 2013 to 2021",
          "* Baidu's revenue growth from 2014 to 2021"
        ],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "slide_030.png",
        "raw_text": "¢ The search engine industry is 20+ years old, having started with\nWebCrawler and Lycos in 1994 who sold banner ads as their business\nmodel\n¢ Search engine revenue today\n— Google: 2021:$257 Billion; 2020: $181 Billion; 2019: $162 Billion; 2018: $116\nBillion; 2017: $109 Billion; 2016: $90 Billion; 2015: $74.5 Billion; 2014: $66 Billion;\n2013: $37 Billion\n— Baidu: 2021: $31 Billion; 2020: $16.4 Billion; 2019: $15 Billion; : $11.3 Billion; 2017:\n$13 Billion; 2016: $10.1 Billion; 2015: $10.2 Billion; 2014: 8.0 Billion\n— Yahoo: 2021: 5.2Billion; 2019: 6.97Billion; 2018: 3.03 Billion; 2017: 3.0 Billion;\n2016: 2.98 Billion;2015: $4.9 Billion; 2014: 4.6 Billion; 2013: 4.6Billion\n— Bing: 2020 $7.74 Billion; 2019: $7.63 Billion; 2018: $7.01 Billion\n* Microsoft says that in Q1 2016 Bing became profitable\nCopyright Ellis Horowitz, 2011-2022 30\nccc"
      },
      {
        "lecture": "se-basics",
        "concepts": [
          "* Google's dominance in search engine market",
          "* Anti-Trust violations",
          "* Importance of maintaining a large index of the web"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Google's sweetheart deal with Apple",
          "* Other search engines like DuckDuckGo relying on Bing's index"
        ],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "slide_031.png",
        "raw_text": "¢ The US is suing Google for anti-Trust violations\n— Google claims it has strong competition in search!!!\n— Google has sweetheart deals with Apple andpays Apple $8+\nBillion/year to be their default search engine\n* Google maintains the largest index of the web\n— Some websites actually deny access to crawlers other than Google\nand Bing as these other crawlers bring in little traffic and consume\nserver cycles\n— Only Google and Bing have the resources to maintain such  large\nindex, e.g. DuckDuckGo no longer crawls the web and uses Bing’\nindex\nCopyright Ellis Horowitz, 2011-2022 31\nccc"
      }
    ],
    "se-evaluation": [
      {
        "lecture": "se-evaluation",
        "concepts": [
          "Search Engine Evaluation",
          "Information Retrieval (IR)",
          "Query Understanding",
          "Ranking Algorithms"
        ],
        "definitions": [
          "**Evaluation metrics**: Measures used to assess the performance of a search engine, e.g., precision, recall, F1-score.",
          "**Relevance**: The degree to which a document is relevant to a user's query.",
          "**Precision**: The ratio of true positives (relevant documents) to total number of retrieved documents."
        ],
        "formulas": [],
        "algorithms": [
          "**Ranking Algorithm**: A step-by-step process for ranking documents based on relevance and other factors."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "slide_001.png",
        "raw_text": "Search Engine Evaluation"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "*  Evaluation metrics for search engines",
          "*  Search result quality guidelines",
          "*  Using log files for evaluation",
          "*  Elements of good search results"
        ],
        "definitions": [
          "*  Precision/recall: Measures of relevance and recall in information retrieval",
          "*  Mean Average Precision (MAP): A metric to evaluate the ranking quality of a search engine",
          "*  Harmonic Mean (HM) and Measure: Metrics for evaluating multiple performance measures"
        ],
        "formulas": [
          "*  Mean Average Precision (MAP): P(R|q) = 1/N ∑ [p(r) * rel(r)] where N is the number of relevant documents, p(r) is the precision at rank r, and rel(r) is the relevance of the document at rank r",
          "*  Harmonic Mean (HM): HM = 2 * MAP * Precision"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "slide_002.png",
        "raw_text": "¢ Defining precision/recall\n¢ Mean Average Precision\n¢ Harmonic Mean and  Measure\n* Discounted Cumulative Gain\n¢ Elements of Good Search Results\n* Google' Search Quality Guidelines\n* Using log files for evaluation\n° A/ Testing\nCopyright Ellis Horowitz, 2011-2022 2"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "*  Search Engine Evaluation"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Searching for:"
        ],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "slide_003.png",
        "raw_text": "- oo\n@ Bing It On - Take the Bing  . . . .\nBEC caeniton som 2 az, - This site is no longer active, but we\nEE Apps ge Bookmarks [}\nBing It\nTry it rself!\nlb Bing vs Google  Ht yoursell:\nhere are some queries:\nSo ee oe - ac versus de current\n- best bottled water\n& - worst hotel inSanta Monica\nOr try these trending searches - how many gears do  need on  bicycle\nKevin Magnussen | Mr Ful WWE | Hillary Clinton acs - Clint Eastwood’ best movie\nCopyright Ellis Horowitz 2011-2022\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. Measuring search engine quality",
          "2. Importance of evaluating search engines"
        ],
        "definitions": [
          "1. **Precision**: # (relevant items retrieved) divided by #(all retrieved items)",
          "2. **Recall**: # (relevant items retrieved) divided by #(all relevant items)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "slide_004.png",
        "raw_text": "¢ How do we measure the quality of search engines?\n¢ Precision = #(relevant items retrieved)\ndivided by\n#(all retrieved items)\n¢ Recall = #(relevant items retrieved)\ndivided by\n#(all relevant items)"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "Relevant vs. Irrelevant information",
          "True Positive (tp), False Positive (fp), True Negative (tn), and False Negative (fn)",
          "Precision, Recall, and Accuracy as measures of classification performance"
        ],
        "definitions": [
          "**True Positive (tp)**: Correctly identified relevant information",
          "**False Positive (fp)**: Incorrectly identified irrelevant information",
          "**True Negative (tn)**: Correctly identified irrelevant information",
          "**False Negative (fn)**: Incorrectly identified relevant information"
        ],
        "formulas": [
          "Precision = tp / (tp + fp)",
          "Recall = tp / (tp + fn)",
          "Accuracy = (tp + tn) / (tp + fp + fn + tn)"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "slide_005.png",
        "raw_text": "Pe TRelevant | Noneevant\nRetrieved True positive (tp) False positive (fp)\nNot retrieved False negative (fn) True negative (tn)\nPrecision = tp/(tp + fp)\nRecall = tp/(tp + fn)\n= The accuracy of an engine is defined as:\nthe fraction of these classifications that are correct\n(tp + tn) / (tp + fp + fn + tn)\nFor web applications,\nPrecision is more important than Recall\nCopyright Ellis Horowitz, 2011-2022 5\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "Set of relevant documents",
          "Set of retrieved documents",
          "Precision and Recall metrics"
        ],
        "definitions": [
          "Relevant documents: a set of documents that are relevant to the search query (implied, not explicitly stated)",
          "Retrieved documents: a set of documents returned by the search system or algorithm (implied, not explicitly stated)"
        ],
        "formulas": [
          "Precision = |A| / (|A| + |AN B|)",
          "Recall = |A| / |AN B|"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "slide_006.png",
        "raw_text": "is set of relevant documents,\n; . you may not be able\nis set of retrieved documents to see them, but\nand  have  bar\nRelevant Non-Relevant over them and it\n.  Hp... AO PH. denotes the\nRetrieved ANB ANB lemem set\nNot Retrieved ANB ANB\nJAN BI\nRecall = ———\n| A|\n, |AN B|\nPrecision = ———\nhttps://en.wikipedia.org/wiki/Precision_and_recall\nCopyright Ellis Horowitz, 2011-2022 6\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **Recall vs. Precision**: High recall can lead to low precision ()",
          "2. The relationship between precision and number of documents retrieved: \"precision decreases as the number of docs retrieved (or recall) increases\" ()"
        ],
        "definitions": [
          "1. **Recall**: Not explicitly defined, but implied as a measure of how well a system retrieves relevant documents ()",
          "2. **Precision**: Not explicitly defined, but implied as a measure of how accurate the retrieved documents are ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. Viewing multiple pages of Google results often does not improve precision at all ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "slide_007.png",
        "raw_text": "¢ You can get high recall (but low precision) by\nretrieving all docs for all queries!\n— arather foolish strategy\n¢ In  good system, precision decreases as the\nnumber of docs retrieved (or recall) increases\n— This is not  theorem, but  result with strong empirical\nconfirmation\n— E.g. viewing multiple pages of Google results often does\nnot improve precision at all\nCopyright Ellis Horowitz, 2011-2022 7"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* There are three Pythagorean means: arithmetic mean, geometric mean, harmonic mean",
          "* These means are useful in analyzing data, with each having different applications and interpretations"
        ],
        "definitions": [
          "* Arithmetic mean: not explicitly defined, but mentioned as a well-known concept",
          "* Geometric mean: the nth root of the product of numbers",
          "* Harmonic mean: strongly tends toward the least element of the list, making it useful in search engine results analysis"
        ],
        "formulas": [
          "* Geometric mean formula: nth-root( product of numbers ) = √[n] (product of numbers)",
          "* Formula for harmonic mean calculation: (1/number 1 + 1/number 2 + ... + 1/number n) / (n-1) = 1/(sum of reciprocals / (n-1))"
        ],
        "algorithms": [
          "2. Take the nth root of the product",
          "3. Take the reciprocal of the result"
        ],
        "examples": [
          "* Calculate the arithmetic mean for the numbers 3, 6, 9, and 12: (3+6+9+12)/4 = 7.5",
          "* Calculate the geometric mean for the numbers 3, 6, 9, and 12: nth-root(1944) = 6.64",
          "2. Take the reciprocal of the result: 1/.17 = 5.88"
        ],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "slide_008.png",
        "raw_text": "¢ There are three Pythagorean means\n— 1. arithmetic mean, 2. geometric mean, 3. harmonic mean\n— of course we all know how to compute the arithmetic mean\n— the geometric mean is the nth root of the product of  numbers\n¢ The harmonic mean tends strongly toward the least element of the list making it\nuseful in analyzing search engine results\n¢ To find the harmonic mean of  set of  numbers\n1. add the reciprocals of the numbers in the set\n2. divide the sum by\n3. take the reciprocal of the result\n* eg. for the numbers 3, 6, 9, and 12\n— The arithmetic mean is: (3+6+9+12)/4 = 7.5\n— The geometric mean is: nth-root(3*6*9* 12) = 4\"-root(1944) = 6.64\n— The harmonic mean is: (1/3+1/6+1/9+1/12)=(.33+.16+.11+.08)/4=0.17 and 1/0.17 = 5.88\nCopyright Ellis Horowitz, 2011-2022 8\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **** F-score (or F-measure): a harmonic mean of precision and recall used to evaluate algorithms and systems",
          "2. **** Harmonic mean: emphasizes the importance of small values, unlike arithmetic mean which is affected by outliers"
        ],
        "definitions": [
          "1. **** Precision: (no definition provided, but mentioned as one of the components of F-score)",
          "2. **** Recall: (no definition provided, but mentioned as one of the components of F-score)",
          "3. **** F-measure: a measure that combines precision and recall",
          "4. **** F-score: another name for the F-measure"
        ],
        "formulas": [
          "1. **** F = 2RP / (R + P): formula for calculating F-score",
          "2. **** Fg = ((α+1)RP / (R + αP)): more general form of F-measure with parameter α"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "slide_009.png",
        "raw_text": "¢ The harmonic mean of the precision and the recall is often\nused as an aggregated performance score for the evaluation of\nalgorithms and systems: called the\nF-score (or F-measure).\n¢ Harmonic mean of recall and precision is defined as\nF= 1 —_ _2RP\n“~~ ll 1 —\ns(a+5) (R+P)\n— harmonic mean emphasizes the importance of small\nvalues, whereas the arithmetic mean is affected more by\noutliers that are unusually large\n¢ More general form of F-Measure\n— fis  parameter that controls the relative\nimportance of recall and precision\n2 2\nFg = (6° +1)RP/(R+ BP)\nCopyright Ellis Horowitz, 2011-2022 9\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "**Recall**: The ratio of relevant items retrieved to all relevant items.",
          "**Precision**: The ratio of relevant items retrieved to all items retrieved.",
          "**Relevant documents**: Documents that are actually relevant to the query or task at hand."
        ],
        "definitions": [
          "**Recall (#/6)**: The number of relevant documents retrieved out of a total of 6 (example used in the slide).",
          "**Precision (#/4)**: The number of relevant documents retrieved out of a total of 4 (example used in the slide)."
        ],
        "formulas": [
          "**Recall = # Relevant items Retrieved / Total # Relevant Items**",
          "**Precision = # Relevant items Retrieved / Total # Items Retrieved**"
        ],
        "algorithms": [],
        "examples": [
          "**Bing vs. Google**: An example used to compare the recall and precision of two search engines.",
          "**3/6 vs. 1/2**: Examples used to illustrate different ratios for recall and precision."
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "slide_010.png",
        "raw_text": "fe\nSchool of Engineerin ° eye\n_ at Fixed Positions\n[| [| [| [| [| [| = the relevant documents\nSteps\nRecall:\n1/6=0.17\nPrecision: ; e.g.\nos = SUSBBBBUUUB\n/ 8 Google\nRecall: Recall 0.17 0.17 0.33 0.5 0.67 0.83 0.83 0.83 0.83 1.0 Result\n1/6=0.17 Precision 1.0 0.5 0.67 0.75 0.8 0.83 0.71 0.63 0.56 0.6\nPrecision:\n1/2=0.5\ne.g.\nRecall: . Bing\nco, fore? = | JU UU,\naon: Recall 0.0 0.17 0.17 0.17 0.33 0.5 0.67 0.67 0.83 1.0\nPrecision 0.0 0.5 0.33 0.25 0.4 0.5 0.57 0.5 0.56 0.6\nRecall:\n3/6 =0.5\nPrecision:\n3/4 = 0.75 Recall=#RelevitemsRetr/allRelevitems\nPrec=#RelevitemsRetr/allltemsRetr\nCopyright Ellis Horowitz 2011-2022\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Ranking #1 and #2:",
          "* Recall and Precision:"
        ],
        "definitions": [],
        "formulas": [
          "* (Ranking #1): (1.0 + 0.67 + 0.75 + 0.8 + 0.83 + 0.6) /6 = 0.78",
          "* (Ranking #2): (0.5 + 0.4+0.5 + 0.57 + 0.56 + 0.6) /6 = 0.52"
        ],
        "algorithms": [],
        "examples": [
          "+ Recall scores: 0.17, 0.17, 0.33, 0.5, 0.67, 0.83, 0.83, 0.83, 0.83, 1.0",
          "+ Precision scores: 1.0, 0.5, 0.67, 0.75, 0.8, 0.83, 0.71, 0.63, 0.56, 0.6"
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "slide_011.png",
        "raw_text": "| | | LJ []  = the relevant documents\ncomputes the Ranking #1 § [] | | | [ [ [ [] Bg\nsum of the Recall 0.17 0.17 0.33 0.5 0.67 0.83 0.83 0.83 0.83 1.0\nprecisions of Precision 1.0 0.5 0.67 0.75 0.8 0.83 0.71 0.63 0.56 0.6\nthe relevant\ndocuments\nwre? == (JE UIUBSSUSB\nRecall 0.0 0.17 0.17 0.17 0.33 0.5 0.67 0.67 0.83 1.0\nPrecision 0.0 0.5 0.33 0.25 0.4 0.5 0.57 0.5 0.56 0.6\nRanking #1: (1.0 + 0.67 + 0.75 + 0.8 + 0.83 + 0.6) /6 = 0.78\nRanking #2: (0.5 + 0.4+0.5 + 0.57 + 0.56 + 0.6) /6 = 0.52\nConclusion: Ranking #1 for this query is best\nCopyright Ellis Horowitz, 2011-2022 ll\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Average precision across multiple queries for relevant documents (  )",
          "* Ranking #1 docs: average precision calculation (  )"
        ],
        "definitions": [
          "* Precision: not explicitly defined, but implied as the ratio of relevant documents to total documents (  )",
          "* Recall: not explicitly defined, but implied as the ratio of relevant documents retrieved to total relevant documents (  )"
        ],
        "formulas": [
          "Reca 0.2 02 04 04 04 06 06 06 08 1.0 5 +.4+.43)/8 = 0.55 (  )"
        ],
        "algorithms": [
          "Note: There are no explicit algorithms or step-by-step processes mentioned in this slide, so I did not mark any content as . Also, there are no concrete examples or case studies provided, so I did not mark any content as [EXAMPLE]."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "slide_012.png",
        "raw_text": "[  [ LJ] [ = relevant documents for query 1\nAverage precision across\nthe two queries for relevant\nRanking #1 docs is:\n(1+ .67+.54+ 444+ 5+\nReca 0.2 02 04 04 04 06 06 06 08 1.0 5 +.4+.43)/8 = 0.55\nPrecision 1.0 0.5 0.67 05 04 0.5 0.43 0.38 0.44 0.5\n[ [ [ = relevant documents for query 2\nene? || BUUBUBUUU\nRecall 0.0 0.33 0.33 0.33 0.67 0.67 1.0 1.0 1.0 1.0\nPrecision 0.0 0.5 0.33 0.25 0.4 0.33 0.43 0.38 0.33 0.3\nCopyright Ellis Horowitz, 2011-2022 12\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **Mean Average Precision (MAP)** : A measure of the effectiveness of a search system that averages the average precision scores for each query."
        ],
        "definitions": [
          "1. **Average Precision (AveP(q))** : The area under the precision-recall curve, representing the ratio of relevant documents to total documents at different recall levels.",
          "2. **Relevance Judgments** : Assessments of which documents are relevant to a particular query."
        ],
        "formulas": [
          "1. **MAP Formula** : MAP = (1/n) \\* Σ AveP(q)"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "slide_013.png",
        "raw_text": "¢ Mean average precision (MAP) for  set of queries is the\nmean of the average precision scores for each query.\nve, AveP(q)\nMAP = OQ\nwhere  is the number of queries\n¢ Summarize rankings from multiple queries by averaging\naverage precision\n¢ This is the most commonly used measure in research papers\n« Assumes user is interested in finding many relevant documents\nfor each query\n* Requires many relevance judgments in text collection\nCopyright Ellis Horowitz, 2011-2022 13\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "Information Retrieval Evaluation",
          "Recall and Precision metrics for evaluating search results"
        ],
        "definitions": [
          "**Recall**: The proportion of relevant documents retrieved out of all relevant documents ( exact formula not provided )",
          "**Precision**: The proportion of relevant documents retrieved out of all documents retrieved ( exact formula not provided )"
        ],
        "formulas": [
          "Recall = number of relevant documents retrieved / total number of relevant documents",
          "Precision = number of relevant documents retrieved / total number of documents retrieved"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "slide_014.png",
        "raw_text": "§ | = relevant documents for query 1\nove BB UJSUUBB\nRecall 0.2 0.2 04 04 04 06 06 06 08 1.0\nPrecision 1.0 0.5 0.67 05 04 0.5 0.43 0.38 0.44 0.5\n8 § 8 = relevant documents for query 2\nere? | | UUBUBUUU\nRecall 0.0 0.33 0.33 0.33 0.67 0.67 1.0 1.0 1.0 1.0\nPrecision 0.0 0.5 0.33 0.25 0.4 0.33 0.43 0.38 0.33 0.3\naverage precision query 1 = (1.0+0.67+0.5 + 0.44 + 0.5)/5 = 0.62\naverage precision query 2 = (0.5 + 0.4 + 0.43)/3 = 0.44\nmean average precision = (0.62 + 0.44)/2 = 0.53\nCopyright Ellis Horowitz 2011-2022\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Mean Average Precision (MAP)"
        ],
        "definitions": [
          "* Assuming precision of zero for a relevant document that never gets retrieved is \"reasonable\""
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "slide_015.png",
        "raw_text": "=Mean Average Precision (MAP)\n= Some negative aspects\n— If arelevant document never gets retrieved, we\nassume the precision corresponding to that relevant\ndoc to be zero (this is actually reasonable)\n— Each query counts equally\n— MAP assumes user is interested in finding many\nrelevant documents for each query\n— MAP requires many relevance judgments in the\ndocument collection"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **** Average over large document collection: The idea of using a large collection of documents to determine relevance.",
          "2. **** Query ensembles: Using multiple queries or search terms to assess relevance.",
          "3. **** Human relevance assessments: Assessing the relevance of search results based on human judgment."
        ],
        "definitions": [
          "1. **** Binary assessment: An assessment that categorizes search results as relevant or not relevant (no nuance).",
          "2. **** Heavily skewed by collection/authorship: A result that is biased towards a particular collection of documents or author."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "slide_016.png",
        "raw_text": "¢ Should average over large document collection and\nquery ensembles\n¢ Need human relevance assessments\n— But people aren’ always reliable assessors\n¢ Assessments have to be binary\n— Nuanced assessments?\n* Heavily skewed by collection/authorship\n— Results may not translate from one domain to another\nCopyright Ellis Horowitz, 2011-2022 16"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **** Discounted Cumulative Gain (DCG) is a measure that penalizes highly relevant documents appearing lower in search result lists.",
          "2. **** Graded Relevance value is reduced logarithmically proportional to the position of the result."
        ],
        "definitions": [
          "1. **** DCG: Discounted Cumulative Gain",
          "2. **** CG (Cumulative Gain): The sum of graded relevance values of search results.",
          "3. **** rel_i (Graded Relevance): The graded relevance value of the result at position i."
        ],
        "formulas": [
          "1. **** pcG = ∑ rel_i / log2(i+1) - log2(j+1)",
          "2. **** DCG, = ∑ (rel_i / log2(z+1)) for z=0 to i-1"
        ],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "slide_017.png",
        "raw_text": "* The premise of DCG is that highly relevant documents appearing lower in\nsearch result list should be penalized as the graded relevance value is reduced\nlogarithmically proportional to the position of the result.\n¢ The discounted CG accumulated at  particular rank position  is defined as\nre  re\npcG, = }> Ch = pel, + > ch\nlog,(i+ 1) — logs ( +1)\nwhere rel; is the graded relevance of the result at position\n* Gain is accumulated starting at the top of the ranking and may be reduced, or\ndiscounted, at lower ranks\n* Typical discount is 1/log (rank)\n— With base 2, the discount at rank 4 is 1/2, and at rank 8 it is 1/3\n¢ An alternative formulation of DCG places stronger emphasis on retrieving\nrelevant documents:  or ( 1\nDCG, = 5» ——_—\n— log, (z+ 1)\nCopyright Ellis Horowitz, 2011-2022 17\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "**Discounting in Document Ranking**: We want high weights for high-ranked documents because searchers are likely to inspect them, and low weights for low-ranked documents."
        ],
        "definitions": [
          "**Rank**: The position of a document in the search results.",
          "**Discount factor**: A value used to divide the relevance grade, commonly chosen as log2(rank + 1)."
        ],
        "formulas": [
          "**Discount factor calculation**: log2(rank + 1)",
          "**Weighting formula**: (weight) = (relevance grade) / (discount factor)"
        ],
        "algorithms": [
          "**Ranking documents with discounting**: Assign high weights to high-ranked documents and low weights to low-ranked documents using the discount factor."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "slide_018.png",
        "raw_text": "we want high weights for high .\nrank documents, because Discount examples\nsearchers are likely to inspect\nthem, and low weights for\nlow rank documents that Discount1 Discount2 Discount1 —_Discount2\nsearchers are unlikely to ever Rank Grade [4/rank] [log2(rank + 1)] Grade Grade\nsee.\nThe discount factor is 1 4 1000 1.000 4.000 4.000\ncommonly chosen\nas log2(rank + 1) and is used 2 3 0.500 0.631 1.500 1893\nto divide the relevance grade.\nUsing  logarithm for the 3 2 0.333 0.500 0.667 1.000\nposition penalty makes the\ndecay effect more gradual\ncompared to using the 4 1 0.250 0.431 0.250 0.431\nposition itself.\n5 1 0.200 0.387 0.200 0.387\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Information Retrieval evaluation measures (e.g., Precision, Recall, F-score)",
          "* Gridded results display vs. sequential scanning of results",
          "* Different types of metrics (Binary, Graded, Cumulative)"
        ],
        "definitions": [
          "* Binary Precision (P): \"The relevance of the top-ranked result\"",
          "* Average Precision (AP): \"Relevance to user scanning low-rank results sequentially\"",
          "* Graded Cumulative Gain (CG): \"Information gain from a set of results\"",
          "* Discount Cumulative Gain (DCG): \"Information gain with positional weighting\"",
          "* Normalized DCG (nDCG): \"How close the results are to the best possible\""
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "slide_019.png",
        "raw_text": "Metrics table\nScale Metric Measures Drawbacks\nBinary Precision (P) The relevance of — Doesn' account\ntheentire results for position\nset (gridded\nresults display)\nBinary Average Precision Relevance toa Large impact of\n(AP) user scanning low-rank results\nresults\nsequentially\nGraded Cumulative Gain Information gain Same as Precision\n(CG) from aresults doesn' factor in\nset position\nGraded Discount Information gain _ Difficult to\nCumulative Gain with positional compare across\n(DCG) weighting queries\nGraded normalized DCG How close the No longer shows\n(nDCG), results aretothe information gain\nbest possible\nFinally see https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\nCopyright Ellis Horowitz 2011-2022 _ _\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **** Search engines have test collections of queries and hand-ranked results",
          "2. **** Recall is difficult to measure on the web",
          "3. **** Precision at top positions (e.g., top 10) is a common evaluation metric"
        ],
        "definitions": [
          "1. **** Recall: ability of a search engine to retrieve all relevant documents from a collection",
          "2. **** Precision: measure of how accurate the search results are, often calculated at the top positions (e.g., top 10)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** Click-through on first result as a non-relevance-based measure of evaluation",
          "2. **** Studies of user behavior in lab settings and A/B testing"
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "slide_020.png",
        "raw_text": "= Search engines have test collections of queries and hand-ranked results\n\" Recall is difficult to measure on the web\n= Search engines often use precision at top  positions, e.g.,  = 10\n=... or measures that reward you more for getting rank / right than for getting\nrank /0 right.\n= Search engines also use non-relevance-based measures\n* Click-through on first result\nNot very reliable if you look at  single click-through ...\nbut pretty reliable in the aggregate.\n= Studies of user behavior in the lab\n= A/ testing"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1.  Google relies on raters to evaluate search results and search experience. (HIGH)",
          "2.  Data generated by raters is statistically analyzed to give a view of the quality of search results and search experience. (HIGH)",
          "3.  Ability to measure the effect of proposed changes to Google's search algorithms is crucial. (MEDIUM)"
        ],
        "definitions": [
          "1.  Raters: Individuals who evaluate search results and search experience.",
          "2.  General Guidelines: Overarching principles used by raters to evaluate search results."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "slide_021.png",
        "raw_text": "* Google relies on raters, working General Guidelines over\nin many countries and contin rien :\nlanguages around the world Sireremtere cath :\n¢ The data they generate is rolled estmnsa mo :\nup Statistically to give agi eceres restore :\n— aview of the quality of search 22 oabctar an oe 3\nresults and search experience aoe, :\n~— an ability to measure the to ;\neffect of proposed changes to Feet are :\nGoogle’ search algorithms trae sr ae\nhttp://\nCopyright Ellis Horowitz, 2011-2022 21\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **Search Quality Evaluator Guidelines**  - The document provides guidelines for evaluators to rate search results.",
          "2. **Rating Scale Categories**  - There are six categories used to evaluate search result relevance."
        ],
        "definitions": [
          "1. **Vital**  - A special rating category that requires further review (Section 4.1 of the Rating Guidelines).",
          "2. **Useful**  - A page that is very helpful for most users.",
          "3. **Relevant**  - A page that is helpful for many or some users.",
          "4. **Slightly Relevant**  - A page that is somewhat related to the query, but not very helpful for most users.",
          "5. **Off-Topic or Useless**  - A page that is helpful for very few or no users.",
          "6. **Unrateable**  - A page that cannot be evaluated (Section 4.6 of the Rating Guidelines)."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. The document provides examples and guidelines for evaluators to rate search results."
        ],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "slide_022.png",
        "raw_text": "+ This document gives evaluators examples and guidelines\nfor appropriate ratings.\n* the evaluator looks at  search query and  result that could\nbe returned. They rate the relevance of the result for that\nquery on  scale described within the document.\nThe six rating scale categories\nm@ -\nLB, searchqualityevaluatorgui\n( www-scf.\nEi Apps ye Bookmarks [}}\nRating Scale Description *\nVital  special rating category. See Section 4.1 of the Rating Guidelines.\nUseful  page that is very helpful for most users.\nRelevant  page that is helpful for many or some users.\n, ‘ page that is not very helpful for most users, but is somewhat related to the query. Some or few users\nSlightly Relevant ‘would find this page helpful\nOff-Topic or Useless __A page that is helpful for very few or no users.\nUnratable Apage that cannot be evaluated.  complete description can be found in Section 4.6 of the Rating\njuidelines.\n‘ >\nCopyright Ellis Horowitz, 2011-2022 22\nEOE>$EeeeeeeGO“_OO"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* : Method for evaluating search result quality (HIGH)",
          "* : Method for testing search algorithm changes on a small scale (MEDIUM)",
          "* : Final evaluation and release of improved search results (HIGH)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* : search results (HIGH)",
          "* : 118,812 people are shown two different sets of search results and asked which they prefer (HIGH)"
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "slide_023.png",
        "raw_text": "1. Precision Evaluations ET  coeeerso\nPeople use the Guidelines to rate =item\nsearch results\n2. Side-by-Side Experiments 118,812\npeople are shown two different oe Te\nsets of search results and asked —\nwhich they prefer hoaal\n3. Live Traffic Experiments 665\nthe search algorithm is altered\nfor  small number of actual users\n4. Full Launch\nfinal analysis by Google engineers\nand the improvement is released\nCopyright Ellis Horowitz 2011-2022\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* A/B testing is comparing two versions of a web page to see which one performs better ()",
          "* Single innovation test ()"
        ],
        "definitions": [
          "* Variants: Two different versions of a web page being compared in an A/B test ()"
        ],
        "formulas": [],
        "algorithms": [
          "5. Evaluate with automatic measure like click-through on first result ()"
        ],
        "examples": [
          "* Comparing two web pages to see which one gives a better conversion rate ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "slide_024.png",
        "raw_text": "¢ A/ testing is comparing two versions of  web page to see which one\nperforms better. You compare two web pages by showing the two variants\n(let' call them  and B) to similar visitors at the same time. The one that\ngives  better conversion rate, wins!\n1. Purpose: Test  single innovation\n2. Prerequisite: You have  large search engine up and running.\n3. Have most users use old system\n4. Divert  small proportion of traffic (e.g., 1%) to an experiment to\nevaluate an innovation\n5. Evaluate with an automatic measure like click through on first result\n* we directly see if the innovation does improve user happiness\n¢ This is the evaluation methodology large search engines trust the most\nCopyright Ellis Horowitz, 2011-2022 24\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "**Using user clicks for evaluation**: This concept highlights the idea that user interactions (clicks) can be used to assess website performance, usability, and overall effectiveness."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "**Clickstream analysis**: A process of analyzing the sequence of user clicks on a website to identify patterns, trends, and areas for improvement. (Note: This is not a mathematical formula, but rather an algorithmic process.)"
        ],
        "examples": [
          "**Case study:** Analyzing click data from an e-commerce website showed that users who clicked on product images were more likely to make a purchase than those who did not."
        ],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "slide_025.png",
        "raw_text": "USING USER CLICKS FOR\nEVALUATION"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Conference on Information and Knowledge Management (CIKM)",
          "* International forum for presentation and discussion of research on information and knowledge management"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* CIKM 2008 conference held in Napa Valley, California",
          "* CIKM 2007 conference held in Lisbon, Portugal, with a best interdisciplinary paper award",
          "* CIKM 2009 conference held in Hong Kong"
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "slide_026.png",
        "raw_text": "ALLRESULTS AL RESUS THAT OO SS LRT # of clicks received\nCIKM 2008 | Home\nNapa Valley Marriott Hotel & Spa: Napa Valley, California October 26-30, 2008\ncaw 2008 cikm2008.0rg - Cachi 49\nPapers Program Committee\nThemes News\nTOR) Important Dates Napa Valley\nTum history on Conference on Information and Knowledge Management (CIKI\nProvides an international forum for presentation and discussion of research on information and\nknowledge management, as well as recent advances on data and knowledge bases\neer  1\nConference on Information and Knowledge Management (CIKM'02:\nSAIC Headquarters, McLean, Virginia, USA, 4-9 November 2002.\nwww.cikm.org/2002 - Cached page\nACM CIKM 2007 - Lisbon, Portugal | 12\nNews and announcements: 12/02 - Best interdisciplinary paper award at CIKM 2007 went to Fei Wu\nand Daniel Weld for Autonomously Semantifying Wikipedia.\nwww .fc.ul_pt/cikm2007 - Cached page\nCIKM 2009 | Home 50\nheld on November 2-6, 2009, Hong Kong. Since 1992, CIKM has successfully brought together\nwww.comp polyu.edu.hk/conference/cikm2009 - Cached page\nConference on Information and Knowledge Management (CIKM)\nCIKM Conference on Information and Knowledge Management The Conference on 4\nInformation and Knowledge Management (CIKM) provides an international forum for presentation\nand\ncikmconference.org - Cached page\nThere is strong position bias, so absolute click rates unreliable\nCopyright Ellis Horowitz, 2011-2022 26\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "*  **Information and Knowledge Management**: Provides an international forum for presentation and discussion of research on information and knowledge management.",
          "*  **CIKM Conference**: A conference that brings together researchers to present and discuss research on information and knowledge management."
        ],
        "definitions": [
          "*  **CIKM (Conference on Information and Knowledge Management)**: An international forum for presentation and discussion of research on information and knowledge management.",
          "*  **CIKM Conference**: A conference that provides an international forum for presentation and discussion of research on information and knowledge management."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  **Case Study: CIKM 2007**: The best interdisciplinary paper award at CIKM 2007 went to Fei Wu and Daniel Weld for Autonomously Semantifying Wikipedia.",
          "*  **Conference Locations**: Examples of locations where the CIKM conference has been held, such as Napa Valley Marriott Hotel & Spa in California (2008) and Hong Kong (2009)."
        ],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "slide_027.png",
        "raw_text": "ALL RESULTS\nCIKM 2008 | Home\nNapa Valley Marriott Hotel & Spa: Napa Valley, California October 26-30, 2008\nCIKM 2008 cikm2008 org\nPapers Program Committee\nThemes News\nImportant Dates Napa Valley\nCinnouose Gane norman ni Kn Management CK User' clic\nProvides an international forum for presentation and discussion of research on information and\nknowledge management, as well as recent advances on data and knowledge bases\nva clk org sequence\nConference on Information and Knowledge Management (CIKM'02)\nSAIC Headquarters, McLean, Virginia, USA, 4-9 November 2002.\nvw thm or2002\nACM CIKM 2007 - Lisbon, Portugal\nNews and announcements: 12/02 - Best interdisciplinary paper award at CIKM 2007 went to Fei Wu\nand Daniel Weld for Autonomously Semantifying Wikipedia.\nwww fc.ul_pt/cikm2007\nCIKM 2009 | Home\nCIKM 2009 (The 18th ACM Conference on Information and Knowledge Management) will be\nheld on November 2-6, 2009, Hong Kong. Since 1992, CIKM has successfully brought together\n‘www.comp polyu.edu.hk/conference/cikm2009\nConference on Information and Knowledge Management (CIKM)\nCIKM Conference on Information and Knowledge Management The Conferepeé on\nInformation and Knowledge Management (CIKM)proides an international fod for presentation\nand\ncikmconference org\nHard to conclude Result] > Result3\nProbably can conclude Result3 > Result2\nCopyright Ellis Horowitz, 2011-2022 27\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Query log files used for both tuning and evaluating search engines",
          "* Query logs contain various techniques such as query suggestion"
        ],
        "definitions": [
          "* Query log files: records of user interactions with a search engine, including queries submitted, results clicked on, and timestamps"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "slide_028.png",
        "raw_text": "¢ Used for both tuning and evaluating search engines\n— also for various techniques such as query suggestion\n* Typical contents of the query log files\n— User identifier or user session identifier\n— Query terms - stored exactly as user entered them\n— List of URLs of results, their ranks on the result list,\nand whether they were clicked on\n— Timestamp(s) - records the time of user events such as\nquery submission, clicks\nCopyright Ellis Horowitz, 2011-2022 28\nTn"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Clicks are not relevance judgments"
        ],
        "definitions": [
          "* Correlation between clicks and relevance judgments"
        ],
        "formulas": [],
        "algorithms": [
          "* Using clickthrough data to predict preferences between pairs of documents (implied, but not explicitly stated as an algorithm)"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "slide_029.png",
        "raw_text": "¢ Clicks are not relevance judgments\n— although they are correlated\n— biased by  number of factors such as\nrank on result list\n¢ Can use clickthough data to predict\npreferences between pairs of documents\n— appropriate for tasks with multiple\nlevels of relevance, focused on user\nrelevance\n— various “policies” used to generate\npreferences\nCopyright Ellis Horowitz, 2011-2022 29"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "1. **Display Improvements** () - The concept of improving search results display to provide more relevant information to users.",
          "2. **autocomplete anticipations** () - A feature that suggests possible search queries based on a user's input.",
          "3. **Extensions to More Data** () - The idea of incorporating additional data sources into search results, such as books, news, images, patents, and air schedules.",
          "4. **Featured Snippets** () - A summary of the most relevant information from a webpage, displayed at the top of the search results page.",
          "5. **Knowledge Graph** () - A knowledge base that provides additional information about entities such as people, places, and organizations."
        ],
        "definitions": [
          "1. **Autocomplete** () - A feature that suggests possible search queries based on a user's input.",
          "2. **Directions** () - A type of search result that provides directions to a location.",
          "3. **Knowledge Graph traffic** () - The information provided by the Knowledge Graph, such as entity relationships and attributes."
        ],
        "formulas": [],
        "algorithms": [
          "1. **Search algorithm** () - An algorithm that determines the order and relevance of search results based on user input.",
          "2. **Spelling correction algorithm** () - An algorithm that corrects spelling errors in search queries."
        ],
        "examples": [
          "1. **Searching by voice** () - An example of how users can search using voice commands.",
          "2. **People Also Ask boxes** () - A feature that displays related questions and answers below the main search results."
        ],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "slide_030.png",
        "raw_text": "Display improvements\n¢ immediate answers The page below discusses the many aspects that\nte. go into producing search results at Google\n* autocomplete anticipations .\n. https://www.google.com/search/howsearchworks\nExtensions to More Data\n* results from books\n* results from news\n¢ results from images Answers from the Directions and\nKnowledge Graph traffic\n* results from patents\n* results from air schedules > >\nNew Input forms\n: Direct answers Featured snippets\n* search by voice\n* search by image\n. . . . > >\n° snippets Rich lists Answers before\n* spelling correction penliratoets\n* translations 5 5\n* People Also Ask boxes\n* use of synonyms\n«use of knowledge graph Copyright Ellis Horowitz, 2011-2022 30\nccc"
      },
      {
        "lecture": "se-evaluation",
        "concepts": [
          "* Comparison of web search engines (marked as )"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Wikipedia link provided for comparison of web search engines (marked as )"
        ],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "slide_031.png",
        "raw_text": "¢ See wikipedia on\n¢ https://en.wikipedia.org/wiki/Comparison_of_web\n_Search_engines\nCopyright Ellis Horowitz, 2011-2022 31"
      }
    ],
    "text_processing": [
      {
        "lecture": "text_processing",
        "concepts": [
          "Information Retrieval (IR) vs. text classification",
          "Standing queries: periodic search for new relevant documents",
          "Relevant vs. not relevant classification"
        ],
        "definitions": [
          "IR: Information Retrieval",
          "Text classification: classification of documents as relevant or not relevant",
          "Standing queries: periodic search for new documents on a specific topic"
        ],
        "formulas": [],
        "algorithms": [
          "Not explicitly stated, but implied:"
        ],
        "examples": [
          "Unrest in the Niger delta region: a specific topic of interest",
          "Google Alerts: modern mass instantiation of standing queries"
        ],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "s1.png",
        "raw_text": "¢ The path from IR to text classification:\n— You have an information need to monitor, say:\n¢ Unrest in the Niger delta region\n— You want to rerun an appropriate query periodically to find new\nnews items on this topic\n— You will be sent new documents that are found\n* Le., it’  not ranking but classification (relevant vs. not relevant)\n¢ Such queries are called standing queries\n— Long used by “information professionals”\n—  modern mass instantiation is Google Alerts\n¢ Standing queries are (hand-written) text classifiers\nCopyright Ellis Horowitz, 2011-2015 2\nRe"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "**Tokenization**: The process of breaking down text into individual tokens or words (implied in the first tweet by @Robertoross)",
          "**Natural Language Processing (NLP)**: A field of study that focuses on interactions between computers and human language (mentioned in various parts of the email)"
        ],
        "definitions": [
          "**Parser**: A component of NLP that attempts to assign a syntactic structure to a given input sentence (mentioned in the second tweet by @Robertoross)",
          "**Tagger**: A component of NLP that assigns a part-of-speech tag to each word in a sentence (mentioned in the second tweet by @Robertoross)"
        ],
        "formulas": [],
        "algorithms": [
          "**CKY parser**: A bottom-up parsing algorithm for context-free grammars, implemented as an assignment for Stanford's nip-class (mentioned in the third link)"
        ],
        "examples": [
          "**Tokenization using PTBTokenizer**: The first tweet by @Robertoross provides an example of tokenizing a text file using the Java class edu.stanford.nlp.process.PTBTokenizer"
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "s2.png",
        "raw_text": "From: Google Alerts\nSubject: Google Alert - stanford -neuro-linguistic nlp OR \"Natural Language Processing\" OR\nparser OR tagger OR ner OR “named entity\" OR segmenter OR classifier OR\ndependencies OR “core nip\" OR corenip OR phrasal\nDate: May 7, 2012 8:54:53 PM PDT\nTo: Christopher Manning\n3 new results for stanford -neuro-linguistic nlp OR \"Natural Language\nWeb Processing” OR parser OR tagger OR ner OR “named entity\" OR segmenter\nOR classifier OR dependencies OR “core nip\" OR corenlp OR phrasal\nTwitter / Stanford NLP Group: @Robertoross If you only n...\n@Robertoross If you only need tokenization, java -mx2m edu.stanford.nlp. process.PTBTokenizer file.txt runs in 2MB\non  whole file for me.... 9:41 PM Apr 28th ...\ntwitter.com/stanfordnip/status/196459102770171905\nJava] LexicalizedParser Ip = LexicalizedParser.loadModel(\"edu ...\nloadModel(\"edu/stanford/nip/models/lexparser/englishPCFG.ser.gz\");. String[] sent = { \"This\", is\", \"an\", “easy”,\n“sentence”, \".\" };. Tree parse = Ip.apply(Arrays.\npastebin.comvaz14R9nd\nMore Problems with Statistical NLP || kuroShin.org\nTags: nip, ai, coursera, stanford, nip-class, cky, nitk, reinventing the wheel, ... Programming Assignment 6 for\nStanford' nip-class is to implement  CKY parser .\nwww. kuroShin.org/story/2012/5/5/11011/68221\nTip: Use quotes (\"like this\") around  set of words in your query to match them exactly. Learn more.\nDelete this alert.\nCreate another alert.\nManage your alerts.\neVvAOoOo_S"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "- Real estate investment ()",
          "- No-money-down property purchase ()",
          "+  Real estate investment",
          "+  No-money-down property purchase"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "- A 22-year-old person buying 6 properties using the ebook's methods ()",
          "+ The anecdotal claim of buying 6 properties at age 22 ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "s3.png",
        "raw_text": "From: \"\" <takworlld@hotmail.com>\nSubject: real estate is the only way... gem oalvgkay\nAnyone can buy real estate with no money down\nStop paying rent TODAY !\nThere is no need to spend hundreds or even thousands for similar courses\nIam 22 years old and  have already purchased 6 properties using the\nmethods outlined in this truly INCREDIBLE ebook.\nChange your life NOW !\nClick Below to order:\nhttp://www.wholesaledaily.com/sales/nmd.htm\nCopyright Ellis Horowitz, 2011-2015 4\nRe"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "**Representation of Text Documents**: How to represent text documents in a way that can be processed by computers.",
          "**Bag of Words**: A type of high-dimensional space used to represent text documents.",
          "**Classification Functions**: Also known as \"classifiers\", these are functions that determine the category of a document."
        ],
        "definitions": [
          "**Document Representation**: The way in which text documents are represented in a computer.",
          "**Bag of Words Space**: A high-dimensional space where each dimension represents a word in the vocabulary.",
          "**Classification Function**: A function that takes a document as input and outputs its category."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "s4.png",
        "raw_text": "¢ Given:\n—  representation of  document\n¢ Issue: how to represent text documents.\n¢ Usually some type of high-dimensional space — bag of words\n—  fixed set of classes:\nC= {Cp Cy... Cf\n* Determine:\n— The category of  by generating  classification\nfunction, say y(d)\n— We want to build classification functions (“classifiers”).\nCopyright Ellis Horowitz, 2011-2015 5"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "1. **Test language**",
          "2. **Data: Cet proof** (likely referring to \"CET\" or \"Certification\")",
          "3. **Artificial Intelligence (AI)**",
          "4. **Multimedia, Machine Learning, Programming, and Intelligence**"
        ],
        "definitions": [
          "1. **NA intelligence** ( likely referring to \"Natural Language Processing\" or \"NLP\")",
          "2. **Temporal semantics collection**",
          "3. **Optimization network**"
        ],
        "formulas": [],
        "algorithms": [
          "1. **Training learning planning programming garbage...** ( likely referring to a step-by-step process in machine learning or AI)"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "s5.png",
        "raw_text": "[ “planning\nTest  language\nData: cet proof\nan NA intelligence”\n(Al) (Programming) (HCl)\nClasses: OS eo é maz\nME’ Garb.Coll. || Multinfedia\nTraining learning planning programming garbage ... wes\nData: intelligence temporal semantics collection\nalgorithm reasoning language memory\nreinforcement plan proof... optimization\nnetwork... language... region...\nCopyright Ellis Horowitz, 2011-2015 6"
      },
      {
        "lecture": "text_processing",
        "concepts": [],
        "definitions": [
          "*  Manual classification: Used by original Yahoo! Directory, Looksmart, about.com, ODP, PubMed"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Accurate when job is done by experts (this is an example of a scenario where manual classification is effective)"
        ],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "s6.png",
        "raw_text": "¢ Manual classification\n— Used by the original Yahoo! Directory\n— Looksmart, about.com, ODP, PubMed\n— Accurate when job is done by experts\n— Consistent when the problem size and team is small\n— Difficult and expensive to scale\n« Means we need automatic classification methods for big\nproblems\nCopyright Ellis Horowitz, 2011-2015 7"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "Rule-based classifiers",
          "Hand-coded rule-based classifiers",
          "Text processing"
        ],
        "definitions": [
          "IDE (Integrated Development Environment): a software tool for writing rules for hand-coded rule-based classifiers."
        ],
        "formulas": [
          "Note that there are no mathematical formulas (algorithms or equations) in the given content, so there is no  category."
        ],
        "algorithms": [
          "The process of creating rules for hand-coded rule-based classifiers, which involves writing specific criteria to classify text."
        ],
        "examples": [
          "News agencies and intelligence agencies use hand-coded rule-based classifiers.",
          "Vendors provide \"IDE\" for writing such rules."
        ],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "s7.png",
        "raw_text": "¢ Hand-coded rule-based classifiers\n— One technique used by news agencies,\nintelligence agencies, etc.\n— Widely deployed in government and enterprises\n— Vendors provide “IDE” for writing such rules"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "* Hand-coded rule-based classifiers"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "s8.png",
        "raw_text": "¢ Hand-coded rule-based classifiers\n— Commercial systems have complex query\nlanguages\n— Accuracy is can be high if  rule has been\ncarefully refined over time by  subject expert\n— Building and maintaining these rules is\nexpensive"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "**Text Classification**: The process of assigning a class label to a document based on its content."
        ],
        "definitions": [
          "**Document**: A piece of text that is being classified.",
          "**Class (C)**: A fixed set of categories or labels that documents can be assigned to (e.g. Cp, Cy, ..., Cf).",
          "**Training Set**: A collection of labeled documents used to train a classifier."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "s9.png",
        "raw_text": "¢ Given:\n—  document\n—  fixed set of classes:\nC= {Cp Cy... Cf\n—  training set  of documents each with  label in\n* Determine:\n—  learning method or algorithm which will enable us to\nlearn  classifier\n— For  test document d, we assign it the class\n(4) EC\nCopyright Elis Horowitz, 2011-2015 10"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "**Supervised Learning**: requires hand-classified training data",
          "**No Free Lunch**: implies that each learning method has its own strengths and weaknesses",
          "**Mixture of Methods**: commercial systems often use a combination of different machine learning methods"
        ],
        "definitions": [
          "**Naive Bayes**: a simple and common supervised learning algorithm",
          "**k-Nearest Neighbors (k-NN)**: a simple and powerful supervised learning algorithm",
          "**Support-vector machines (SVMs)**: a newer and generally more powerful supervised learning algorithm"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**Building up data by amateurs**: can refine and improve the training dataset over time",
          "**Commercial systems using mixture of methods**: often combine multiple machine learning techniques to achieve best results"
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "s10.png",
        "raw_text": "¢ Supervised learning\n— Naive Bayes (simple, common)\n— k-Nearest Neighbors (simple, powerful)\n— Support-vector machines (newer, generally more\npowerful)\n— ... plus many other methods\n— No free lunch: requires hand-classified training data\n— But data can be built up (and refined) by amateurs\n¢ Many commercial systems use  mixture of\nmethods\nCopyright Elis Horowitz, 2011-2015 Hl"
      },
      {
        "lecture": "text_processing",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **EXAMPLE**:  - A review of a movie (with satirical humor, romantic, etc.) that discusses its characteristics."
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "s11.png",
        "raw_text": "love this movie! It' sweet,\nbut with satirical humor. The\ndialogue is great and the\nadventure scenes are fun... It\nmanages to be whimsical and\nromantic while laughing at the —\nconventions of the fairy tale —\ngenre.  would recommend it to\njust about anyone. I've seen it\nseveral times, and I' always\nhappy to see it again whenever\nhave  friend who hasn' seen\nit yet.\nCopyright Ellis Horowitz, 2011-2015 12\nRe"
      },
      {
        "lecture": "text_processing",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "s12.png",
        "raw_text": ")="
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "*  Supervised learning classifiers can use various features in text processing",
          "*  Bag of words view of documents"
        ],
        "definitions": [
          "*  Feature (in supervised learning): any sort of characteristic used to describe a document or text",
          "*  URL, email address, punctuation, capitalization, dictionaries, network features: examples of features that can be used in text processing"
        ],
        "formulas": [],
        "algorithms": [
          "*  Simplest bag of words view of documents:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "s13.png",
        "raw_text": "* Supervised learning classifiers can use any sort of\nfeature\n— URL, email address, punctuation, capitalization,\ndictionaries, network features\n¢ In the simplest bag of words view of documents\n— We use only word features\n— we use all of the words in the text (not  subset)\nCopyright Elis Horowitz, 2011-2015 4"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "* Text collections have a large number of features",
          "* Selection can make particular classifiers feasible",
          "* Reduces training time",
          "* Makes runtime models smaller and faster",
          "* Can improve generalization (performance)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Some classifiers can't deal with 1,000,000 features",
          "* Training time for some methods is quadratic or worse in the number of features"
        ],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "s14.png",
        "raw_text": "¢ Text collections have  large number of features\n— 10,000 — 1,000,000 unique words ... and more\n¢ Selection may make  particular classifier feasible\n— Some classifiers can’  deal with 1,000,000 features\n¢ Reduces training time\n— Training time for some methods is quadratic or worse in the\nnumber of features\n¢« Makes runtime models smaller and faster\n¢ Can improve generalization (performance)\n— Eliminates noise features\n— Avoids overfitting\nCopyright Ellis Horowitz, 2011-2015 15"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "1.  **Simplest Feature Selection Method**: The most common terms can be used for feature selection with no particular foundation.",
          "2.  **Well-Estimatable Terms**: Words that can be well-estimated are often available as evidence."
        ],
        "definitions": [
          "1.  **Feature Selection**: The process of selecting a subset of relevant features from the original set of features."
        ],
        "formulas": [],
        "algorithms": [
          "1.  **Simple Feature Selection Method**: Use the most common terms for feature selection, but no particular foundation is required."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "s15.png",
        "raw_text": "¢ The simplest feature selection method:\n— Just use the most common terms\n— No particular foundation\n— But it make sense why this works\n¢ They are the words that can be well-estimated and\nare most often available as evidence\n— In practice, this is often 90% as good as better\nmethods\n— Smarter feature selection — future lecture\nCopyright Elis Horowitz, 2011-2015 16"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "Naive Bayes has found a home in spam filtering",
          "Spam filters often use features beyond just words"
        ],
        "definitions": [
          "Spam filtering (no definition provided)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Paul Graham's Plan for Spam (http://www.paulgraham.com/spam.html) is an example of how Naive Bayes is used in spam filtering."
        ],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "s16.png",
        "raw_text": "¢ Naive Bayes has found  home in spam\nfiltering\n— Paul Graham’   Plan for Spam\n* http://www.paulgraham.com/spam.html\n— Widely used in spam filters\n— But many features beyond words:\n¢ black hole lists, etc.\n* particular hand-crafted text patterns\nCopyright Elis Horowitz, 2011-2015 17"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "Document as a vector",
          "High-dimensional vector space",
          "Classification in high-dimensional space"
        ],
        "definitions": [
          "*  Vector: each document is represented by one component for each term (word)",
          "*  Dimensionality: 10,000+ dimensions or even 100,000+"
        ],
        "formulas": [],
        "algorithms": [
          "Normalization of vectors to unit length (no specific steps mentioned)"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "s24.png",
        "raw_text": "* Each document is  vector, one component for\neach term (= word).\n* Normally normalize vectors to unit length.\n¢ High-dimensional vector space:\n— Terms are axes\n— 10,000+ dimensions, or even 100,000+\n— Does are vectors in this space\n* How can we do classification in this space?\nCopyright Ellis Horowitz, 2011-2015 25"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "* Vector space classification",
          "* Labeled set of points (equivalently, vectors)",
          "* Documents in the same class form contiguous region of space",
          "* Documents from different classes don't overlap (much)"
        ],
        "definitions": [
          "+ \"Documents\" can be considered as : Input data points or instances in the text classification problem",
          "+ \"Classes\" can be considered as : Categories or labels assigned to documents (e.g., spam/not spam, positive/negative review)"
        ],
        "formulas": [],
        "algorithms": [
          "* Build surfaces to delineate classes in the space"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "s25.png",
        "raw_text": "¢ In vector space classification, training set\ncorresponds to  labeled set of points\n(equivalently, vectors)\nPremise 1: Documents in the same class form\ncontiguous region of space\n¢ Premise 2: Documents from different classes don’\noverlap (much)\n¢ Learning  classifier: build surfaces to delineate\nclasses in the space"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "Text classification or categorization"
        ],
        "definitions": [
          "Government",
          "Sci (Science)",
          "Arts",
          "Copyright Ellis Horowitz 2011-2012"
        ],
        "formulas": [],
        "algorithms": [
          "None explicitly mentioned in this slide content"
        ],
        "examples": [
          "None provided in this slide content"
        ],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "s27.png",
        "raw_text": "@  @\n@ Government\n@Sci\nScience\n@ Arts\nCopyright Ellis Horowitz 2011-2012"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "+  Text categorization ( Government, Science, Arts)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "s28.png",
        "raw_text": "° @\n° @\n7 @\n@ @ @ Government\n° . ®Science\na, @ Arts"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "*  Centroid: the vector space representation of a set of documents"
        ],
        "definitions": [
          "*  **D**: the set of all documents that belong to class"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  The example given is not a concrete example, but rather an explanation of the concept"
        ],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "s29.png",
        "raw_text": "—\nHe)=—— d¥(a)\n|  | deD,\n¢ Where D, is the set of all documents that belong to\nclass  and v(d) is the vector space representation of\nd.\n¢ Note that centroid will in general not be  unit\nvector even when the inputs are unit vectors.\nCopyright Ellis Horowitz, 2011-2015 30"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "*  Rocchio forms a simple representative for each class: the centroid/prototype",
          "*  Classification: nearest prototype/centroid"
        ],
        "definitions": [
          "*  Centroid/Prototype: A simple representative formed by Rocchio for each class"
        ],
        "formulas": [],
        "algorithms": [
          "*  The Rocchio algorithm forms a centroid/prototype for each class",
          "*  Classification is performed by finding the nearest prototype/centroid"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "s30.png",
        "raw_text": "¢ Rocchio forms  simple representative for\neach class: the centroid/prototype\n¢ Classification: nearest prototype/centroid\nCopyright Ellis Horowitz, 2011-2015 31"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "*  kNN (k-Nearest Neighbor) algorithm",
          "*  Classification of documents using nearest neighbors"
        ],
        "definitions": [
          "*  k-neighborhood: The set of the k-nearest neighbors to a document d"
        ],
        "formulas": [
          "*  P(c|d) ≈ #(c)/ for larger k (Note: P(c|d) represents the probability of class c given document d, and #(c) is the number of documents in class c)"
        ],
        "algorithms": [
          "*"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "s34.png",
        "raw_text": "¢ kNN =  Nearest Neighbor\n¢ To classify  document d:\n¢ Define k-neighborhood as the  nearest neighbors of\n¢ Pick the majority class label in the k-neighborhood\nFor larger  can roughly estimate P(c|d) as #(c)/"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "**Voronoi Diagram**: A way of partitioning a plane into regions based on proximity to points."
        ],
        "definitions": [
          "**Copyright**: Ownership or rights over original work (relevant for the Voronoi diagram citation)."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "s35.png",
        "raw_text": "@  @\ne.. @\n(|\n()\n@ Government\n@Sci\nScience\n@ Arts\nVoronoi diagram — Copyright Ellis Horowitz 2011-2012\nRe"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "* **Just store the labeled training examples**",
          "* **kNN (k-Nearest Neighbors)**",
          "* **Contiguity hypothesis**"
        ],
        "definitions": [
          "* **Testing instance** (an example to be classified)",
          "* **Database D** (a set of labeled training examples)"
        ],
        "formulas": [],
        "algorithms": [
          "+ Assign  the category of the most similar example in D."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "s36.png",
        "raw_text": "Learning: just store the labeled training examples\n¢ Testing instance  (under 1NN):\n— Compute similarity between  and all examples in D.\n— Assign  the category of the most similar example in D.\n¢ Also called:\n— Case-based learning\n— Memory-based learning\n— Lazy learning\n¢ Rationale of kNN: contiguity hypothesis\nCopyright Ellis Horowitz, 2011-2015 37"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "*  1-Nearest Neighbor (1NN) algorithm",
          "*  Robustness of k-Nearest Neighbors algorithm"
        ],
        "definitions": [
          "*  Atypical example: a single example that is significantly different from the others in its category.",
          "*  Noise: an error in the category label of a single training example."
        ],
        "formulas": [],
        "algorithms": [
          "*  1-Nearest Neighbor (1NN) algorithm: uses only the closest example to make predictions, prone to errors due to atypical examples and noise.",
          "*  k-Nearest Neighbors algorithm: finds examples and returns the majority category of these, typically with an odd value of k to avoid ties."
        ],
        "examples": [
          "*  The 1NN algorithm is prone to errors due to atypical examples (e.g., a single outlier) and noise in the category label of a single training example."
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "s37.png",
        "raw_text": "Using only the closest example (1NN) subject to errors due\nto:\n— Asingle atypical example.\n— Noise (i.e., an error) in the category label of  single\ntraining example.\n¢ More robust: find the  examples and return the majority\ncategory of these\nkis typically odd to avoid ties; 3 and 5 are most common\nCopyright Ellis Horowitz, 2011-2015 38"
      },
      {
        "lecture": "text_processing",
        "concepts": [
          "*  No feature selection necessary",
          "*  Scales well with large number of classes"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Classes can influence each other - Small changes to one class can have ripple effect"
        ],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "s39.png",
        "raw_text": "¢ No feature selection necessary\n¢ No training necessary\n¢ Scales well with large number of classes\n— Don’ need to train  classifiers for  classes\nClasses can influence each other\n— Small changes to one class can have ripple effect\n¢ Done naively, very expensive at test time\nIn most cases it’ more accurate than NB or Rocchio"
      }
    ],
    "web_crawling": [
      {
        "lecture": "web_crawling",
        "concepts": [
          "* Categorize them accordingly with the corresponding marks (, [DEFINITION], [FORMULA], [ALGORITHM], [EXAMPLE], and [PRIORITY])"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "slide_001.png",
        "raw_text": "Crawlers and Crawling"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  Web crawler: a computer program that visits web pages in an organized way",
          "*  Web crawlers are sometimes called spiders or robots",
          "*  Importance of understanding web crawlers for web crawling"
        ],
        "definitions": [
          "*  Googlebot: Google's crawler",
          "*  Yahoo! Slurp: Yahoo's former web crawler (now retired)",
          "*  Bingbot, Adidxbot, MSNbot, MSNBotMedia, BingPreview: Bing's five crawlers"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  List of web crawlers at http://en.wikipedia.org/wiki/Web_crawler",
          "*  Google's crawler is called googlebot (http://support.google.com/webmasters/bin/answer.py?hl=en&answer=182072)",
          "*  Yahoo!'s web crawler is/was called Yahoo! Slurp (http://wikipedia.org/wiki/Yahoo_Search)"
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "slide_002.png",
        "raw_text": "*  web crawler is  computer program that visits web pages in an\norganized way\n— Sometimes called  spider or robot\n¢  list of web crawlers can be found at\nhttp://en.wikipedia.org/wiki/Web_crawler\nGoogle’ crawler is called googlebot, see\nhttp://support.google.com/webmasters/bin/answer.py?hl=en&answer=182072\n* Yahoo’ web crawler is/was called Yahoo! Slurp, see\nhttp://en.wikipedia.org/wiki/ ahoo!_ Search\n* Bing uses five crawlers\n—  Bingbot, standard crawler\n—  Adidxbot, used by Bing Ads\n— MSNbot, remnant from MSN, but still in use\n— MSNBotMedia, crawls images and video\n—  BingPreview, generates page snapshots\n* — For details see: http:/www.bing.com/webmaster/help/which-crawlers-does-bing-use-8c 1 84ec0\nCopyright Ellis Horowitz, 2011-2022 2\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "1. **Quality in web crawling**: finding the \"Best\" pages first",
          "2. **Efficiency in web crawling**: avoiding duplication or near duplication",
          "3. **Etiquette in web crawling**: behaving politely to not disturb website performance"
        ],
        "definitions": [
          "1. **Coverage**: percentage of the web that should be covered",
          "2. **Relative Coverage**: comparison of coverage between competitors"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "slide_003.png",
        "raw_text": "* How to crawl?\n— Quality: how to find the “Best” pages first\n— Efficiency: how to avoid duplication (or near duplication)\n— Etiquette: behave politely by not disturbing  website’ performance\n* How much to crawl? How much to index?\n— Coverage: What percentage of the web should be covered?\n— Relative Coverage: How much do competitors have?\n* How often to crawl?\n— Freshness: How much has changed?\n— How much has really changed?\nCopyright Ellis Horowitz, 2011-2022 3"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Web Crawling",
          "Seed pages (known pages to start with)",
          "Fetching and parsing web pages",
          "Database storage of crawled pages"
        ],
        "definitions": [
          "Queue: a data structure that holds URLs to be fetched",
          "Fetch: retrieving a web page from the internet",
          "Parse: analyzing the content of a web page"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "slide_004.png",
        "raw_text": "¢ Initialize (begin with known “seed” pages)\n¢ Loop: Fetch and parse  page\n— Place the page in  database\n— Extract the URLs within the page\n— Place the extracted URLs on  queue\n— Fetch  URL on the queue and repeat"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  Web crawling",
          "*  Indexes"
        ],
        "definitions": [
          "*  Unseen Web",
          "*  Seed URLs",
          "*  Frontier (of crawled pages)"
        ],
        "formulas": [],
        "algorithms": [
          "*  Crawling process:"
        ],
        "examples": [
          "*  Online edition of \"Our textbook\" (cited from Cambridge UP, 2009)"
        ],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "slide_005.png",
        "raw_text": "2 0 Web crawling and indexes\nURLs crawled\nand parsed ‘ Soe\nUnseen Web ene\nSeed \\\\ URLs frontier See\npagess— Online edition (c) 2009 Canbridge UP\nWeb :\n— é Our textbook\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "1.  - Crawling the entire web is not feasible with one machine (distributed processing)",
          "2.  - Handling/Avoiding malicious pages",
          "3.  - Latency/bandwidth to remote servers can vary widely"
        ],
        "definitions": [
          "1.  - Spider traps: dynamically generated pages that can trap crawlers",
          "2.  - Robots.txt stipulations: rules set by webmasters to control crawling behavior",
          "3.  - Politeness: avoiding hitting a server too often"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "slide_006.png",
        "raw_text": "* Crawling the entire web isn’ feasible with one machine\n— But all of the above steps can be distributed\n¢ Challenges\n— Handling/Avoiding malicious pages\n¢ Some pages contain spam\n¢ Some pages contain spider traps — especially\ndynamically generated pages\n— Even non-malicious pages pose challenges\n¢ Latency/bandwidth to remote servers can vary widely\n* Robots.txt stipulations can prevent web pages from\nbeing visited\n¢ How can one avoid mirrored sites and duplicate pages\n— Maintain politeness — don’ hit  server too often\nCopyright Ellis Horowitz, 2011-2022 6\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Protocol for web crawler limitations",
          "Robots.txt file defines crawling permissions"
        ],
        "definitions": [
          "Robotstxt.org: a protocol that defines limitations for web crawlers",
          "Robots.txt file: a file placed in the root directory to announce crawling requests",
          "Crawling: the process of a web crawler visiting and retrieving data from websites"
        ],
        "formulas": [
          "Note that there are no mathematical formulas or algorithms in this content, so there is no  or [ALGORITHM] category."
        ],
        "algorithms": [],
        "examples": [
          "Ticketmaster.com/robots.txt: an example of a website's robots.txt file"
        ],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "slide_007.png",
        "raw_text": "¢ There is  protocol that defines the limitations for  web crawler as it\nvisits  website; its definition is here\n— http://www. robotstxt.org/orig.html\n¢ The website announces its request on what can(not) be crawled by\nplacing  robots.txt file in the root directory\n— €@.g. see\nhttp://www.ticketmaster.com/robots.txt\nCopyright Ellis Horowitz, 2011-2022 7"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "**robots.txt directives**: rules that control which pages a robot can crawl on a website",
          "**User-agent**: identifies the type of robot visiting the domain (e.g., Slurp)",
          "**Disallow**: specifies URLs that should not be crawled"
        ],
        "definitions": [
          "**robots.txt file**: a text file placed in the root directory of a website to control crawling",
          "**Allow**: specifies URLs that can be crawled (opposite of Disallow)",
          "**User-agent directive**: a rule that applies to a specific type of robot"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**robots.txt example**:"
        ],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "slide_008.png",
        "raw_text": "* No robot visiting this domain should visit any URL starting with\n\"/yoursite/temp/\":\nUser-agent: *\nDisallow: /yoursite/temp/\n* Directives are case sensitive\n¢ Additional symbols allowed in the robots.txt directives include:\n— '™* _ matches  sequence of characters\n— '$' - anchors at the end of the URL string\n¢ Example of '*':\nUser-agent: Slurp\nAllow: /public*/\nDisallow: /* print*.html\nDisallow: /*?sessionid\nCopyright Ellis Horowitz, 2011-2022 8\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  Robots.txt files",
          "*  User-agent directives"
        ],
        "definitions": [
          "*  Robots.txt file: a text file placed at the root of a website's domain that contains instructions for web crawlers",
          "*  User-agent: a software program that acts on behalf of a user, such as a web crawler"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  ScienceDirect's robots.txt file (https://www.sciencedirect.com/robots.txt)"
        ],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "slide_009.png",
        "raw_text": "¢ In researching how websites treat crawlers, Zack Maril looked at 17\nmillion robots.txt files and discovered many places where Google is\ngiven an advantage; e.g. see\n¢ https://www.sciencedirect.com/robots.txt\n° E.g.\n¢ # go away ? tell all others not in the list below to stay out!\n¢ User-agent: *\n* Disallow: /\n# As of 02/09/2021, there are 10 crawlers welcomed by SD\nUser-agent: Googlebot\n¢ Disallow: /cache/MiamiImageURL/\n¢ Disallow: /science?_ob=MiamiCaptionURL*\nCopyright Ellis Horowitz, 2011-2022 9\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "1. **Determining bias to search engines from robots.txt **",
          "2. **Favored and disfavored robots **",
          "3. **Robot names in robots.txt files **",
          "4. **Association between robot names and AP(r) values **"
        ],
        "definitions": [
          "1. **AP(r)**: Not explicitly defined, but it appears to be a measure or score associated with each robot name (e.g., -0.0291).",
          "2. **robot.txt**: A file used by web servers to communicate with crawlers and spiders about which parts of the website should not be crawled."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **Table 2: Top 10 favored and disfavored robots**: A concrete example showing the top 10 robot names, their frequencies, and AP(r) values.",
          "2. **Figure 2: Most frequently used robot names**: An illustration of the most common robot names in robots.txt files."
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "slide_010.png",
        "raw_text": "SC\n¢ Determining bias to search engines from robots.txt, Giles, Sun,\nZhuang, Penn State\nFavored Robots (Sample size  = 2925)\nrobot Nyavor] Nass | APO) |\name 3 3000\ngoogle 0.0084 ¥ 2500\nyahoo 0.2041 __ | 0.0075 = |\nmsn [349 [9 | 0.1162 _ | 0.0059 noe\nscooter 0.0058\nTycos [9o1__|5 | 0.0294 | 0.0031\nnetmechanid 84 [10 [0.0253 | 0.0029 _\nhidig 0.0041__| 0.0012 2 500\nteoma 0.0034 | 0.0011\ncodlebot* [8 [0 [0.0027 | 0.0010 = °\nmomspider|6 [0 _[ 0.0021 0.0008 .\nSa Robots (Sample size  = 2925 “FSF EF FESS ES\niple size  = 2925) CS FS FESS SF HF\nrobot Njavor| Nast | AP(r) [7 o* *  $$\nname o¢ of\nmsiecrawler} 0 | 85 __| -0.0291 _| 0.0031\nia_archiver ~0.0164 | 0.0023\ncherrypicket 0__[37__[-0.0126 [0.0021 Figure 2. Most frequently used robot names\nemailsiphon| 3 [34 | -0.0106 | 0.0019 . .\nroverbot 0.0017 in robots.txt files. The height of the bar rep-\npsbot 9.006 resents the number of times  robot appeared\nwebzip [0 [21 | -0.0072 | 0.0016\nwget 0.0016 in our dataset.\nTinkwalker 0.0015\nasterias 0.0015\nTable 2. Top 10 favored and disfavored\nrobots.  is the standard deviation of AP(r).\npyright Ellis Horowitz, 2011-2022 10\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "+  BREADTH-FIRST SEARCH: A web crawling algorithm that examines all pages at a given level before moving on to the next level."
        ],
        "definitions": [
          "+  Level: The hierarchical structure of web pages, where each page has a level (e.g., level 0 for the starting page, level 1 for its direct neighbors, etc.)"
        ],
        "formulas": [],
        "algorithms": [
          "+  BREADTH-FIRST SEARCH algorithm:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "slide_011.png",
        "raw_text": "Breadth-first Search\nLevel\nExamine all pages at level  before examining pages at level i+1"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "**Depth-first Search**: A traversal algorithm used to explore a graph or tree data structure."
        ],
        "definitions": [
          "**At each step move to page down the tree**: This phrase describes the behavior of the Depth-first Search algorithm, where it moves down one level in the tree at each iteration."
        ],
        "formulas": [
          "Note that there are no mathematical formulas or examples in this content, so the categories  and [EXAMPLE] remain empty. However, since this algorithm is a key concept in web crawling, it's essential to focus on understanding how it works and applying it correctly."
        ],
        "algorithms": [
          "**Depth-first Search Algorithm:**"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "slide_012.png",
        "raw_text": "Depth-first Search\nAt each step move to  page down the tree"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Web Wide Crawl",
          "PageRank algorithm developed by Google for determining page value",
          "Breadth-First Search (BFS) crawling brings in high-quality pages early"
        ],
        "definitions": [
          "PageRank: an algorithm for determining the value of a page",
          "BFS (Breadth-First Search): a graph traversal algorithm that visits nodes level by level"
        ],
        "formulas": [],
        "algorithms": [
          "BFS crawling process (although not explicitly described, it's mentioned as a method that brings high-quality pages early)"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "slide_013.png",
        "raw_text": "Web Wide Crawl (328M pages) [Najo01]\nBFS crawling brings in high quality\n2° pages early in the crawl\n0 5 10 15 20 25 30 35 40 45 50 55\nDay of crawl\nAverage PageRank score by day of crawl\nPage Rank is an algorithm developed by\nGoogle for determining the value of  page\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* Web crawling process",
          "* Crawling a webpage involves downloading its contents and extracting links to other relevant pages"
        ],
        "definitions": [
          "* Queue (Q): a data structure used to hold URLs to be crawled, with the front of the queue being the next URL to be processed",
          "* Inverted index: an index that maps words or phrases to their corresponding documents or webpages"
        ],
        "formulas": [],
        "algorithms": [
          "Web Crawling Algorithm:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "slide_014.png",
        "raw_text": "Initialize queue (Q) with initial set of known URL’s.\nLoop until  empty or page or time limit exhausted:\nPop  URL, call it L, from the front of Q.\nIf  is not an HTML page (e.g. .gif, jpeg, ....)\ncontinue the loop\nIf  has already been visited, continue the loop.\nDownload page, P, for\nIf cannot download  (e.g. 404 error, robot excluded)\ncontinue loop\nIndex  (e.g. add to inverted index and store cached copy)\nParse  to obtain list of new links N.\nAppend  to the end of\nEnd loop"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "How new links are added to the queue determines the search strategy",
          "Search strategies can be breadth-first (FIFO) or depth-first (LIFO)",
          "Focused crawlers direct their search towards \"interesting\" pages",
          "Heuristic ordering of URLs in the queue can improve crawling efficiency"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "Focused Crawling: New Approach by S. Chakrabarti et al (algorithm for re-ordering URLs in the queue based on their relevance and frequency of change)"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "slide_015.png",
        "raw_text": "* How new links are added to the queue determines the search strategy.\n¢ FIFO (append to end of Q) gives breadth-first search.\n¢ LIFO (add to front of Q) gives depth-first search.\n¢ Heuristically ordering the  gives  “focused crawler” that directs its\nsearch towards “interesting” pages; e.g.\n—  document that changes frequently could be moved forward\n—  document whose content appears relevant to some topic. can be moved\nforward\n— e.g. see Focused Crawling:  New Approach by ’S. Chakrabarti et al\n— https://www.sciencedirect.com/science/article/pii/ 1389128699000523\n* One way to re-order the URLs on the queue is to:\n* Move forward URLs whose In-degree is large\n* Move forward URLs whose PageRank is large\n— We will discuss the PageRank algorithm later\nCopyright Ellis Horowitz, 2011-2022 15\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  The web is a graph, not a tree, which means that links can be bidirectional and cyclic.",
          "*  An Accrawler must efficiently index URLs as well as already visited pages.",
          "*  To determine if a URL has already been seen, the crawler must store URLs in a standard format and develop a fast way to check if the URL has already been seen."
        ],
        "definitions": [
          "*  Accrawler: A type of web crawler that efficiently indexes URLs as well as already visited pages.",
          "*  Web graph: The structure of links between web pages, which is bidirectional and cyclic."
        ],
        "formulas": [],
        "algorithms": [
          "*  To determine if a URL has already been seen:",
          "*  To determine if a new page has already been seen:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "slide_016.png",
        "raw_text": "* Accrawler must detect when revisiting  page that has already been crawled\n(Remember: the web is  graph not  tree).\n¢ Therefore,  crawler must efficiently index URLs as well as already visited\npages\n* To determine if  URL has already been seen,\n— Must store URLs in  standard format (discussed ahead)\n— Must develop  fast way to check if  URL has already been seen\n¢ To determine if  new page has already been seen,\n— Must develop  fast way to determine if an identical page was already indexed\n— Must develop  fast way to determine if  near-identical page was already\nindexed\nCopyright Ellis Horowitz, 2011-2022 16\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Finding all links in a page and extracting URLs is crucial for web crawling."
        ],
        "definitions": [
          "*  **Relative URL**: A URL that must be completed to form a complete absolute URL.",
          "*  **Absolute URL**: A complete URL that can be used as is, without any additional processing."
        ],
        "formulas": [],
        "algorithms": [
          "HIGH"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "slide_017.png",
        "raw_text": "¢ Must find all links in  page and extract URLs;\nvar links = document.querySelectorAll(\"a\");\nfor (var  = 0;  < links.length; i++) {\nvar link = links[i].getAttribute(\"href\");\nconsole.log(link); }\n— But URLs occur in tags other than <a>, e.g.\n— <frame src=“site-index.html’>, <area, href=\"‘...”>, <meta>, <link>, <script>\n* Relative URL’ must be completed, e.g. using current page URL or <base> tag\n— <ahref=“proj.html’> to http://www.myco.com/special/tools/proj.html\n— < href=‘../outline/syllabus.html”> to http://www.myco.com/special/outline/syllabus.html\n* Two Anomalies\n1. Some anchors don’ have links, e.g. < name=“‘banner”’>\n2. Some anchors produce dynamic pages which can lead to looping\n< href=http://www.mysite.com/search?x=arg1 &y=arg2>\nCopyright Ellis Horowitz, 2011-2022 17\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* URLs are often long and storing all unique URLs can require large amounts of storage space",
          "* Hashing on host/domain name to determine uniqueness",
          "* Trie data structure for efficient lookup",
          "* Delta-encoded text file for compact storage"
        ],
        "definitions": [
          "* **Terabyte (TB)**: 1 trillion bytes = 1,000 GB",
          "* **Petabyte (PB)**: 1 million terabytes = 1,000 TB",
          "* **Trie data structure**: a compact digital trie, or prefix tree, used to store and retrieve strings",
          "* **Delta-encoded text file**: a method of storing URLs as the difference between consecutive URLs"
        ],
        "formulas": [],
        "algorithms": [
          "2. Use trie data structure to determine if path/resource is same as one in URL database",
          "1. Store each entry as the difference (delta) between current and previous URL",
          "+ Checkpointing: store full URL periodically"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "slide_018.png",
        "raw_text": "* URLs are rather long, 80 bytes on the average, implying 1 trillion URLs will require 80\nTerabytes\n* Recently Google reported finding 30 trillion unique URLs, which by the above\nwould require 2400 terabytes (or 2.4 petabytes) to store\n1. One Proposed Method: To determine if  new URL has already been seen\n— First hash on host/domain name, then\n— Usea trie data structure to determine if the path/resource is the same as one in the URL\ndatabase\n2. Another Proposed Method: URLs are sorted lexicographically and then stored as\ndelta-encoded text file\n* Each entry is stored as the difference (delta) between the current and previous\nURL; this substantially reduces storage\n+ However, restoring the actual URL is slower, requiring all deltas to be applied to\nthe initial URL\n* To improve speed, checkpointing (storing the full URL) is done periodically\nCopyright Ellis Horowitz, 2011-2022 18\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "**Tries**: A data structure that can store multiple \"words\" with the same prefix."
        ],
        "definitions": [
          "**Endmarker symbol**: A special character ($), used to indicate the end of a word.",
          "**Viterbi algorithm**: Not explicitly defined, but mentioned as having time complexity O(NK)."
        ],
        "formulas": [
          "**Search time for trie**: O(K)",
          "**Search time for binary search tree**: O(K \\* log N)"
        ],
        "algorithms": [
          "**Viterbi algorithm**: Not explicitly defined, but mentioned as having time complexity O(NK).",
          "**Greedy algorithm (grep)**: Not explicitly defined, but mentioned as being used to determine if a new URL is in the set."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "slide_019.png",
        "raw_text": "¢ Simplest (and worst) algorithm to determine\nif  new URL is in your set\n- grep - <search_url> <url_file>\n— For  URLs and maximum length K, viterbi.\ntime is O(NK)\n¢ Characteristics of tries ar\n— They share the same prefix among es fee febem\nmultiple “words”\n— Each path from the root to  leaf ea  /\\\ncorresponds to one “word” /people /courses — /people /courses _/people../courses\n— Endmarker symbol, $, at the ends of all\nwords ; ; If we store  URLs, each of maximum length K,\n. To avoid confusion between words in  binary search tree, then the search time is\nwith almost identical elements O(K*log N); however, using a-trie, the search\n— Assume all words are $ time is O(K), at the expense of more storage\nterminated\nCopyright Ellis Horowitz, 2011-2022 19\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  **URL Normalization**: The process of standardizing URLs to eliminate variations."
        ],
        "definitions": [
          "*  **Hash**: A unique string of characters generated from a URL, used to identify it.",
          "*  **Canonicalization**: The process of selecting a single, preferred version of a URL when there are multiple variations."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*"
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "slide_020.png",
        "raw_text": "¢ For example, all the following URLs have the same meaning (return the\nsame web page), but different hashes:\n— http://www.google.com\n— http://www.google.com/\n— https://www.google.com\n— www.google.com\n— google.com\n— google.com/\n— google.com.\nCopyright Ellis Horowitz, 2011-2022 20"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  URL Normalization",
          "*  Scheme and host components of a URL are case-insensitive",
          "*  Percent-encoding triplet is case-insensitive",
          "*  Default port (port 80 for the \"http\" scheme) can be removed from or added to a URL"
        ],
        "definitions": [
          "*  Scheme: refers to the protocol used in a URL (e.g., http, https)",
          "*  Host: refers to the domain name or IP address of a website",
          "*  Percent-encoding triplet: a sequence of characters preceded by a percentage sign (%)"
        ],
        "formulas": [],
        "algorithms": [
          "*  URL Normalization:"
        ],
        "examples": [
          "*  Convert \"HTTP://www.Example.com/\" to lowercase: http://www.example.com/",
          "*  Capitalize letters in escape sequences: \"%3A\" -> \"%C2%\"",
          "*  Decode percent-encoded octets of unreserved characters: http://www.example.com/%7Eusername/ -> http://www.example.com/~username/",
          "*  Remove default port: http://www.example.com:80/bar.html -> http://www.example.com/bar.html"
        ],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "slide_021.png",
        "raw_text": "1. Convert the scheme and host to lower case. The scheme and host\ncomponents of the URL are case-insensitive.\n— HTTP://www.Example.com/ — http://www.example.com/\n2. Capitalize letters in escape sequences. All letters within  percent-encoding\ntriplet (e.g., '\"%3A\") are case-insensitive, and should be capitalized.\nExample:\n—_ http://www.example.com/a%c2%b1b >\nhttp://www.example.com/a%C2% 1b\n3. Decode percent-encoded octets of unreserved characters.\nhttp://www.example.com/%7Eusername/ —\nhttp://www.example.com/~username/\n4. Remove the default port. The default port (port 80 for the “http” scheme)\nmay be removed from (or added to)  URL. Example:\n—_ http://www.example.com:80/bar.html —\nhttp://www.example.com/bar.html\n. See https://en.wikipedia.org/wiki/URL_normalization\nCopyright Ellis Horowitz, 2011-2022 21\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Spider Trap: A situation where a crawler re-visits the same page over and over again.",
          "Session ID Management: The use of unique IDs to keep track of visitors, often used in J2EE, ASP, .NET, and PHP.",
          "URL Length Monitoring: A technique to avoid spider traps by monitoring the length of URLs and stopping if it gets too long."
        ],
        "definitions": [
          "Spider Trap: A situation where a crawler re-visits the same page over and over again due to unique IDs in URLs.",
          "Session ID: A unique identifier used to keep track of visitors."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "www.webmasterworld.com/page.php?id=2646844 13484654: An example URL with a unique ID that can lead to a spider trap."
        ],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "slide_022.png",
        "raw_text": "¢  spider trap is when  crawler re-visits the same page over and over again\n¢ The most well-known spider trap is the one created by the use of Session ID’\n— J2EE, ASP, .NET, and PHP all provide session ID management\n¢  Session ID is often used to keep track of visitors, and some sites puts  unique\nID in the URL:\n— Anexample is www.webmasterworld.com/page.php?id=2646844 13484654\n(Note this URL doesn' exist).\nEach user gets  unique ID and it' often requested from each page.\nThe problem here is when Googlebot comes to the page, it spiders the page and\nthen leaves, it goes to another page and it finds  link to the previous page, but\nsince it has been given  different session id now, the link shows up as another\nURL.\n* One way to avoid such traps is for the crawler to be\ncareful when the querystring “ID=“ is present in the URL\n+ Another technique is to monitor the length of the URL,\nand stop if the length gets “too long”\nCopyright Ellis Horowitz, 2011-2022 22\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* First generation of spam web pages (use of repeated terms)",
          "* Second generation of spam web pages (cloaking)",
          "* Third generation of spam web pages (doorway pages)"
        ],
        "definitions": [
          "* Keyword stuffing: using high frequency of repeated terms to score high on search engines",
          "* Cloaking: technique used by spammers to return different page to crawlers than users",
          "* Doorway page: a page designed to rank highly for certain keywords but returns commercial page when browser requests it"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Example of keyword stuffing: \"We sell custom cigar humidors. Our frequency custom cigar humidors are handmade.\"",
          "* Explanation of cloaking technique"
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "slide_023.png",
        "raw_text": "¢ The first generation of spam web pages consisted of\npages with  high number of repeated terms, so as to Google' example of keyword stuffing:\nscore high on search engines that ranked by word\nWe sell custom cigar humidors. Our\nfrequency custom cigar humidors are handmade.\n+ + If ‘re thinki  buyi\n— Words were typically rendered in the same color as se eARAGE! olecse. contacrour\nthe background, so as to not be visible, but still custom cigar humidor specialists at\ncount custom.cigar.humidor@example.com.\n* The second generation of spam used  technique called\ncloaking;\n— When the web server detects  request from\ncrawler, it returns  different page than the page it\nreturns from  user request\n— The page is mistakenly indexed\n¢  third generation, called  doorway page, contains text and metadata chosen to rank\nhighly on certain search keywords, but when  browser requests the doorway page it\ninstead gets  more “commercially oriented” (more ads) page\n* Cloaking and doorway pages are not permitted according to Google’ webmaster\nsuggestions\nSee http://support.google.com/webmasters/bin/answer.py?hl=en&answer=66355\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "**DNS Caching Server**: A server that stores DNS responses to reduce the number of requests made to the DNS resolver.",
          "**UDP for DNS**: The use of User Datagram Protocol (UDP) for transmitting DNS requests and responses.",
          "**Parallel Threads Waiting**: The ability of a web crawler to perform multiple tasks concurrently using parallel threads."
        ],
        "definitions": [
          "**DNS Resolver**: A process that resolves domain names to IP addresses.",
          "**Client-Server Architecture**: The design pattern where a client requests and receives data from a server.",
          "**Caching**: Storing frequently accessed data in a temporary storage area to reduce the number of requests made to the original source."
        ],
        "formulas": [],
        "algorithms": [
          "**Web Crawler Algorithm**:",
          "**DNS Request-Response Process**:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "slide_024.png",
        "raw_text": "Caching DNS Asyne UDP\n(slack about DNS prefetch\nexpiry dates client . .\n*  co » The diagram points out all\nDNS Text indexing Text 33\nco resolver and other repository geys  the key elements of\nDNS client (UDP) analyses & index. £4*= crawler;\ncache £ D2 Notice\nMEG PSISRISIORIRIOESIOD OEE BOE | BzB .\naaaaann nn nae nennnaaae naa ano 2s  1. The DNS caching server\n2. Use of UDP for DNS\nHyperlink :\n— wait  ait HTTP extractor & 3. Load and thread monitor\nes ‘for HTTP send normalizer 4. Parallel threads waiting\n| DNS Socket  for  page to download\navaliable)\nPer-server\nqueues Page fetching context/thread isPageKnown? sk\nmeta-data 5S\n7 Is\nire\nLoad monitor Persistent URL\n& work-thread global work isUrlVisited? approval\nmanager pool of URLs guard\nCopyright Ellis Horowitz, 2011-2022 24\nOO eGS37OM"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Measuring and tuning a crawler for peak performance involves improving URL parsing speed, network bandwidth speed, and fault tolerance."
        ],
        "definitions": [
          "*  Refresh Strategies: the frequency at which the crawling process is restarted.",
          "*  Duplicate pages: pages that have been previously crawled and do not need to be recrawled.",
          "*  Mirror sites: websites that contain identical or similar content as another website."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "slide_025.png",
        "raw_text": "¢ Measuring and tuning  crawler for peak performance eventually\nreduces to\n— Improving URL parsing speed\n— Improving network bandwidth speed\n— Improving fault tolerance\n* More Issues (some of which are discussed ahead)\n— Refresh Strategies: how often is the process re-started\n— Detecting duplicate pages\n— Detecting mirror sites\n— Speeding up DNS lookup (see previous slide)\n— URL normalization (discussed earlier)\n— Handling malformed HTML\nCopyright Ellis Horowitz, 2011-2022 25"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "DNS lookup implementation",
          "DNS caching server"
        ],
        "definitions": [
          "DNS lookup: the process of resolving domain names to IP addresses",
          "DNS caching: storing previously resolved IP-domain name mappings for future use",
          "Pre-fetching client: making DNS resolution requests while parsing a page",
          "UDP (User Datagram Protocol): a protocol used for DNS resolution"
        ],
        "formulas": [],
        "algorithms": [
          "1. DNS caching:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "slide_026.png",
        "raw_text": "—  common operating system' implementation of DNS lookup is\nblocking: only one outstanding request at  time; so\n1. DNS caching: build  caching server that retains IP-domain\nname mappings previously discovered\n2. Pre-fetching client\n* once  page is parsed,\n— immediately make DNS resolution requests to the caching server;\nand\n— ifunresolved, use UDP (User Datagram Protocol) to resolve from\nthe DNS server\n3. Customize the crawler so it allows issuing of many resolution\nrequests simultaneously; there should be many DNS resolvers\nCopyright Ellis Horowitz, 2011-2022 26\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* Network delay in downloading individual pages is a bottleneck in web crawling",
          "* Having multiple threads running in parallel can improve throughput",
          "* A thread of execution is the smallest sequence of programmed instructions that can be managed independently by the scheduler"
        ],
        "definitions": [
          "* Thread: the smallest sequence of programmed instructions that can be managed independently by the scheduler",
          "* Process: a component of which threads are part"
        ],
        "formulas": [],
        "algorithms": [
          "* Distribute URL's to threads to guarantee equitable distribution of requests across different hosts",
          "* Early Google spider had multiple coordinated crawlers with about 300 threads each, downloading over 100 pages per second in 2010"
        ],
        "examples": [
          "* Google downloads ~50,000 pages/second or a billion+ pages in a day (as estimated in 2021)",
          "* Early Google spider's coordinated crawlers with multiple threads"
        ],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "slide_027.png",
        "raw_text": "* One bottleneck is network delay in downloading individual pages.\n¢ Itis best to have multiple threads running in parallel each requesting  page\nfrom  different host.\n— athread of execution is the smallest sequence of programmed instructions that\ncan be managed independently by  scheduler.\n— In most cases,  thread is  component of  process.\n— Multiple threads can exist within the same process and share resources\n* Distribute URL’ to threads to guarantee equitable distribution of requests\nacross different hosts to maximize through-put and avoid overloading any\nsingle server.\n* Early Google spider had multiple coordinated crawlers with about 300 threads\neach,\n— together they were able to download over 100 pages per second back in 2010\n— Itis estimated that in 2021 Google downloads ~50,000 pages/seconds or\nAbillion+ in  day, S€ https://www.quora.com/How-many-pages-are-Google-bots-crawling-every-second\nCopyright Ellis Horowitz, 2011-2022 27\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  **Centralized crawler control**: a system where one main crawler controls multiple parallel crawlers running on a LAN",
          "*  **Distributed crawling**: a system where multiple crawlers run on widely distributed machines with or without cross communication"
        ],
        "definitions": [
          "*  **Parallel crawler**: a process that consists of multiple crawling processes communicating via local network (intra-site parallel crawler)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  **Scenario 1: Centralized crawler control on LAN**: describes a system where one main crawler controls multiple parallel crawlers running on a local area network",
          "*  **Scenario 2: Distributed crawling on widely distributed machines**: describes a system where multiple crawlers run on widely distributed machines with or without cross communication"
        ],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "slide_028.png",
        "raw_text": "* Once the crawler program itself has been optimized, the next issue to\ndecide is how many crawlers will be running at any time\n¢ Scenario 1:  centralized crawler controling  set of parallel crawlers all\nrunning on  LAN\n—  parallel crawler consists of multiple crawling processes\ncommunicating via local network (sometimes called an intra-site\nparallel crawler)\n¢ Scenario 2:  distributed set of crawlers running on widely distributed\nmachines, with or without cross communication\nCopyright Ellis Horowitz, 2011-2022 28\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  Distributed crawlers must periodically update master index"
        ],
        "definitions": [
          "*  Incremental update: generally \"cheap\" due to compression and differential updates"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Organizing crawlers by country, region, or available bandwidth as a strategy for managing distributed crawlers"
        ],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "slide_029.png",
        "raw_text": "¢ If crawlers are running in diverse\ngeographic locations, how do we\norganize them\n— By country, by region, by available bandwidth\n— Distributed crawlers must periodically update\nmaster index\n— But incremental update is generally “cheap”\n¢ Why? Because\n— a. you can compress the update, and\n— b. you need only send  differential update\nboth of which will limit the required\ncommunication\nCopyright Ellis Horowitz, 2011-2022 29"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* **Scalability**: ability to handle large-scale web-crawls",
          "* **Network-load dispersion and reduction**: distributing network load by dividing the web into regions"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "* **Crawl strategy**: a method for determining which pages to crawl next"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "slide_030.png",
        "raw_text": "¢ Benefits:\n— scalability: for large-scale web-crawls\n— costs: use of cheaper machines\n— network-load dispersion and reduction: by dividing the web into regions\nand crawling only the nearest pages\n¢ Issues:\n— overlap: minimization of multiple downloaded pages\n— quality: depends on the crawl strategy\n— communication bandwidth: minimization\nCopyright Ellis Horowitz, 2011-2022 30"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* **** Three strategies for web crawling: Independent, Dynamic assignment, Static assignment"
        ],
        "definitions": [
          "* **** Independent strategy: \"no coordination, every process follows its extracted links\"",
          "* **** Dynamic assignment: \"a central coordinator dynamically divides the web into small partitions and assigns each partition to a process\"",
          "* **** Static assignment: \"Web is partitioned and assigned without central coordinator before the crawl starts\""
        ],
        "formulas": [
          "* No formulas or equations are explicitly stated, so no  label."
        ],
        "algorithms": [
          "* **** None explicitly stated, but implied that each strategy has its own algorithm for web crawling"
        ],
        "examples": [
          "* **** No specific example given, but understanding of the three strategies can be applied to various scenarios"
        ],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "slide_031.png",
        "raw_text": "— Three strategies\n1. Independent:\n> no coordination, every process follows its extracted\nlinks\n2. Dynamic assignment:\n> acentral coordinator dynamically divides the web\ninto small partitions and assigns each partition to\nprocess\n3. Static assignment:\n> Web is partitioned and assigned without  central\ncoordinator before the craw] starts\nCopyright Ellis Horowitz, 2011-2022 31\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Inter-partition links",
          "Handling inter-partition links in web crawling",
          "Firewall mode, Cross-over mode, Exchange mode for handling inter-partition links"
        ],
        "definitions": [
          "Inter-partition links: Links between different partitions in a web crawling system",
          "Firewall mode: A method of handling inter-partition links where the process does not follow them",
          "Cross-over mode: A method of handling inter-partition links where the process follows them and discovers more pages",
          "Exchange mode: A method of handling inter-partition links where processes exchange URLs"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 32,
        "slide_file": "slide_032.png",
        "raw_text": "Links from one partition to another (inter-partition links) can\nbe handled in one of three ways: Partitioni —_—~Partition2\n1 Firewall mode: [ |\nprocess does not follow any ;\ninter-partition link ' '\n2 Cross-over mode: iat\nprocess also follows inter- ' rt '\npartition links and possibly —\ndiscovers also more pages in its ' rt '\npartition if\n3 Exchange mode: LLLZSAM [NS INK ee\nprocesses exchange inter-\npartition URLs; this mode\nrequires communication\nCopyright Ellis Horowitz, 2011-2022 32\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  Exchange mode limitations",
          "*  Batch communication",
          "*  Replication in web crawling",
          "*  URL-hash based partitioning",
          "*  Site-hash based partitioning",
          "*  Hierarchical partitioning (e.g., by TLD)",
          "*  Firewall crawlers and their benefits",
          "*  Cross-over approach for 100% quality"
        ],
        "definitions": [
          "*  Exchange mode: a method of communication in web crawling where processes exchange information",
          "*  Batch communication: sending multiple URLs at once from one process to another",
          "*  Replication: duplicating popular URLs at each process to reduce exchange overhead"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 33,
        "slide_file": "slide_033.png",
        "raw_text": ". If exchange mode is used, communication can be limited by:\n— Batch communication: every process collects some URLs and\nsends them in  batch\n— Replication: the  most popular URLs are replicated at each\nprocess and are not exchanged (previous crawl or on the fly)\n. Some ways to partition the Web:\n—  URL-hash based: this yields many inter-partition links\n- Site-hash based: reduces the inter partition links\n— Hierarchical: by TLD, e.g. .com domain, .net domain ...\n. General Conclusions of Cho and Garcia-Molina\n— Firewall crawlers attain good, general coverage with low cost\n- Cross-over ensures 100% quality, but suffer from overlap\n— Replicating URLs and batch communication can reduce\noverhead\nCopyright Ellis Horowitz, 2011-2022 33\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "+ Combination of policies for Web crawler behavior",
          "+ Selection policy",
          "+ Re-visit policy",
          "+ Politeness policy",
          "+ Parallelization policy"
        ],
        "definitions": [
          "+ Selection policy: states which pages to download",
          "+ Re-visit policy: states when to check for changes to the pages",
          "+ Politeness policy: states how to avoid overloading websites",
          "+ Parallelization policy: states how to coordinate distributed web crawlers"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 34,
        "slide_file": "slide_034.png",
        "raw_text": "* The behavior of  Web crawler is the outcome of\ncombination of policies:\n—  selection policy that states which pages to download.\n—  re-visit policy that states when to check for changes to the pages.\n—  politeness policy that states how to avoid overloading websites.\n—  parallelization policy that states how to coordinate distributed web\ncrawlers.\nCopyright Ellis Horowitz, 2011-2022 34\nTn"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  **Dynamic Web**: The web is dynamic, with many new pages, updated pages, deleted pages, etc.",
          "*  **Page Update Tracking**: Periodically check crawled pages for updates and deletions to maintain freshness."
        ],
        "definitions": [
          "*  **LastModified indicator**: A timestamp indicating the last time a page was modified."
        ],
        "formulas": [],
        "algorithms": [
          "*"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 35,
        "slide_file": "slide_035.png",
        "raw_text": "¢ Web is very dynamic: many new pages, updated pages, deleted pages,\nete.\n¢ Periodically check crawled pages for updates and deletions:\n— Just look at LastModified indicator to determine if page has changed,\nonly reload entire page if needed\n¢ Track how often each page is updated and preferentially return to\npages which are historically more dynamic.\n* Preferentially update pages that are accessed more often to optimize\nfreshness of more popular pages.\nCopyright Ellis Horowitz, 2011-2022 35"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "1. **** Steady crawler: runs continuously without pause",
          "2. **** Shadowing: implies new set of pages are collected and stored separately",
          "3. **** In-place updating: updates index current by replacing old versions with new ones",
          "4. **** Multiple crawlers: typically used by search engines to improve crawling efficiency"
        ],
        "definitions": [
          "1. **** Shadowing: a method of collecting and storing new pages separately from the current database",
          "2. **** In-place updating: updating the index by replacing old versions with new ones without separating them"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 36,
        "slide_file": "slide_036.png",
        "raw_text": "¢  steady crawler runs continuously without pause\n— Typically search engines use multiple crawlers\n¢ When  crawler replaces an old version by  new page, does it do it\n“in-place” or “shadowing”\n— Shadowing implies  new set of pages are collected and stored\nseparately and all are updated at the same time\n— The above implies that queries need to check two databases, the current\ndatabase and the database of new pages\n— Shadowing either slows down query processing or decreases freshness\n¢ Conclusions:\n— running multiple types of crawlers is best\n— Updating in-place keeps the index current\nCopyright Ellis Horowitz, 2011-2022 36"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* Re-visiting policies ()",
          "* Uniform policy ()",
          "* Proportional policy ()",
          "* Average freshness ()",
          "* Cho and Garcia-Molina's result that uniform policy outperforms proportional policy in terms of average freshness ()"
        ],
        "definitions": [
          "* Uniform policy: re-visiting all pages with the same frequency, regardless of change rate ()",
          "* Proportional policy: re-visiting pages more often based on their estimated change frequency ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 37,
        "slide_file": "slide_037.png",
        "raw_text": "* Two simple re-visiting policies\n— Uniform policy: This involves re-visiting all pages in the\ncollection with the same frequency, regardless of their rates of\nchange.\n— Proportional policy: This involves re-visiting more often the pages\nthat change more frequently. The visiting frequency is directly\nproportional to the (estimated) change frequency.\n* Cho and Garcia-Molina proved the surprising result that, in terms\nof average freshness, the uniform policy outperforms the\nproportional policy in both  simulated Web and  real Web crawl.\n* The explanation for this result comes from the fact that, when\npage changes too often, the crawler will waste time by trying to re-\ncrawl it too fast and still will not be able to keep its copy of the page\nfresh.\n* To improve freshness, we should penalize the elements that change\ntoo often\nCopyright Ellis Horowitz, 2011-2022 37\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* Sitemap is a list of pages accessible to crawlers",
          "* Helps search engine crawlers find pages on the site"
        ],
        "definitions": [
          "* Sitemap: A list of pages of a web site accessible to crawlers",
          "* XML (Extensible Markup Language): Used as the standard for representing sitemaps"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Example XML sitemap for a three-page website"
        ],
        "priority": "MEDIUM",
        "slide_number": 38,
        "slide_file": "slide_038.png",
        "raw_text": "¢  sitemap is  list of pages of  web site accessible to crawlers\n¢ This helps search engine crawlers find pages on the site\n* XML is used as the standard for representing sitemaps\n¢ Here is an example of an XML sitemap for  three page . .\nwebsite Back in 2006 Google introduced\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> the sitemap format; now Bing,\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> Yahoo, and Ask also support\n<url> sitemaps\n<loc>http: //www.example.com/?id=who</loc>\n<lastmod>2009-09-22</lastmod>\n<changefreq>monthly</changefreq> See the Google, Bing, Yahoo, Ask announcement:\n<priority>0.8</priority> </url> http://www.google.com/press/pressrel/sitemapsorg.html\n<url>\n<loc>http: //www.example.com/?id=what</loc>\n<lastmod>2009-09-22</lastmod>\n<changefreq>monthly</changefreq>\n<priority>0.5</priority> </url>\n<url>\n<loc>http: //www.example.com/?id=how</loc>\n<lastmod>2009-09-22</lastmod>\n<changefreq>monthly</changefreq>\n<priority>0.5</priority> </url>\n</urlset> Copyright Ellis Horowitz, 2011-2022 38\nEOE>$EeeeeeeGO“_OO"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "Use consistent, fully-qualified URLs to ensure Google crawls your site correctly.",
          "Sitemaps can be posted anywhere on your site but only affect descendants of the parent directory.",
          "Session IDs should not be included in sitemap URLs."
        ],
        "definitions": [
          "Fully-qualified URL: A URL that includes the domain name and path (e.g., https://example.com/path/to/page)."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Hreflang tags can be used to indicate alternate URLs for different languages or regions:"
        ],
        "priority": "MEDIUM",
        "slide_number": 39,
        "slide_file": "slide_039.png",
        "raw_text": "¢ Use consistent, fully-qualified URLs. Google will crawl your URLs exactly as listed\n«  Asitemap can be posted anywhere on your site, but  sitemap affects only descendants of\nthe parent directory\n¢ Don' include session IDs from URLs in your sitemap.\n¢ Sitemap files must be UTF-8 encoded, and URLs escaped appropriately.\n¢ If you have two versions of  page, list in the sitemap only the one you prefer to appear in\nsearch results\n¢ Ifyou have alternate pages for different languages or regions, you can use hreflang to\nindicate the alternate URLs.\n. <head>\n<title>Widgets, Inc</title>\n<link rel=\"alternate\" hreflang=\"en-gb\"\nhref=\"https://en-gb.example.com/page.htm1\" />\n<link rel=\"alternate\" hreflang=\"en-us\"\nhref=\"https://en-us.example.com/page.html\" />\n<link rel=\"alternate\" hreflang=\"en”\nhref=\"https://en.example.com/page.html\"  />\n¢ There are many sitemap generator tools, e.g. https://slickplan.com/sitemap\nCopyright Ellis Horowitz, 2011-2022 39\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* Multiple crawlers used by Google (e.g., Googlebot, AdsBot)",
          "* Different types of crawlers for specific tasks (e.g., images, news, video)",
          "* Importance of understanding how Googlebot sees a website",
          "1. **Multiple Crawlers Used by Google**  (HIGH)",
          "3. **Importance of Understanding How Googlebot Sees a Website**  (HIGH)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* The mention of specific examples (e.g., Googlebot Images, AdsBot Mobile Web) can be considered as examples",
          "2. **Examples of Specific Crawlers**: Googlebot Images, AdsBot Mobile Web, Googlebot Video, etc."
        ],
        "priority": "MEDIUM",
        "slide_number": 40,
        "slide_file": "slide_040.png",
        "raw_text": "* Google now uses multiple crawlers “we osetitey neon\n— APIs-Google ‘orients seers ta ae goalie\nAdSense sme ites alc\n— AdsBot Mobile Web oar  )\n_ Googlebot Images car Geel Pineapster aaa\n— Googlebot News see tae en it\n— Googlebot Video “ ooetecseesin\n— Googlebot (desktop) =| =|\n— Googlebot (smartphone) cmonanen | nee | Sone\n— Mobile AdSense STE inant SST\n— Mobile Apps Android “See\n— Google Read Aloud For details see\nhttps://support.google.com/webmasters/answer/1061943?hl=en\nsee also Google' tool for checking how Googlebot sees your website\nhttps://support.google.com/webmasters/answer/6066468?rd=2\nCopyright Ellis Horowitz, 2011-2022 40\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "* Googlebot cannot see within Flash files, audio/video tracks, and content within programs",
          "* Many versions of Googlebot are run on multiple machines located near the site they are indexing",
          "* Importance of creating an empty robots.txt file to prevent \"File not found\" in website error log"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Creating an empty robots.txt file to prevent \"File not found\" in website error log",
          "* Using \"nofollow\" meta tag to prevent Googlebot from following any links on a page",
          "* Adding \"rel='nofollow'\" attribute to individual links to prevent Googlebot from following them"
        ],
        "priority": "MEDIUM",
        "slide_number": 41,
        "slide_file": "slide_041.png",
        "raw_text": "* Begins with  list of webpage URLs generated from previous crawls\n* Uses Sitemap data provided by webmasters\n¢ Many versions of Googlebot are run on multiple machines located near\nthe site they are indexing\n* Googlebot cannot see within Flash files, audio/video tracks, and content\nwithin programs\n° Advice\n— To prevent “File not found” in  website’ error log, create an empty\nrobots.txt file\n— To prevent Googlebot from following any links on  page, use\n“nofollow” meta tag\n— To prevent Googlebot from following an individual link, add\n“rel=‘nofollow’” attribute to the link\nCopyright Ellis Horowitz, 2011-2022 4l"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "**Web Crawling**: The process of automatically scanning and indexing web pages to gather information.",
          "**IP Address Verification**: Verifying the IP address of a web crawler to ensure it is legitimate."
        ],
        "definitions": [
          "**Reverse DNS Lookup**: A process of looking up an IP address in the DNS system to retrieve its corresponding domain name.",
          "**Forward DNS Lookup**: A process of looking up a domain name in the DNS system to retrieve its corresponding IP address."
        ],
        "formulas": [],
        "algorithms": [
          "**Method 1 for Verifying Googlebot's IP Address**",
          "**Method 2 for Verifying Googlebot's IP Address**"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 42,
        "slide_file": "slide_042.png",
        "raw_text": "* Method 1\n1. Runa reverse DNS lookup on the accessing\nIP address from your logs, using\nthe host command.\n2. Verify that the domain name is\neither googlebot.com or google.com.\n3. Runa forward DNS lookup on the domain\nname retrieved in step 1 using\nthe host command on the retrieved domain\nname. ‘\n“creationTime\": \"2022-07-21T15:54:44.265580\",\n. : ee “prefixes”:\n4. Verify that it' the same as the original revere ezenevge tet)\naccessing IP address from your logs. (cipvevetin' dooisaacovanni 32/64\"),\n¢ Method 2 {clpvercetin-: “auotsdeets4attsiess/ee\")\n. (Cipvepeetix' “2001:4e60s4801s1880/60°),\n— match the crawler' IP address to the list of coer cdot tteas agen aes /eany”\n1   Ip dd {\"ipv6Prefix\": \"2001:4860:4801:1b::/64\"},\nooglebo addresses ; oes\nCopyright Ellis Horowitz, 2011-2022 42\nccc"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  Crawl rate: The number of requests per second Googlebot makes to a site when crawling it."
        ],
        "definitions": [
          "*  Recrawl: Requesting Google to crawl new or updated content on a site.",
          "*  Googlebot: A program that crawls the web for Google search engine."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Crawl rate example: 5 requests per second.",
          "*  URL for requesting recrawl: https://developers.google.com/search/docs/advanced/crawling/ask-google-to-recrawl"
        ],
        "priority": "MEDIUM",
        "slide_number": 43,
        "slide_file": "slide_043.png",
        "raw_text": "¢ The term crawl rate means how many requests per second Googlebot\nmakes to your site when it is crawling it: for example, 5 requests per\nsecond.\n¢ You cannot change how often Google crawls your site, but if you want\nGoogle to crawl new or updated content on your site, you can request\nrecrawl; for details see\nhttps://developers.google.com/search/docs/advanced/crawling/ask-google-to-recrawl\n¢ You can reduce the crawl rate by\n— returning pages with 500, 503 or 429 http status codes\n— Setting  new rate in the Search Console\n—  video discussing Google’ Crawl Status Report can be found here:\nhttps://support.google.com/webmasters/answer/9679690\nCopyright Ellis Horowitz, 2011-2022 43"
      },
      {
        "lecture": "web_crawling",
        "concepts": [
          "*  Googlebot must understand and execute JavaScript code to extract meaningful features from web pages.",
          "*  Browsers render HTML hierarchy, but also include transformations via CSS and JavaScript."
        ],
        "definitions": [
          "*  Googlebot: a search engine's software agent that crawls the web and indexes content.",
          "*  DOM (Document Object Model): a hierarchical representation of an HTML document."
        ],
        "formulas": [],
        "algorithms": [
          "*  3-phase process for Googlebot to process web pages with JavaScript:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 44,
        "slide_file": "slide_044.png",
        "raw_text": "* Why: browsers don’ just render the DOM\nhierarchy of HTML, they include transformations URLs\nvia CSS and JavaScript, and for Googlebot to\nextract the most meaningful features from  web | index |\npage it would be necessary to have access to these Susie it HTML\ntransformations\n— Therefore Googlebot must understand and 3\nexecute JavaScript code\n* Conclusion: Googlebot and Chrome share  great 8\ndeal of code\n* Googlebot processes web pages with JavaScript .\nin 3 phases\n1. Crawling — processing all links\n2. Rendering — executing JS and then looping\nback to 1\n3. Indexing\n* As of 2019 Googlebot runs the latest Chromium\nrendering engine\n* Note: Server-side rendering saves Googlebot\nfrom rendering the page\nccc"
      }
    ],
    "web_serving_basics": [
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  Web Trends",
          "*  Measurements"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  None explicitly mentioned on this slide, but it's likely that the lecture will provide examples of web trends and measurements later on."
        ],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "slide_001.png",
        "raw_text": "Web Trends and Measurements\nCopyright"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "+ The web has undergone significant changes over the last 30+ years",
          "+ Understanding the different dimensions of the web is crucial for building a web search engine today",
          "+ Scale, complexity, and growth are important factors to consider"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "+ Building a web search engine today requires understanding the different dimensions of the web",
          "+ Many early slides come from Mary Meeker, a prominent venture capital firm (Kleiner Perkins Caufield & Byers)"
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "slide_002.png",
        "raw_text": "¢ Web has changed dramatically over the last 30+ years\n¢ If one is building  web search engine today it is important to\nunderstand the different dimensions of the web\n— Scale, complexity and growth are only  few important\nfactors\n¢ In today’ lecture  try to quantify some of the trends to better\nunderstand where the web is going and why\n* many of the early slides come from Mary Meeker (formerly of)\nKleiner, Perkins, Caufield and Byers,  major venture capital\nfirm, http://www.kpcb.com/\nCopyright"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* The internet is used by a significant portion of the world's population ()",
          "* The internet penetration rate can be expressed as a percentage of the total population ()"
        ],
        "definitions": [
          "* Internet penetration rate: The percentage of people in a region who use the internet ()",
          "* Region: A geographic area such as North America, Europe and Central Asia, etc. ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* The chart shows the share of population using the internet for different regions ()",
          "* The fact that 5.03 billion people use the internet today is an example of the internet's widespread usage ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "slide_003.png",
        "raw_text": "total of 5.03 billion people around the world use the internet today —\nequivalent to 63.1 percent of the world' total population\nShare of the population using the internet\n@ Add country\nNorth America\nry Europe and Central Asia\nss Middle East and North Africa\nLatin America and Caribbean\nEast Asia and Pacific\n40! South Asia\nSub-Saharan Africa\nCHART MA AB OUR £ DOWNLOA -\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "1. Global Internet Properties () - refers to popular websites with a global reach",
          "2. China's growing influence on the internet ()",
          "3. Top 10 Internet Properties by Global Monthly Unique Visitors ()"
        ],
        "definitions": [
          "1. Baidu - Chinese search engine ()",
          "2. Tencent - Chinese holding company of internet properties ()",
          "3. Sohu.com Inc. - Chinese online media and community service provider ()",
          "4. comScore - a company that tracks website traffic and user behavior ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Amazon.com ()",
          "2. China's growing influence on the internet, with >86% of users outside America ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "slide_004.png",
        "raw_text": "OSCE 2/14 — 6 of Top 10 Global Internet Properties ‘Made in USA’...\nhighly popular Internet websites; >86% of Their Users Outside America...China Rising Fast\nBaidu is  Chinese search engine Top 10 Internet Properties by Global Monthly Unique Visitors, 3/14\nGoogle\nTencent is  Chinese holding Kcr\ncompany of Internet properties,\nFacebook Ts\namong the most popular being,\nQQ, for chatting; Yao! Ty\nSohu.com Inc. is  Chinese online Wikipedia\nmedia, search, gaming, Alibatya (MM\ncommunity and mobile service Said wen Gcere\ngroup. Tencent [S| = International Users\nSohu MU\nAmazon.com [ii\n0 200 400 600 800 1,000 1,200 1,400\nMonthly Unique Visitors (MMs)\n@ PCB Source: comScore, 3/4, 131\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "Mobile internet users growth rate in China",
          "Year-over-year (Y/Y) growth comparison"
        ],
        "definitions": [
          "Y/Y growth rate: a measure of change from one year to another, calculated as the percentage increase or decrease over the same period."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "The comparison of China Mobile Internet Users' growth rate between +9% and +8%."
        ],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "slide_005.png",
        "raw_text": "\\ China Mobile Internet Users =\n817MM.+9% vs. +8% Y/\nChina Mobile Internet Users vs. Y/ Growth\n8 China Mobile Internet Users == Y/ Growth\nie\nCopyright\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "1. **** China Mobile Internet Usage Leaders: Tencent, Alibaba, Baidu dominate 71% of mobile time spent",
          "2. **** Share of Mobile Time Spent: WeChat leads with ~200 minutes per user, average QQ"
        ],
        "definitions": [
          "1. **** WeChat: a messaging and social media app in China",
          "2. **** Tencent Video: an online video streaming service owned by Tencent",
          "3. **** Baidu Browser: a web browser developed by Baidu",
          "4. **** AliPay: a mobile payment platform owned by Alibaba"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** Tencent dominates mobile internet usage in China with 71% market share",
          "2. **** WeChat leads with ~200 minutes per user, average QQ"
        ],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "slide_006.png",
        "raw_text": "fe\nTA 4\nChina Mobile Internet Usage Leaders...\nTencent + Alibaba + Baidu = 71% of Mobile Time Spent\nShare of Mobile Time Spent, April 2016 = WeChat\nDaily Mobile Time Spent = ~200 Minutes per User, Average QQ\n= QQ Browser\nTencent = Tencent Video\n= Tencent News\nTencent Games\n= QQ Music\n= JD.com\nAll Others = QQ Reading\n29%\n= UCWeb Browser\nTaobao\n= Weibo\n= YouKu Video\n= Momo\n= Shugi Novel\n=AliPay\nAutoNavi\n4 = Mobile Baidu\n. = iQiyi / PPS Video\nBaidu = Baidu Browser\nBaidu Tieba\n91 Desktop\nBaidu Maps\nAll Other\nOO eGS37OM"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "slide_007.png",
        "raw_text": "1 bed\n1,473.4 1,465.5\n1,437.2 aS\n1,372.6 1,354.8 ‘\n1,301.7 1,281.2\n5 1,018.7\n1 6\n75 725.3\n494.5\n304.7\n173.5\n2009 = =201¢ ¢ 2 2 2014 2015 2 2017. 201 19 ) 2021\nDetails: Worldwide; IDC; 2009 to 2021\n_——"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* The amount of global digital information created and shared is increasing exponentially",
          "* Digital info has grown 9x in five years to nearly 2 zettabytes in 2011, per IDC."
        ],
        "definitions": [
          "* Zettabyte: 1 zettabyte = 1,024 exabytes",
          "* Exabyte: 1 exabyte = 1,024 petabytes",
          "* Petabyte: 1 petabyte = 1,024 terabytes",
          "* Terabyte: 1 terabyte = 1,024 gigabytes"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* The growth of digital information from documents to pictures to tweets in online information."
        ],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "slide_008.png",
        "raw_text": "World’ Content is Increasingly Findable + Shared + Tagged -\nDigital Info Created + Shared up 9x in Five Years\nAmount of global digital information created & shared\nThere has been exponential growth — from documents to pictures to tweets —\nin online information: grew 9x in five years to nearly 2 zettabytes* in 2011, per IDC.\n1 Zettabyte = 1,024 Exabytes\n1 Exabyte = 1,024 Petabytes Global Digital Information Created & Shared, 2005 — 2015E\n1 Petabyte = 1,024 Terabytes 8\n1 Terabyte = 1,024 Gigabytes =\nor 2.\n1 Zettabyte = 1,000,000,000,000 §£ 6\ngigabytes 5\nao]\n2s\n== 2\n3 ; |\ni)\n2005 2007 2009 2011 2013E 2015E\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* The growth of photo sharing remains robust despite new platforms emerging",
          "* Photos uploaded and shared daily number is doubling every year (since 2005-2014)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Yahoo making a major upgrade to Flickr (+500 million)",
          "* Instagram being purchased by Facebook for $1 billion (2010)",
          "* Snapchat's photo messaging application developed by Stanford students ($9B valuation)"
        ],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "slide_009.png",
        "raw_text": "| Photos Alone = 1.8B+ Uploaded & Shared Per Day...\nGrowth Remains Robust as New Real-Time Platforms Emerge\n500 million photos are Daily Number of Photos Uploaded & Shared on Select Platforms,\nuploaded every day and that 2005 - 2014YTD\nnumber is doubling every year ‘a0\nYahoo has recently made   +500\nmajor upgrade to Flickr  . = Flickr\n3 @ Snapchat\nInstagram was in 2010 é eae iSinetagram\npurchased by Facebook for ed =\n$1 billion 8s 900 = Facebook\n25 WhatsApp (2013, 2014 only)\nSnapchat is  photo messaging 2 600\napplication developed by two 2\nStanford students ($9B  300\nvaluation); *\n2005 2006 2007 2008 2009 2010 2011 2012 2013 2014YTD\n1 7\nbobby Murphy - Evan Spiegel\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "Online Video & Entertainment",
          "Hours of video uploaded to YouTube every minute (as a key metric)",
          "Growth rate of online video content between 2014 and 2020"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "The example of YouTube uploading 500 hours of video every minute in February 2020"
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "slide_010.png",
        "raw_text": "Internet » Online Video & Entertainment\nHours of video uploaded to YouTube every minute as of February 2020\n* DOWNLOAD\naro it\n00 < Source\n—~ Show sources\n* Show publishe\nsoo Use Ask Statist\nRelease date\nRegion\nWorldwide\n100 Survey time pe\nfas) 60 ia ¢ 2007 to Febr\n6 15 20 Supplementary\nn'0; May Mar'10 Ne May ‘1  May 14 May 0 revious figures\n* As of February 2020, more than 500 hours of video were uploaded to YouTube every minute.\n* This equates to approximately 30,000 hours of newly uploaded content per hour.\n* The number of video content hours uploaded every 60 seconds grew by around 40 percent\nbetween 2014 and 2020. 10\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  Mobile devices (excluding tablets) generate a significant portion of global website traffic.",
          "*  The percentage of mobile device-generated traffic has consistently hovered around 50% since 2017."
        ],
        "definitions": [
          "*  StatCounter: A tool used to track and analyze website traffic data (not explicitly defined, but implied)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Q1 2015 to Q2 2022: Period of time considered in the analysis (example of a specific timeframe)"
        ],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "slide_011.png",
        "raw_text": "In the second quarter of 2022, mobile devices (excluding tablets) generated 58.99 percent of\nglobal website traffic, consistently hovering around the 50 percent mark since the beginning of\n2017 before permanently surpassing it in 2020.\nbed\n¢ 58.999 bd\n54.55. 0578455.799\naA RDP E53, s50852.4% ___ SLs ERS Gig 2.2% . &\n48.33% 7 Ah78-9 19\n44.699 6\n= 42.16%\n38.38: BSA Te\n3 32.85%)\n31.16%,\nPI CPHLAAFPSAPAA SALAS SAA CHAI SPADA CAAA\nDetails: Worldwide; StatCounter; Q1 2015 to Q2 2022; excluding desktop and tablet devices\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "1. **** Tablet growth is more rapid than smartphone growth, specifically iPad growth is ~3x iPhone growth.",
          "2. **** The rate of tablet adoption (iPad) vs. smartphone adoption (iPhone) can be compared using cumulative unit shipments."
        ],
        "definitions": [
          "1. **** Cumulative unit shipments: the total number of units shipped over a certain period of time (in this case, 12 quarters).",
          "2. **** Post-launch: referring to the period after a product is launched in the market."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** The comparison of iPhone and iPad cumulative unit shipments over 12 quarters post-launch, with specific numbers provided.",
          "2. **** The illustration of tablet growth (iPad) being ~3x faster than smartphone growth (iPhone)."
        ],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "slide_012.png",
        "raw_text": "Tablet Growth =\nMore Rapid than Smartphones, iPad = ~3x iPhone Growth\nFirst 12 Quarters Cumulative Unit Shipments, iPhone vs. iPad\n160,000\niPad =iPhone\n140,000\n= 120,000\n£ 100,000\neo\n= 80,000\n> 60,000\n°\n20,000\noO:\nt) 1 2 3 4 5 6 7 8 9 170 611 12\nQuarters After Launch\nSource: Apple, as of CQ1:13 (12 quarters post iPad launch).\nLaunch Dates: iPhone (6/29/07), iPad (4/3/10). 44\nCopyright"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* Product Finding = Often Starts @ Search (Amazon + Google...)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* 36% Search Engine"
        ],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "slide_013.png",
        "raw_text": "Product Finding =\nOften Starts @ Search (Amazon + Google...)\nWhere Do You Begin Your Product Search?\n49%\nAmazon\n36%\nSearch Engine\nCopyright"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  Technology Cycles: refers to the pattern of technological innovation and adoption in computing devices",
          "*  10-Year Cycle: a common trend where new technologies tend to last about 10 years before being replaced or surpassed"
        ],
        "definitions": [
          "*  Mainframe: an early type of computer that served as a centralized system for processing data and applications",
          "*  Mini: refers to the miniaturization of computers in the 1970s, which led to smaller and more portable devices",
          "*  Personal Desktop: a type of computer designed for individual use, popularized in the 1980s",
          "*  Internet Mobile: refers to the shift towards mobile internet access and computing devices (smartphones, tablets)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Wearables Coming on Strong: an example of a technology cycle accelerating faster than the typical 10-year trend"
        ],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "slide_014.png",
        "raw_text": "fe\nTechnology Cycles — Still Early Cycle on Smartphones + Tablets,\nNow Wearables Coming on Strong, Faster than Typical 10-Year Cycle\nTechnology Cycles Have Tended to Last Ten Years\nWearable /\nMainframe Mini Personal Desktop Internet Mobile Internet — Everywhere\nComputing Computing Computing Computing Computing Computing\n1960s 1970s 1980s 1990s 2000s 2014+\nate | | || femme |\\ = || Ee\nWY) \\\\\nOthers?\nImage Source: Computersciencelab.com, Wikipedia, IBM, Apple, Google, NTT docomo, Google, Jawbone, Pebble. 2"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "1. Re-Imagination of Computing Operating Systems ()",
          "2. Global Market Share of Personal Computing Platforms by Operating System Shipments ()"
        ],
        "definitions": [
          "1. Wintel ()",
          "2. Amiga ()",
          "3. TRS-80 ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. Global Market Share of Personal Computing Platforms by Operating System Shipments ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "slide_015.png",
        "raw_text": "RE\nRe-Imagination of Computing Operating Systems -\niOS + Android = 60% Share vs. 35% for Windows\nGlobal Market Share of Personal Computing Platforms by Operating System Shipments, 1975 — 2012\n1983 1998 — 2005 2012\nWintel - 25% Wintel - 96% Wintel - 35%\n100%\nFZ 90%\nrs\n5 80%\nP=\n3S\n70%\nBs\nE= 60%\nag TRS-80 WinTel\n5S 50%\nEm 40%\nof -\n2 =\né  30%\no” 20%\n£ Other\n10% '\n$ ° . Amiga Android\n= 0%\n= 1975 1977 1979 1981 1983 1985 1987 1989 1991 1993 1995 1997 1999 2001 2003 2005 2007 2009 2011\niB Source: Asymco.com (as of 2011), Public Filings, Morgan Stanley Research, Gartner for 2012 data. 109\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "Market Share of Cloud Hosting Providers",
          "Leading Cloud Hosting Providers (Microsoft Azure, Alibaba Group, Google Cloud Platform, etc.)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "slide_016.png",
        "raw_text": "Market Share Of Leading Cloud Hosting Providers\nTop 10 Providers by Total 2020 Market Share\nMicrosoft Azure\nAlibaba Group\nGoogle Cloud Platform\nRackspace\nCenturyLink\nIBM Cloud\nOVHcloud\nDigitalOcean, Inc.\nPercent of Market Share\nSource: intricately data, April 2021\n__——"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* Amazon Web Services (AWS) is leading the cloud charge"
        ],
        "definitions": [
          "* S3: AWS' storage product, used as a proxy for AWS scale/growth"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "slide_017.png",
        "raw_text": "...While The Cloud Rises\nAmazon Web Services (AWS) Leading Cloud Charge...\nObjects Stored in Amazon S3* (B)\n2,000\n1,500\n°\noO\nan\n4,000\n: !\na)\n5 500\n“ '\n0 ee |\na4 a4 a4 a4 a4 a4 Qi Q3 Q2\n2006 2007 2008 2009 2010 2011 2012 2012 2013\n@     *Note: S3 is AWS’ storage product and used as proxy for AWS scale / growth 74\nSource: Company data.\nCopyright\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* Cloud Revenue Re-Accelerating",
          "* Volume Effects"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Amazon AWS = Microsoft Azure = Google Cloud  (note: this is more of a comparison than an example)"
        ],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "slide_018.png",
        "raw_text": "...Computing Big Bangs Volume Effects =\nCloud Revenue Re-Accelerating +58% vs. +54% Q/\nCloud Service Revenue — Amazon + Microsoft + Google\nmAmazon AWS =«=Microsoft Azure = Google Cloud\nCopyright\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  Voice-related commands have increased significantly since 2008 after the launch of iPhone and Google Voice Search."
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "slide_019.png",
        "raw_text": "Google Trends imply queries associated with voice-related commands have\nrisen >35x since 2008 after launch of iPhone & Google Voice Search\nGoogle Trends, Worldwide, 2008 — 2019\n“Navigate Home\n—~ Call Mom\nCall Dad\nmint cmtanahetnsinstl reese fete ne\n2008 2009 2010 2011 2012 2013 2014 2015 2019\nCopyright Ellis Horowitz, 2011-2022 19\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  Voice-Based Mobile Platform Front-Ends: The idea that voice can replace typing in mobile interactions",
          "*  Natural / Conversational Language: The use of natural language processing (NLP) to understand user requests"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  The process of using voice-based mobile platform front-ends to replace typing, but no specific steps are provided."
        ],
        "examples": [
          "*  Google Assistant: An example of a voice-based mobile platform front-end",
          "*  Nearly 70% of Requests are Natural / Conversational Language: A statistic illustrating the prevalence of natural language processing in user requests"
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "slide_020.png",
        "raw_text": "RE\nVoice-Based Mobile Platform Front-Ends =\nVoice Can Replace Typing\nGoogle Assistant\nNearly 70% of Requests are Natural / Conversational Language, 5/17\nlL 20% of Mobile Queries Made via Voice, 5/16\nCc 4  QO\nKLEINER Se: ood v0 16, map: Nacumon 21\nPERKINS Copyright\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  Voice assistants (e.g., Amazon Echo)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Amazon Echo as an example of a voice assistant"
        ],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "slide_021.png",
        "raw_text": ".Voice =\n47MM Amazon Echo Base + ~2x in One Year\nAmazon Echo Installed Base Amazon Echo Skills\nCopyright Ellis Horowitz, 2011-2022 21\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* Growth in number of users connected",
          "* Transition from desktop/laptop use to mobile",
          "* Move away from server farms to cloud computing",
          "* Decreased dominance of Microsoft Windows"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "slide_022.png",
        "raw_text": "* Growth in number of users connected\n* Growth in Smartphone use\n* Growth in digital data, especially photos and video\n* Growth in Social Media as an advertising platform\n* Transition from desktop/laptop use to mobile\n* Growth in tablet usage over desktops/laptops\n* Decreased dominance of Microsoft Windows\n* Move away from server farms to cloud computing\n* Growth in voice communication with devices\nCopyright\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* The World Wide Web is dynamic and hard to describe accurately over time"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Measuring the Web by various methods, such as number of websites, languages of web pages, rate of change of pages, etc."
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "slide_023.png",
        "raw_text": "* The World Wide Web (the Web, the publicly accessible web) is so\ndynamic it is hard to describe it and have the description be valid for\nvery long\n¢ In this lecture we look at what is known,\n— Measuring the Web by number of web sites\n— Measuring the Web by the Languages of Web Pages\n— Measuring the Web by Rate of Change of Pages\n— Measuring the Web by Document Content Type\n— Measuring the Web by linkage\n— Measuring the Web as  Graph\n— Measuring the Web by Content\n— (using the best statistics we can find)\nCopyright Ellis Horowitz, 2011-2022 23"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  The total number of websites on the internet (approximately 1.7 billion)",
          "*  The popularity of different web servers (Apache, nginx, Microsoft IIS)",
          "*  The percentage of inactive/parked websites (around 75%)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "slide_024.png",
        "raw_text": "Jan. 2020:\n~1.7 Billion sites Total number of Websites\n2,000,000,000 EE Websites\nnginx web server\nhad the largest\ngrowth; 1,500,000,000\nOver 50% of websites\nAre hosted either by 1,000,000,000\nApache or nginx;\nBut Microsoft web 500,000,000\nservers still power\n43.2% of all sites\nAround 75% of websites\nare not active, but parked\nhttp://www. internetlivestats.com/total-number-of-websites/\nCopyright\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* The number of websites in the world has been growing rapidly over the years ()",
          "* Websites growth rate from 1991 to 2021 is significant ()"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* The graph showing the growth of websites from 1991 to 2021 is an example of how website numbers have increased over time ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "slide_025.png",
        "raw_text": "et\nNumber of websites\nin the world\nThe global number of websites has more than\ndoubled from 2015 to 2021. Websites growth rate\n= from 1991 to 2021\n2 Billion\n1.6 1 8 6 Billion\nwebsites\nin 2021\n1.2\no8 863.1\n0.4\n1 23.5K 017.1  207.0\nAug, 1995 2000 2005 2010 2015 2021\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* Domain Count Statistics for TLDs (TLD = Top-Level Domains)",
          "* The importance of having a record count that represents all domains known about, which is usually more accurate than other sources"
        ],
        "definitions": [
          "* TLD: Top-Level Domain (e.g. .com, .tk, etc.)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Tokelau (a country) has a high number of domains in the .tk TLD"
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "slide_026.png",
        "raw_text": "Domain Count Statistics for TLDs\n136 million in .com, record, Our Count represents all domains we know about, whichis usually more accurate;\n21 million in .tk, 14 TLD @ Our Count ¢\nmillion in .de, etc 20m\ntk 21,014,704 é\n. . de 14,586,920 toom|~\nwhat is .tk and why is set saost ort\nit so large? (Tokelau) = 11,579,296 8M\norg 10,398,501\nom\nuk 10,361,314\ninfo 5,528,696 40M\nal 5,042,167\nru 4,928,746 20M\noh 3,673,059\nsony sow 0 Pree re ee TPP EPP EPP  PTET TE TT\nbr 3,282,608 ESSTESSE RS OSS ERS FREES ETS ESELIGE\nfr 3,195,248\ntop 2,921,081\nau 2,845,979\nit 2,786,780\nca 2,611,854\npl 2,302,199 PSS\nbiz 2,269,454 wow\nloan 2,253,686\nDisplay: Top25 - Default - None\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "**KEY CONCEPTS **"
        ],
        "definitions": [
          "**DEFINITIONS **"
        ],
        "formulas": [
          "Note that there are no mathematical formulas or algorithms in this content, so I did not mark anything with  or [ALGORITHM]."
        ],
        "algorithms": [],
        "examples": [
          "**EXAMPLES **"
        ],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "slide_027.png",
        "raw_text": "RE\n= =\nSchool of Engineering Web Page Language Diversity\n¢ The Web contains pages in many different languages\n* Characters in  language are encoded such that each character is paired with  number\n* Unicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together\nconstitute  modern, unified character encoding.\n* Most modern web browsers feature automatic character encoding detection. In Firefox, for\nexample, see the View/Character Encoding submenu, shown below\n¢ In HTML one can specify the character encoding using\n* <meta http-equiv=“Content-Type content=“text/html” charset=utf-8>\n°) Google - Mozilla Firefox Her .\nEle Edt RIMM History Bookmarks pls Help « If charset is missing ISO-\n‘olbars  .\n‘ ee >| = = 8859-1 is taken as the default\n= » odle.com @)- A) SB: a\\\\ 2: :\nPa : unless there is  browser\n+Ellis  ear - Ellis Horowitz | { «ff ;\nan Westen (308501 setting,\nBasco Fl ‘Customize List. Est European >| Western (150-6859-15)\nFirebug Big aT East Asian >| Western (IBM-850)\nSenter tsdiocazem >| ox (ows 1232) + Websites in non-western\nChinese Tadhtional (8ig5) Unicode >| Celtic (150-8859-14) :\nWestern (Windows-1252) pan languages typically use UTF-8\nGreek (IS0-8859-7) Unicode (UTF-16) Greek (MacGreek).\nCentral European (150-8859-2) User Defined Grosk (Windows 1253)\nIcelandic (MacIcelandic)\nNordic (IS0-8859-10)\n< —— ”\nmy  AICIEIESIWZNE\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "1. **Language diversity**: Estimated 40,000 languages created by humans, with only 6,000-9,000 still in use.",
          "2. **Internet language shift**: Decline of English as primary language among internet users from 80% to 40%."
        ],
        "definitions": [
          "1. **Content languages for websites**: Refers to the primary language used on a website.",
          "2. **Primary language**: The language in which most content is written on a website or spoken by its users."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **Study by the United Nations**: Examined pages on search engines to identify primary languages, resulting in 0-80% distribution of content languages among websites as of 2014.",
          "2. **English language dominance (1996-2008)**: Occupied roughly 80% of web pages during this period."
        ],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "slide_028.png",
        "raw_text": "¢ Itis estimated that about 40,000 different ;\nlanguages have been created by human an\nbeings German\n* Only between 6,000-9,000 are still in use Japanese\nish\n+ Study done by the United Nations Spanish\n— http://unesdoc.unesco.org/images/0018/ Chinese\n001870/187016e.pdf Ponvauese\nallan\n— The methodology was to examine the Polish\npages on  search engine and attempt to Turkish\nidentify the primary language in which Dutch\nthe page is written Others\n— Conclusions 0% 5% 10% 20% 30% 40% 50%\n* From 1996 — 2008 English was Content languages for websites as of 12 March 2014\npredominant, occupying roughly\n80% of web pages\n+ At the same time the number of\nInternet users who had English\nas their primary language\ndropped from 80% to 40%\nCopyright Ellis Horowitz, 2011-2022 28\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "**Web Serving Basics**: The fundamental principles of serving web content.",
          "**Java Applet/Servlet**: Java-based components used to extend the functionality of web servers."
        ],
        "definitions": [
          "**HTML (Hypertext Markup Language)**: A standard markup language used to create structure and content on the web."
        ],
        "formulas": [],
        "algorithms": [
          "**HTTP Request-Response Cycle**: A step-by-step process of how a client's request is processed by a server, including:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "slide_029.png",
        "raw_text": "5 Ww] | HTML\nJava ,\ni= of SS\n| ~ \\ Sa al"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* Content types have a wide range (16,000 to 51,000)",
          "* Parsing content types is necessary for various applications"
        ],
        "definitions": [
          "* Apache Tika toolkit: detects and extracts metadata and text content from documents using existing parser libraries",
          "* Indexing technology: used to organize and retrieve data efficiently (e.g., Lucene, Solr)"
        ],
        "formulas": [],
        "algorithms": [
          "+ Unifies parsers under a single interface",
          "3. Identify language they belong to (using N-grams)"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "slide_030.png",
        "raw_text": "¢ By some accounts, there are 16,000 to 51,000 content types*\n* What to do with content types?\n— Parse them\n* How? The Apache Tika™ toolkit detects and extracts metadata and text content from\nvarious documents using existing parser libraries. Tika unifies these parsers under\nsingle interface. Tika is useful for search engine indexing\n+ Extract their text and structure\n— Index their metadata\n¢ Use an indexing technology like\n* Lucene, http://lucene.apache.org/\n¢ Solr, http://lucene.apache.org/solr/\n* Google Search Appliance (http://www.google.com/enterprise/search/products/gsa.html)\n— Identify what language they belong to\n« N-grams\n*http://filext.com/ (see if you can name the top 20 file extensions)\nCopyright Ellis Horowitz, 2011-2022 30\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "slide_031.png",
        "raw_text": "{ what fe types can Googe nc» Wg\n€\nConsidering the fact that there are thousands\nGoogle = of file types of content stored on the web,\nGoogle actually indexes only  small number,\nWebmaster Tools | Heprene less than 3 dozen, but they may well constitute\nWhat file types can Google index? far more than  majority of the available content\nLearn more Google can index the content of most types of pages and ~ Related\nfles. The most common fle types we index includ: Flash and other rich media files\n«Ade Fash (0\none * Adobe Portable Document Format (pdf) Google+ Webmaster FAQ.\nitelinks * Adobe PostScript ( ps)\ntow are videos rank + Microsoft PowerPoint (ppt, pts)\nmo\nC# source code (cs)\nhttp://support.google.com/webmasters/bin/answer.py?hl=en&answer=35287\nCopyright Ellis Horowitz, 2011-2022 31\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  The Web has approximately 86 billion websites",
          "*  The distribution of websites across TLDs (e.g., .com) is uneven, with 72% in .com",
          "*  The number of web pages is estimated to be around 30 trillion unique URLs",
          "*  HTML, PDF, Word, Excel, PPT, and others are examples of content types",
          "*  The Web graph follows a power law distribution for in-degree and out-degree",
          "+ The Web has approximately 86 billion websites ()",
          "+ Distribution of websites across TLDs is uneven ()",
          "+ Number of web pages estimated to be around 30 trillion unique URLs ()",
          "+ HTML, PDF, Word, Excel, PPT, and others are examples of content types ()",
          "+ Storage required to hold a single snapshot of the Web: 100 petabytes ()"
        ],
        "definitions": [
          "*  TLD (Top-Level Domain): e.g., .com, .org, etc.",
          "*  In-degree and out-degree distribution: refers to the number of incoming and outgoing links in a network",
          "+ Languages in which documents are written: English (55%), French, German, Spanish, Chinese ()"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Google's storage requirement: 24 petabytes per day",
          "*  Internet Archive's storage size: over 10 petabytes",
          "+ Categories of Content: pornography, spam, mirrors ()",
          "+ Cost of storing 1 petabyte: under $1,000 ()"
        ],
        "priority": "MEDIUM",
        "slide_number": 32,
        "slide_file": "slide_032.png",
        "raw_text": "+ How many websites? ~/.86 billion\n* How are they distributed across TLDs or across countries?\n— 112 million out of 148 million belong to .com or about 72%\n* How many web pages are there? 30 trillion unique URLs from Google found in 2012,\nsee http://googleblog.blogspot.com/2008/07/we-knew-web-was-big.html\n* Which content types hold the most information: HTML, PDF, Word, Excel, PPT, others?\n— There are thousands of different content types\n* How much storage is required to hold  single snapshot of the Web?\n—  trillion web pages at 100K bytes per page requires 100 petabytes\n* 1 petabyte storage costs under $1,000, so $100,000 of equipment will work\n— Google processes 24 petabytes per day, http://en.wikipedia.org/wiki/Petabyte\n— The Internet Archive has more than 10 petabytes, http://en.wikipedia.org/wiki/nternet_Archive\n* What are the languages in which the documents are written?\n— According to the Internet Archive, about 55% is in English, other popular languages include: French,\nGerman, Spanish and Chinese, also see http://en.wikipedia.org/wiki/Languages_used\\on_the_Internet\n* General properties of the Web graph\n—  In-degree and out-degree distribution follows  power law\n* Categories of Content: pornography, spam, mirrors\n— Presumably there is  lot of the above, but little concrete data on how much\nCopyright Ellis Horowitz, 2011-2022 32\nEOE>$EeeeeeeGO“_OO"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "Human editors were used by Yahoo! in the past to assemble large directories.",
          "The European Car Webzine focuses on prestige marques, includes articles, web broadcasting, screen savers, dealer business & economy."
        ],
        "definitions": [
          "Yahoo! Directory Search Results: a list of search results from Yahoo!'s directory.",
          "Umbrella organization for car sharing companies in Europe: an organization that oversees and coordinates the activities of car sharing companies in Europe."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "European Car Sharing Business and Economy - an example of a structured directory of car sharing companies in Europe.",
          "EuroNCAP (European New Car Assessment Programme) - aims to provide consumers with realistic and independent assessment of the safety performance of cars sold in Europe."
        ],
        "priority": "LOW",
        "slide_number": 33,
        "slide_file": "slide_033.png",
        "raw_text": "european cars - Yahoo! Directory Search Results - Mozilla Firefox Eek)\nfile Edit View History Bookmarks Thols Help\ncir. search. yahoo.com/sea = A006 SPS9E : a)-  = ee\n2 [etl asses “8:**-! ¢ Yahoo originally used\nhuman editors\nALIOO!, | european cars  editors to\nDIRECTORY assemble  large\nAlso try: european carnars, european cars for sal, More hierarchically\nshow  :\nnen European Car Sharing structured directory of\nBusiness and Economy Umbrella organization for car sharing companies in Europe.\n— vic heainn  web pages.\nats\nurepean New Car Assessment Programme (EuroNCAP) = .\nMore Aims to provide motoring Consumers with  realistic and independent assessment of the http://www.yahoo.com/\nsafety performance of cars sold in Europe, . 5\nsoy te Cen: Ren ones De Se 7| vanes ca retains ne\nLast months European Car fres Da lierarchy. as seen to the\nLast & months Take place September 22, 2000, to protest problems of urban mobility, air pollution, and left; under european cars\nLastyear noise. é\nwow.22september.org “ we see categories:\nClassicDriver com The European Car Webzine regional with 93 matches,\nFocuses on prestige marques, includes articles, web broadcasting, screen savers, dealer business & economy with\nwww. classiedtiver.com  79 matches, etc\n“ >\nCopyright Ellis Horowitz, 2011-2022 33\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "1. **ODP - Open Directory Project**:  A collaborative effort to organize the web (HIGH)",
          "2. **Ontology**:  A systematic arrangement of concepts or entities in a particular field or domain (MEDIUM)",
          "3. **Distributed directory**:  A system where data is stored and managed across multiple locations or servers (MEDIUM)",
          "4. **RDF format**:  A standard format for representing data on the web using resource description framework (LOW)"
        ],
        "definitions": [
          "1. **Open Directory Project (ODP)**:  A collaborative effort to organize the web, started by Netscape (HIGH)",
          "2. **Distributed directory**:  A system where data is stored and managed across multiple locations or servers (MEDIUM)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **DMOZ's categorization structure**:  An example of how DMOZ organizes websites into categories, such as Arts, Reference, Regional, etc. (HIGH)",
          "2. **RDF format usage**:  An example of how RDF format is used to represent data on the web (LOW)"
        ],
        "priority": "MEDIUM",
        "slide_number": 34,
        "slide_file": "slide_034.png",
        "raw_text": "°) ODP - Open Directory Project - Mozilla Firefox Hex) . .\nFle Eat ew Hitoy Booknars Tos Help * Open Directory Project,\n¢ wwe. dmoz ore ¢|/Q)- +|| || - .\nC2 | @ eee = 4\\8 effort to organize the web\n[AIEALIEZ open directory project Aol Search. according to an ontology;\nabout dmoz | dmozblog | suggest URL | help | link | editorlogin . Ps\n¢ An approach similar to\n(earch adrancea Yahoo’ s:\n°\nAuts Fone teat ¢ Based on the distributed\nMovies, Television, Music. Jobs, Real Estate, Investing. Intemet, Software, Hardware. .\nGames Heatth Home labor of volunteer editors\nVideo Games, RPGs, Gambling... Fitness, Medicine, Alternative... Family, Consumers, Cooking. (“net-citizens provide the\nKids and Teens News Recreation collective brain”).\nArts, School Time, Teen Life. Media, Newspapers, Weather. Travel Food, Outdoors, Humor.\nReference Regional Science ¢ Used by most other search\nMaps, Education, Libraries. US, Canada, UK, Europe. Biology, Psychology, Physics engines\nShopping Society Sports\nClothing, Food, Gifts People, Religion, Issues. Baseball, Soccer, Basketball. ° Started by Netscape.\nWorld\nCatala, Dansk, Deutsch, Espatiok Frangais, Italiano, fl ACER, Nederlands, Polok, Pyccrat Svensk _ http://www.dmoz.org/\n— ¢ Distributes its data using\n(EEE ep buita tne targest nurnan-edited directory of the web _\nRDF format\n4 976 587 sites - 93,429 editors - over 1 009,376 categories  DMOZ shut down in 2016\nCopyright Ellis Horowitz, 2011-2022 34\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "*  Open Directory - a online directory of web content",
          "*  Mozilla Firefox - a web browser"
        ],
        "definitions": [
          "*  Science category in the Open Directory (104,420 entries)",
          "*  Computers: Computer Science category in the Open Directory (2,971 entries)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  The Open Directory listing for \"Science\" and its subcategories",
          "*  The number of entries in the Science category (104,420)"
        ],
        "priority": "MEDIUM",
        "slide_number": 35,
        "slide_file": "slide_035.png",
        "raw_text": "Open Directory - Science - Mozilla Firefox ER) ]\nFile Edit View History Bookmarks Tbols Help File Edit View History Bookmarks Tbols Help\n@ open Directory - Science + ~ |} @ open Directory - Computers: Computer Scie. | :\nm™ @ wn dnas.orgiscience @- AB: 2 -he\n[HIGALCAL open citectry project Asi Search. [HIGALCAL open citectry project Aaigomrch,\nses [bag [er te Tak SESS\n(ech te ere recto __ Gaia ite entre arectry zi\nTop: Science (104,420) Description ‘Top: Computers: Computer Science (2,971) Description\n[AIBICIDIEIEIGIHITIJIKILIMINIQIPIQIRISIZIUIVIWIXIXI\n+ AcademicDepartments (655) People 271)\n+ Aston SOS\n‘\neee\n# Academic Departments (9)\nBy Region (0)\n¢ Educational Resomces (352)\nFioployment (69) # Reference (389) Computers gona 209\n@ Mathade and Tachnianac £7001 #\n3 ‘This category in other languages:\n: hoe ” . “ . ”\nSelecting Category “Science Selecting Category Computer Science\nCopyright\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "* The Internet Archive's snapshot of the World Wide Web",
          "* Apache Nutch",
          "* Wayback Machine",
          "* Crawler algorithm"
        ],
        "definitions": [
          "* Petabytes: a unit of digital information storage (approximately 1 petabyte = 1,000 terabytes)",
          "* Seed sites: initial websites used as starting points for web crawling"
        ],
        "formulas": [],
        "algorithms": [
          "+ Automatically opens links on those pages and archives content",
          "* Wayback Machine's database growth rate: approximately 100TB of data per month"
        ],
        "examples": [
          "* The Internet Archive has been taking snapshots every two months since 1997",
          "* Over 412 billion web pages saved over time",
          "* Rapid growth rate of the Wayback Machine's database (approximately 100TB/month)"
        ],
        "priority": "MEDIUM",
        "slide_number": 36,
        "slide_file": "slide_036.png",
        "raw_text": "¢ The Internet Archive has been taking  ae — —\nsnapshot of the World Wide Web every two   as\nmonths since 1997 — has used Apache Nutch\n¢ The results are made available through the Sear  Se\nWayback Machine, vrenver anenive\n* — Its database is approximately 4.5 petabytes (ayogenmnaenine rons soy\n* The founder is Brewster Kahle 412 Billion web pages saved over time. voxarr\n* For the past 13 years, the Internet Archive — . .\nhas been growing rapidly, most recently by iz =\" om BE\nabout 100TB of data per month. :\n¢ Their crawler surveys the web every two Ob toon BE) secon serie  swerszexon\nmonths The algorithm first performs  broad Wj chin iia Aste mi yout ce mange _\ncrawl that starts with  few \"seed sites,\" such Kopemeatakccis ie Ucneteniva®  tinaenettanoniemr\nas Yahoo' directory. After snapping  shot of —— _\nthe home page, it then moves to any referable\npages within the site until there are no more\npages to capture. If there are any links on\nthose pages, the algorithm automatically\nopens them and archives that content as well.\nCopyright Ellis Horowitz, 2011-2022 36\nccc"
      },
      {
        "lecture": "web_serving_basics",
        "concepts": [
          "1. **** Multilingual Databases: Databases that store information in multiple languages to cater to diverse user needs.",
          "2. **** Deep Web: A part of the internet that is not indexed by search engines and can only be accessed through specific browsers or tools, designed for anonymity (e.g., Tor).",
          "3. **** Dark Web: A subset of the Deep Web, often associated with illicit activities.",
          "4. **** Government Resources: Information stored in databases that are restricted to authorized personnel or require special access."
        ],
        "definitions": [
          "1. **** Deep Web Technologies: Refers to the technologies used to access and navigate the Deep Web.",
          "2. **** Tor: A browser designed for anonymity, used to access the Deep Web.",
          "3. **** Dark Web: A subset of the Deep Web characterized by illicit activities."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. **** Medical Records: Example of sensitive information stored in databases, which may be part of the Deep Web.",
          "2. **** Competitor Websites: Examples of websites that may provide valuable insights for businesses or organizations.",
          "3. **** Social Media: Platforms where individuals and organizations share information, potentially including subscription-based content."
        ],
        "priority": "MEDIUM",
        "slide_number": 37,
        "slide_file": "slide_037.png",
        "raw_text": "School of Engineering\n— :\ncea 18) rAC = VA =  —_\nAcademic Information        Multilingual Databases\nMedical Records Financial Records\nLegal Documents 2 Inteme but isn RE acces Government Resources\nVS fa feb. crawle\nScientific Reports ij fa S. Competitor Websites\nSubscription Information Social Media ee cvon-specific\n% Repositories\npart of the Deep Web accessible only through certain browsers such as Tor designed to\nensure anonymity. Deep Web Technologies has zdbosinvolvement with the Dark Web.\nllaaal Infarmatic ~ Rea Traffickina cites"
      }
    ],
    "youtube": [
      {
        "lecture": "youtube",
        "concepts": [
          "1.  **Video Search Engine**: A web-based search engine that crawls the web primarily for video content. (Slide 1)",
          "2.  **Indexing of Video Content**: The process of acquiring metadata associated with a video, such as author, title, creation date, duration, coding quality, tags, description, subtitles, and transcription.",
          "3.  **Ranking of Videos**: The process of ordering videos under a query based on relevance, user preferences, date of upload, number of views, or user rating."
        ],
        "definitions": [
          "1.  **Metadata**: Data associated with a video, such as author, title, creation date, duration, coding quality, tags, description, subtitles, and transcription.",
          "2.  **Video Recognition**: The process of identifying and extracting metadata from a video."
        ],
        "formulas": [
          "Note that there are no mathematical formulas in the provided content, so I did not mark any as ."
        ],
        "algorithms": [
          "1.  **Indexing Algorithm**: Acquiring meta-data associated with the video (e.g., author, title, creation date, duration, coding quality, tags, description)."
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 1,
        "slide_file": "s2.png",
        "raw_text": "«  video search engine is  web-based search engine which crawls the web\nprimarily for video content.\n— YouTube is not strictly  video search engine as it does not crawl the web\nlooking for video content\n* The indexing of video content is normally done by acquiring meta-data associated\nwith the video, e.g.\n— Author, title, creation date, duration, coding quality, tags, description\n— Other aspects of video recognition are subtitles (using formats STR or SUB) and\ntranscription (using format TTXT)\n* The ranking of videos under  query is generally done using:\n— Relevance: using metadata and user preferences\n— Ordered by date of upload\n— Ordered by number of views So indexing and ranking\n— Ordered by duration are  lot simpler than\n— Ordered by user rating cin search\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "+  Video search engines",
          "+  Web-wide video search engine",
          "+  All-content search engine",
          "+  Integrated universal search engine for science-oriented videos"
        ],
        "definitions": [
          "+  CastTV: A web-wide video search engine founded in 2006 (no longer existing)",
          "+  Munax: An all-content search engine that powers both nationwide and worldwide search engines with video search",
          "+  ScienceStage: An integrated universal search engine for science-oriented videos"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "+  CastTV (no longer existing)",
          "+  Munax (released their first version in 2005 and no longer active)",
          "+  ScienceStage (no longer active)",
          "+  Blinkx/RhythmOne (uses speech recognition and visual analysis to process downloaded video)"
        ],
        "priority": "MEDIUM",
        "slide_number": 2,
        "slide_file": "s3.png",
        "raw_text": "* Those no longer existing\n— CastTV was  Web-wide video search engine that was founded in 2006\n* No longer active\n— Munax released their first version all-content search engine in 2005 and powers\nboth nationwide and worldwide search engines with video search\n° http://www.munax.com/ no longer active\n— ScienceStage is an integrated universal search engine for science-oriented\nvideos. All videos are also semantically matched to millions of research\ndocuments from open-access databases.\n— No longer active\n*  few remain\n— Bing does crawl for videos, see https://www.bing.com/videos/\n% blinkx (renamed as RhythmOne) was launched in 2004 and uses speech\nrecognition and visual analysis to process downloaded video rather than rely on\nmetadata alone\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "* Video hosting is highly concentrated on a small number of websites due to large file sizes involved",
          "* YouTube.com has become the defacto site for uploading videos"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Vimeo.com, the first to support HD video, focuses on short, arty films",
          "* Vevo.com, a joint venture of major music companies, hosts high-quality videos",
          "* Dailymotion.com, owned by Vivendi, hosts high-quality videos"
        ],
        "priority": "MEDIUM",
        "slide_number": 3,
        "slide_file": "s4.png",
        "raw_text": "* Largely because of the large file sizes involved, video hosting is highly concentrated\non  fairly small number of websites\n— vimeo.com, first to support HD video, focuses on short, arty, films\n— vevo.com,  joint venture of Universal Music Group, Sony Music Entertainment\nand Warner Music Group\n— dailymotion.com, owned by Vivendi, hosts high quality videos\n* Most of these websites which host video allow their videos to,be embedded on other\nwebsites\n* YouTube.com has become the defacto site for uploading videos\n* — It is legal to crawl YouTube, see their Terms of Service,\nwww. youtube.com/static?template=terms\n* “3, You are not allowed to access the Service using any automated means (such as robots,\nbotnets or scrapers) except (a) in the case of public search engines,.in accordance with\nYouTube’ robots.txt file; or (b) with YouTube’ prior written permission;”\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Subscription video on demand services (e.g., Hulu, Netflix)",
          "*  Ownership structure of Hulu (jointly owned by major media companies)"
        ],
        "definitions": [
          "*  Subscription video on demand service: a service that allows users to access content for a fee",
          "*  Major media companies: large corporations involved in the production and distribution of media content (e.g., Disney, Comcast)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Hulu's ownership structure (jointly owned by Walt Disney, 21st Century Fox, Comcast, and Time Warner)",
          "*  Netflix's evolution from delivering DVDs to developing original content",
          "*  Recent entry of Disney+ into the market"
        ],
        "priority": "MEDIUM",
        "slide_number": 4,
        "slide_file": "s5.png",
        "raw_text": "¢ Hulu is an America subscription video on demand service jointly owned by Walt\nDisney, 21’ Century Fox, Comcast, and Time Warner\n— In December 2017, Disney acquired Fox' partial ownership, giving it  majority\nstake; other owners include Comcast\n* Netflix is an American subscription video on demand service, that originally delivered\nDVDs;\n— They develop their own content as well as offering content from major film\ndistributors\n¢« Amazon Prime is an American subscription video on demand service offering\ntelevision and file shows for rent or purchase\n¢ Disney+  recent entry\n* There are many others: XtremeHD, Sling TV, Apple TV+, HBO Max, Acorn TV, etc\n¢ Entertainment\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "**Video Subtitling Services**: There are three main types of video subtitling services: open caption, closed caption, and SDH (Subtitles for the Deaf and Hard of Hearing).",
          "**Speech Recognition**: used to extract phrases from audio transcripts for better indexing.",
          "**Text Recognition**: uses OCR on video slides to detect words."
        ],
        "definitions": [
          "**SRT**: stands for “SubRip Subtitle” file, a common subtitle/caption file format in text format."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**TalkMiner System**: an example of Text Recognition using OCR on video slides to detect words. (see https://www.youtube.com/watch?v=7N6L_m9LywM)"
        ],
        "priority": "MEDIUM",
        "slide_number": 5,
        "slide_file": "s6.png",
        "raw_text": "+ Subtitles: there are two formats, one for subtitles and one for transcripts\n— There are three main types of video subtitling services:\n1. open caption: burned into the video\n2. closed caption: can be turned on/off, generally at the bottom of the screen\n3. SDH (Subtitles for the Deaf and Hard of Hearing): similar to closed-caption, but\nincludes words describing actions or moods\n— SRT or SUB for subtitles\n* SRT. srt) stands for “SubRip Subtitle” file, and it' the most common\nsubtitle/caption file format. It is  text format\n— TYXT for transcripts\n+ Speech Recognition, used to extract phrases from audio transcripts for better indexing\n— Gaudi, Google Audio Indexing uses voice recognition to locate the exact spot where\nwords are spoken\n—_ https://www.searchenginejournal.com/google-audio-search-will-it-ever-be-\npossible/397129/\n— Text Recognition: uses OCR on video slides to detect words,\n— e.g. TalkMiner System, see https://www.youtube.com/watch?v=7N6L_m9LywM.\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  YouTube is a video hosting website",
          "*  The site allows users to upload, view, rate, share, add to favorites, report, and comment on videos",
          "*  Google acquired YouTube in 2006 for $1.65 billion"
        ],
        "definitions": [
          "*  Video hosting website: a platform that stores and delivers video content (not explicitly defined in the text, but implied)",
          "*  Web traffic analysis company: a company that analyzes online traffic patterns (specifically mentioned as Alexa Internet)"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  YouTube generated revenue of $19.8 billion in 2020",
          "*  The website was ranked as the second most popular site by Alexa Internet in January 2022"
        ],
        "priority": "MEDIUM",
        "slide_number": 6,
        "slide_file": "s7.png",
        "raw_text": "* YouTube is an American video :\nhosting website headquartered in San\nig\nBruno, California, created by three —\nformer PayPal employees: Chad Hurley, — dN\nSteve Chen, Jawed Karim in February\n2005.\n* In November 2006, it was bought by ' 9 cen one\nGoogle for US$1.65 billion ey eens -\n* In 2020 Google announced that\nYouTube generated revenue of $19.8 ap “= oon dec ee\nbillion “\n* The site allows users to upload, view, rate,\nshare, add to favorites, report and comment on ve wees\nvideos ee sama 2 293\n. In January 2022, the website was ranked as the soon pene once\nsecond most popular site by Alexa Internet,  \\\nweb traffic analysis company (now owned by\nAmazon)\n——See also For details see Related Articles page, Mar 2020\nhttps://en.wikipedia.org/wiki/List_of_mo\nst_popular_websites\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "* YouTube is a search engine ()",
          "* YouTube processes more than 3 billion searches per month ()",
          "* YouTube is transforming and has surpassed other search engines like Bing ()"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 7,
        "slide_file": "s8.png",
        "raw_text": "= . °\nSchool of Engineering\n¢ YouTube - The 2nd @ vatibe Te tnslges  — * *\nLargest Search SN eee\n. . . SS   ee ee ee\nEngine (cite hoa) aL\n* YouTube processes more YouTube is Transforming - os\nwa rocesses more than\nthan 3 billion searches   > | ioe Was We Dace 3 billion\nmonth le\n. searches  month\ntoh :\n¢ It' bigger than Bing, ME Founded in 2005 by\nYahoo!, Ask and AOL 2005 | three former PayPal Boughtin\ncombined! meee CCL Google 2006\nSIE by Google for\n. http://www.mushroomnetworks.com/ $1.65 billion\ninfographics/youtube---the-2nd- 5\nlargest-search-engine-infographic boing Aol.\n@ vi\n. Wa >\nBigger than Bing, Yahoo, Fastest growing video sharing\nAsk and AOL combined website in the world at the moment\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "1. **** YouTube traffic growth rate: 60 hours of video uploaded every minute",
          "2. **** Large-scale data storage capacity: estimated to be 1 sextillion gigabytes"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 8,
        "slide_file": "s9.png",
        "raw_text": "Ke\nschool of Engineering YOU  ube Traffic - Some Facts\n¢ As of 2021:\n— 60 hours of video are uploaded every minute, or one hour of\nvideo is uploaded to YouTube every second.\n— Over 4 billion videos are viewed  day\n— Over 800 million unique users visit YouTube each month\n— Over 3 billion hours of video are watched each month on\nYouTube\n— More video is uploaded to YouTube in one month than the 3\nmajor US networks created in 60 years\n— 70% of YouTube traffic comes from outside the US\n— YouTube is localized in 39 countries and across 54 languages\n— It is estimated that YouTube holds 1 sextillion gigabytes of data\n— _ https://www.quora.com/What-is-the-total-size-storage-capacity-of-YouTube-and-at-what-rate-is-it-\nincreasing-How-is-Google-keeping-up-with-the-increasing-demands-of- outube%E2%80%99s-capacity-\ngiven-that-thousands-of-videos-are-uploaded-every-day\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "YouTube's major hurdles (beyond crawling, indexing, and ranking)"
        ],
        "definitions": [
          "*  Content Distribution Network (CDN): a system for distributing videos worldwide",
          "*  ContentID system: YouTube's monetization system"
        ],
        "formulas": [],
        "algorithms": [
          "The YouTube Recommendation System (no specific details provided, but mentioned as a key concept)"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 9,
        "slide_file": "s10.png",
        "raw_text": "¢ Since crawling, indexing and ranking are not big challenges for\nYouTube, what are the major hurdles\n1. What video formats are acceptable\n— For uploading\n— For downloading\n2. How are videos to be displayed on: desktops, iPhones, iPads, Android\ndevices, etc\n3. How does YouTube distribute videos worldwide\n—  Acontent distribution network (CDN)\n4. How does YouTube monetize its website?\n— YouTube’  ContentID system\n5. How does YouTube keep users watching\n— The YouTube Recommendation System\nCopyright Ellis Horowitz, 2011-2022"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  YouTube requires a registered user to upload videos",
          "*  Channels include thumbnails of uploaded videos, members subscribed, favorite videos, friends' lists",
          "*  Having 1 million subscribers as a YouTuber can earn between $300,000 - $2 million per year",
          "*  To be in the top 1000 YouTubers, you must have approximately 1.8 million subscribers"
        ],
        "definitions": [
          "*  Registered user: a user who has created an account on YouTube to upload videos",
          "*  Channel: a type of account on YouTube that includes uploaded video thumbnails, subscribed members, favorite videos, and friends' lists",
          "*  YouTuber: an individual with a YouTube channel"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Pscrib With 1 million subscribers as a YouTuber",
          "*  Wop seagull Ellis Horowitz's YouTube channel"
        ],
        "priority": "MEDIUM",
        "slide_number": 10,
        "slide_file": "s11.png",
        "raw_text": "* In order to upload  video you must be  registered user\n* Inaddition YouTube offers  special type of account called  channel; channels\ninclude\n— thumbnails of videos you've uploaded,\n— members to whom you’ve subscribed,\n— videos from other members you've picked as favorites,\n— lists of members who are your friends,\npscrib 4 Me With 1 million subscribers,  YouTuber\n~ YOUR SUDSCTIDETS, Ant will make between $300,000 — $2 million\n* Biggest YouTube Channels as of 2021 To be in the top 1000 YouTubers you\nmust haye ~1.8 million subscribers\n, As of 09/2020, there are more than\nal 2000 YouTubers with over  million\nsubscribers\noo https://www.statista.com/statistics/277758/most-popular-youtube-\n\" channels-ranked-by-subscribers/\nWop seagull Ellis Horowitz, 2011-2022\nRe"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  YouTube captures videos",
          "*  Upload process on YouTube (making a video live)",
          "*  Video management on YouTube (adding more videos, custom thumbnail)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "+ Add custom thumbnail"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 11,
        "slide_file": "s12.png",
        "raw_text": "(@ 1 of 1 uploaded - YouTube Ellis\n@ Secure _https://wwwyoutube.com/ te °\nHE apps [) CSC1572Home Page [) CSCI571 Home Page [) CSCI351 Home Page [Elis Horowitz’ Hom... If] Computer Science... [) DynONS -- Host Se. FSlother Bookmarks\nYou Search * 20\nYouTube\ncaptures: Oe ene Im\nlo cick Publ to make your vdeo tive ft soe\nName  ee\nLennonBithday mb .\nDescription Upload status\n‘Upload comp Description iso share on\nTags Yervieo ies Ao ao\n‘nttps//youtu be/KX014x51 Rw. 9 |\nquality:\nNote: YouTube Tee ee\nimmediately seem “me\n: fp comer\nassigns  URI ie eos) woroTHuwenans\n] Cs ‘custom thumbnail\nNote: YouTube  to eae taxi (8\nsuggests possible _ - aa &\nthumbnails Video Manager + Add more videos\nVideo on How to Upload  Video\nhttps://support.google.com/youtube/answer/57407\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Video/Audio quality",
          "*  Encoding into streamable file format for faster video/audio quality"
        ],
        "definitions": [
          "*  Upload status: current state of uploaded video (Select language >)"
        ],
        "formulas": [],
        "algorithms": [
          "*  Encoding process: encode into streamable file format for faster video/audio quality (no specific steps provided)"
        ],
        "examples": [
          "*  Example of a YouTube upload status (\"Select language >\")"
        ],
        "priority": "MEDIUM",
        "slide_number": 12,
        "slide_file": "s13.png",
        "raw_text": "€\nHt apps) CSC1572 Home Page [) CSCI571 Home Page [) CSCI351 Home Page [} Ellis Horowitz’ 4ém... [ff Computer Science... [) DynDNS -- Host Se. » PBlother Bookmarks\n= Youlmn Search * sal (?)\n= y> EB click ‘Publish’ to make youssideo live. Draft saved,\nae\nOriginal language Translate into (0)\nUpload status: Select language Select language >\nhtps:/fyoutube/KX014x561RW\nLennonBirthday\nVideo / Audio quality\nfasterif you encode into\nstreamable fie format. For\n‘more information, vist ou\nHelp Center.\nF__ Get professional translation @\nVideo Manager + Add more videos\nrr"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "**Monetization**: YouTube allows creators to specify how they want to be monetized (paid promotions, sponsorships, etc.)",
          "**Age Restrictions**: YouTube enables age restrictions on videos, allowing creators to control who can view their content",
          "**License and Ownership**: The Standard YouTube License governs the use of licensed content on the platform"
        ],
        "definitions": [
          "**Syndication**: The process of reusing or redistributing content (audio/video) on other platforms",
          "**Captioning**: Adding text to videos for accessibility and comprehension",
          "**Embedding**: Incorporating YouTube videos into external websites or platforms"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**Paid Promotion**: A creator specifies that their video contains paid product placement, sponsorships, or endorsements"
        ],
        "priority": "MEDIUM",
        "slide_number": 13,
        "slide_file": "s14.png",
        "raw_text": "OY ea 1011 cated veiribe\nHf Apps [)\nYou * 210\nOd  —— =\nYouTube allows - Ei Garis onieratncin | shew\nthe creator to  . owes seve\nspecify: — Commenta category\nparen Zaliow comments: People & Blogs :\n~~\nLicense and ownership ED erersinges— (Zhernenvennan rem =\nSyndication TSN cng ad geome  Vie language\nCaption <> ‘Standard YouTube License - =\nEmbedding Syeention ‘ow von cons ast te ese\nAge restrictions nsec\nCategories ° ecorting die .\n(Za eo saison the watch ape pubic vible\n(Zion enbeting\n([ANaotify subscribers @ /\nge resrictions This video contains paid promotion such as paid product\nEnable age reeticton @ placement, sponsorships or endorsement @\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "1. **YouTube Search Algorithm**",
          "2. **Vevo**"
        ],
        "definitions": [
          "1. **Vevo**: A video hosting service that specializes in music videos"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. Search results for \"Katy Perry\" on YouTube",
          "2. Vevo's role in hosting music videos for artists like Katy Perry"
        ],
        "priority": "MEDIUM",
        "slide_number": 14,
        "slide_file": "s15.png",
        "raw_text": "Re\n°\n- - .\nSchool of Engineering Sample YouTube Search Results for Katy Perry\n= Yu katy perry * 0\nPiers > —— Katy Perry\nFirst result is an Ad pein Ni —_ ;\n2\"4 and 4\" results are 4. 7\nstored at Vevo Katy Perry - Chained To The Rhythm (Lyric Video) yoo =\nfe Skip Marley Top Tracks Albums\n34 and 5\" results are ene er oemmanene: fo ‘\nlinks to  Katy Perry vevo  — ‘  Gan\nchannel with 106 videos = KatyPerryVEVO Firework .\nTo the right is  mix Sa 4 —— —\nof Katy Perry songs  | ne One That Got way ‘\n“elated” PIR FEMAPRS Katy Perry - Roar (Official) nconsionaly .\nand some “relate igs 3; inn —\nartists ye BE cxivoer tomtay recys rane ingrenerutivens nicl mse |) vewat\nRe"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "Computer Science Education: The importance of computer science education and its challenges.",
          "Online Resources for Learning: The role of online platforms like YouTube in providing educational resources for computer science students."
        ],
        "definitions": [
          "View Count: A measure of the number of views a video has received on YouTube.",
          "Ranking Algorithm: The method used by YouTube to determine the order and visibility of search results."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Computer Science Education Challenges: The video \"Computer science education: why does it suck so much and what can be done about it?\" highlights the challenges faced by computer science students, including lack of resources and outdated curriculum.",
          "Successful Online Courses: The MIT 6.0 Introduction to Computer Science course with over 420,000 views demonstrates the effectiveness of online learning platforms in reaching a large audience."
        ],
        "priority": "MEDIUM",
        "slide_number": 15,
        "slide_file": "s16.png",
        "raw_text": "y, ei\nDd  \\/\nrh  rh\nSchool of Engineering YouTube Search Results\nPT ute) ‘computer science\nBegins with an ad “ =|  Computer science eduction: why does it suck so\n* much and what iit didn? | Ashley Gavin |\nTechnology For Students\nThe next 4 results are \\  (gu\nordered by the number of ry 3 °\nviews: 420,004, x! ase\n369.979, 228.004 | os Lec 1| MIT 6.0 Introduction to Computer Science\nts OES ‘and Programming, Fall 2008\nSome PE Lecture 0 Introduction to Computer Science\nSubsequent listings are = *\nmixture of highly ae =\nviewed videos, but older, = - VE aay Sess%oHow imvorantis Matin  Computer\ne.g. Lec | MIT has » (07) Bead\n. 7 Computer Science  good major? .\n3 million+ views but is | at .\n7 years old\n=| Computer Science Explained in less then 3\nminutes\nIt is not obvious how the Computer scence is for everyone | Hadi Partvi | |\n. : TEDxRaine :\nranking was determined Eater\nBH] —_Computer Science Tutor\nBETA  Computer Science vs Set aught vs Coding \\ | ae: [sae\nifeeen) Sy) Bootcamp (ft. Quincy Larson)\n“1010\n‘COMPUTER?\nSCIENCE .« Vig at tenet ina Computer Scene\n® Compute science education: why does it suck so\n* much and what iit didn? | Ashley Gavin | Seti orce- what eta\n(gu 4 723]\nCC  ee—"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "+ FEATURES",
          "1.  **FILTERS**: YouTube's search filters",
          "2.  **FEATURES ALGORITHMS**: Mentioned but not explicitly explained"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "1. No specific algorithm mentioned, but \"search algorithms\" or \"optimization techniques\" might be relevant ( Potential answer:  \"Search optimization algorithm\")"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 16,
        "slide_file": "s17.png",
        "raw_text": "CouTube algorithms Qe\n* During  search\nYouTube provides .\nfilters for users to\nrefine their search: :\n* UPLOAD DATE\n« TYPE\n* DURATION  taro to Agathe: Crs Courve Computer Slsnce #13\n¢ FEATURES ALGORITHMS.\n« SORT BY raf\nParra) MIT 6.006 introduction to Algorithms, Fall 2011\nJohn MacCormick’ Ne Agorthms That Changed the Future\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "YouTube uses a set of metrics to rank search results",
          "Video quality, metadata, views, likes, shares, links, subtitles/closed captions are all ranking factors"
        ],
        "definitions": [
          "Meta Data: video titles, descriptions, tags that are core ranking factors",
          "HD (High Definition) videos rank higher than low quality videos"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 17,
        "slide_file": "s18.png",
        "raw_text": "* YouTube uses the following metrics for ranking search results:\n1. Meta Data\n— video titles, descriptions and tags are core ranking factors\n— include links to  website and social profiles\n2. Video Quality\n— HD ranks higher than low quality videos\n3. Number of views, likes, shares and links\n4. Subtitles and Closed Captions\n— captions are crawled by the YouTube search engine and used for ranking\n* What is not known is how YouTube weights the individual\nfactors to make up their final ranking\nCopyright Ellis Horowitz, 2011-2022\nNeen"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  YouTube supports multiple video formats for uploading (MOV, MP4, AVI, WMV, FLV, 3GP, MPEGPS, WebM)",
          "*  Aspect Ratio matters when uploading videos to YouTube",
          "*  Video file size limit on YouTube is 128GB",
          "*  Default video length limit on YouTube is 15 minutes (can be extended)"
        ],
        "definitions": [
          "*  Aspect ratio: the ratio of width to height of a video"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Recent tennis match with short life cycle (example of a video with limited lifespan)"
        ],
        "priority": "MEDIUM",
        "slide_number": 18,
        "slide_file": "s19.png",
        "raw_text": "* YouTube Upload Characteristics\n— YouTube supports 8 video formats for uploading: MOV, MP4 (MPEG4), AVI,\nWMV, FLV, 3GP, MPEGPS, WebM\n— Aspect Ratio: the standard aspect ratios are: 4:3 or 16:9. When the video is\nuploaded to the site, YouTube will either leave it as-is (for 16:9) or add vertical\nblack bars (for 4:3)\n— The maximum file size you can upload to YouTube is 128GB.\n— By default, you can upload videos that are up to 15 minutes long, though\nthat can be extended\n— Many videos have  short life cycle, e.g.  recent tennis match that is soon\nforgotten, however, there is no time limit for videos to remain on YouTube,\nunless\n* You delete the video.\n* You delete your account.\n* You violate copyright or community guidelines\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  YouTube videos are played in the browser, assuming it supports HTML5",
          "*  No native support for running YouTube videos on some devices (e.g. Apple products), requires separate app or transcoding",
          "*  Different video standards used by various devices (e.g. H.264)"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  YouTube apps exist for Android and iPhone devices",
          "*  Examples of other devices that can play YouTube videos:"
        ],
        "priority": "MEDIUM",
        "slide_number": 19,
        "slide_file": "s20.png",
        "raw_text": "* Desktops/laptops\n— Videos are played in your browser assuming it supports HTMLS\n— This avoided the need to use Adobe Flash Player\n¢ Smartphones\n— YouTube apps exist for Android and iPhone devices\n¢ There is no native support for running YouTube videos so  separate app is\nrequired\n— For YouTube' videos to run on Apple products YouTube' content had to be\ntranscoded into Apple' preferred video standard, H.264\n* Other Devices\n— Apple TV, Fire TV, iPod Touch,\n— TiVo, PlayStation, Wii Game consoles,\n— Xbox Live, Roku Players\n— Google Chromecast\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Computer algorithms play a crucial role in YouTube's recommendation system to maximize watch time."
        ],
        "definitions": [
          "*  YouTube Search: A feature that returns search results based on user queries."
        ],
        "formulas": [],
        "algorithms": [
          "*  The process of maximizing watch time through recommendations is an algorithmic approach.",
          "*  YouTube's recommendation algorithm uses computer algorithms to choose the first result (based on \"ee eo\" query)."
        ],
        "examples": [
          "*  A search for \"computer algorithms\" returns approximately 521,000 results.",
          "*  The article mentioned at the end of the slide (\"Algorithms Take Over YouTube's Recommendations...\") is an example of how algorithms affect human behavior."
        ],
        "priority": "MEDIUM",
        "slide_number": 20,
        "slide_file": "s21.png",
        "raw_text": "fe\nSchool of Engineeri ° ° .\n‘Recommendations to Retain Viewers\nGBoulube computer algorithms:\n* YouTube Search \" ee eo\nResults Example ‘About 521,000 results = rurer\niOr query ALGORITHMS\ncomputer “08\nalgorithms” aT M6006 oducton Ath al 21\nchoose the first =\nFREE Vets en stootn?- Do. alo\nresult (ota\n. TIS SIFUGIRSS IT D2te Structures and Algorithms Complete Tutorial Computer\nRecommendations are made  ‘and Education for All\nto maximize watch time Made Easy\neis Advanced Algorithms (COMPSC! 224), Lecture 1\n| <8, tat Concepts of Algor Flow Char & ¢ Programming\nhttps://www.nbcnews.com/tech/social-media/algorithms-take-over-youtube-s-recommendations-highlight-human-problem-n867596\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "* OUTube Recommendation Algorithm ()",
          "* Query ()",
          "* Selection ()"
        ],
        "definitions": [
          "* None explicitly mentioned, but \"query\" can be considered a definition:  A query is likely referring to the user's search or request for video recommendations."
        ],
        "formulas": [],
        "algorithms": [
          "+ Selection ()",
          "+ Subsequent selection of videos (not specified as a separate algorithm, but rather part of the overall process) ()"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 21,
        "slide_file": "s22.png",
        "raw_text": "fe\nSchool of Engincerng 4+ OUTube Recommendation Algorithm\nrte creer °\n* Given the query ° =\n“ . ” FUNCTION joo ose. ssupenserreusen\ncomputer algorithms eau TOYS & HOBBIES\nfollowed by  selection, ea =\nYouTube makes  ao NENTeS — .\nrecommendations for bone tee eeos } ots, Sell\nsubsequent videos wo tpatanc uh cana tangtr mane .\n* Recommendations account ¢ ne aS\nfor 60% of all video clicks “cegemstnsenenve narovensneracms its aaa\nCopyright Ellis Horowitz, 2011-2022\nRe"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "Association Rule Mining",
          "Co-visitation counts",
          "Relatedness (r(vi,vj))",
          "Normalization function (f(v; vj))"
        ],
        "definitions": [
          "**Co-visitation count**: Number of times two videos are co-watched.",
          "**Relatedness (r(vi,vj))**: Measure of how related two videos are, based on co-visitation counts.",
          "**Normalization function (f(v; vj))**: Function that takes into account the global popularity of both seed and candidate videos."
        ],
        "formulas": [
          "r(vi,vj) = cij / √(ci × cj)"
        ],
        "algorithms": [
          "To determine related videos:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 22,
        "slide_file": "s23.png",
        "raw_text": "¢ Association Rule Mining\n— For each pair of videos v; v; compute co-visitation counts, i.e. they count how\noften they were co-watched; if c;; is the co-visitation count, then relatedness is\ndefined as\nr(vi,v;) = Cig\n4  —_\n,  (vi, U5 )\nwhere c; and c; are the total occurrence counts across all sessions for videos\nvy, and v;. f(v; vj) is  normalization function that takes the global\npopularity of both the seed video and the candidate video into account; e.g.\nSO Vv) = G1 * Gj\nThe set of related videos, R; for  given seed video v; is determined by taking the\ntop  candidate videos ranked by their scores r(v,, vj)\nRelated videos induce  directed graph over the set of videos, namely:\nFor each pair of videos (v;, v;), there is an edge e, from v; to v; iff vis in R;\nFor details see: The YouTube Recommendation System\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.9301 &rep=rep1 &type=pdf\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "* Video-rich snippet",
          "* YouTube's dominance in video-rich snippets"
        ],
        "definitions": [
          "* Video-rich snippet",
          "* Web search"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "* Searching for \"tutorial on bitcoin\""
        ],
        "priority": "MEDIUM",
        "slide_number": 23,
        "slide_file": "s25.png",
        "raw_text": "video rich snippet means that when Mm -io\nsomeone searches for something on Google, ai ae\nyou can have  small tiny video show up £.  : aw  ae  =  = .\nnext to your result to let the user know that ———————— eee —\nparticular result (yours) has  video to help =WISTIA pices Ereequent Vitec aeuep es Dormelae\nGoogle weeded out the video competition in noon pe ime =\nWeb search by predominantly displaying\nonly video-rich snippets for YouTube videos at po\nback in 2014. see\nHere is  graph outlining the percentage bi ssapaay peor\nshare of video-rich snippets in Google; eearereas bs\n91% are from YouTube “ jon\nsee ous . Source: Wistia\nhttps://wistia.com/blog/where-did-my-\nvideo-snippets-go\ne.g. try “tutorial on bitcoin”\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "A Content Distribution Network (CDN) consists of a large set of content servers and means for dynamically selecting servers based on knowledge of the location of the user and possibly the content being requested."
        ],
        "definitions": [
          "CDN: Content Distribution Network",
          "Server: A computer that provides services or resources to other computers over a network"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Some sights operate their own CDN, e.g. Google, YouTube",
          "Third-party companies that offer CDN services such as Akamai, Limelight and Level 3 Communications (now part of Century Link)"
        ],
        "priority": "MEDIUM",
        "slide_number": 24,
        "slide_file": "s26.png",
        "raw_text": "* Acontent distribution network (CDN) consists of  large set of content\nservers and  means for dynamically selecting servers based on\nknowledge of the location of the user and possibly the content being\nrequested\n* Some sights operate their own CDN, e.g. Google, YouTube\n¢ There are third party companies that offer CDN services such as\nAkamai, Limelight and Level 3 Communications (now part of Century\nLink)\n* See the Akamai video for 5 minutes (Tom Leighton, start at 0:44-5:00),\n* https://www.youtube.com/watch?v=Ni_60cbMydg\nCopyright Ellis Horowitz, 2011-2022"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "+  Identifying billions of videos",
          "+  Efficiently delivering video to desktop/mobile device"
        ],
        "definitions": [
          "* Unique identifier assigned by YouTube:  A fixed-length, 11 character string, base 64 identifier"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 25,
        "slide_file": "s27.png",
        "raw_text": "* Two Critical Technology Challenges for YouTube:\n— how to identify billions of videos\n— How to efficiently deliver the video to the desktop/mobile device\n* The Solutions:\n* Identification: YouTube assigns  fixed-length, 11 character string, base\n64, unique identifier to each video, see\n* https://www.youtube.com/watch?v=gocwRvLhDf8 (5 min)\n* Efficient Delivery: YouTube makes use of Google' data centers using\nthem as  content distribution network\n— https://www.youtube.com/watch?v=6yrijdhvAtl (2 mins)\nCopyright Ellis Horowitz, 2011-2022"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "YouTube Video Cache Locations",
          "Geographical distribution of video content"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "Google's data centers map (https://www.google.com/about/datacenters/inside/locations/index.html)",
          "Geographic distribution of YouTube video cache locations across the world (Africa, Atlantic Ocean, etc.)"
        ],
        "priority": "MEDIUM",
        "slide_number": 26,
        "slide_file": "s28.png",
        "raw_text": "+  map of Google’  data centers, see\n+ — https://www.google.com/about/datacenters/inside/locations/index.html\naint te St aes\nLf %& Atlantic  ~\n%-% ocean ‘os! ot\nSee Africa % Hes\n‘Ameri\nbeet % Seen] guustrain\nge\nFigure 4: Geographical distribution of YouTube\nVideo Cache Locations.\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "**Video Transcoding**: The technique of converting video into multiple different formats and resolutions to make it playable across different devices and bandwidths.",
          "**Content Distribution Network (CDN)**: A system that sends transcoded copies of videos to various locations for faster playback.",
          "[DEFINITION] **Video Transcoding**:",
          "[DEFINITION] **Content Distribution Network (CDN)**:"
        ],
        "definitions": [
          "**Metadata**: Additional information about a video, such as its title, description, and tags."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 27,
        "slide_file": "s29.png",
        "raw_text": "1. videos are uploaded from  desktop to  eer\ncentral Data Center Suealverey\n2. the video is then transcoded into multiple\nformats Load balancer\n3. transcoded copies are sent to the *\nContent Distribution Network ie\n-|_|_F\nVideo transcoding is  technique of Seopa!\nconverting  video into multiple different - :\nformats and resolutions to make it playable\nacross different devices and bandwidths. Netadoa Cacte) (Metadata 08,\nThe technique is also known as video |\nencoding. This enables YouTube to ae; se\nstream videos in different resolutions such as BEE jtansoden cone ARM) * BEE\n144p, 240p, 360p, 480p, 720p, 1080p & 4K. ane Conetoton qmse s_fendter\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  **DNS Resolution**: Local DNS server resolves domain name (www.youtube.com) to IP address",
          "*  **YouTube Video Delivery Process**: 4-step process for delivering YouTube video:"
        ],
        "definitions": [
          "*  **DNS (Domain Name System)**: System for translating domain names into IP addresses",
          "*  **HTTP GET Request**: Type of request sent to a server to retrieve data",
          "*  **Front-end Web Server**: Server that handles requests and delivers web content"
        ],
        "formulas": [],
        "algorithms": [
          "*  **YouTube Video Delivery Process**:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 28,
        "slide_file": "s30.png",
        "raw_text": "qq =\nlocal DNS server resolves ial ( CJ  ~\nwww.youtube.com and is re- ( YouTube Video Cloud __\ndirected to  YouTube server ——Il FA = CI\nwhich downloads the page XG UO\ninformation and  pointer to 2 4\nYouTube server that can\nDNS resolutions for YouTube\ndeliver the video, e.g. Oe ce once wa\nv23.lscache5.c. youtube com  | youtube.com, and other youtube\n. o~ . server hostnames.\nThe request to v23.Iscache5\n‘  HTTP GET request to 2 HTTP Reply containing the basic HTML\nmay be further resolved [video tront end servers download the video. page, and URLs to download other\ncomponents including flash video file.\nFront end web server 3 HTTP GET request to 4 HTTP Reply with flash video file.\n(Hostname www.youtube.com) *\" download the flash video file.\n4 steps describing the delivery of  YouTube video\nP' is\nhttp://www-users.cs.umn.edu/~zhzhang/Papers/youtube-tech-report.pdf\nCopyright Ellis Horowitz 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  YouTube video delivery system design consists of three components: flat video ID space, multi-layered logical server organization, and 3-tiered physical cache hierarchy.",
          "*  Video ID space is \"flat\" and has five \"anycast namespaces\" (with two unicast namespaces)."
        ],
        "definitions": [
          "*  Anycast namespace: a mechanism that allows packets to be sent to one of multiple destinations.",
          "*  Unicast namespace: a mechanism that allows packets to be sent to a single destination."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 29,
        "slide_file": "s31.png",
        "raw_text": "¢ The design of the\nYouTube video delivery\nVideo ID Space\nsystem consists of three\ncomponents: Coetbaiig ff\n1.  “flat” video id space,  Pity\n2.  multi-layered logical am awa, ST seconaany\nserver organization ey \\ tf\nconsisting of five “ Nomowpacs   _\nanycast namespaces [_ hh\n(and two unicast — 2a\naltcache Cache Hierarchy\nnamespaces), and\n3.  3-tiered physical Figure 3: YouTube Architectural Design.\ncache hierarchy with\n(at least) 38 primary\nlocations, 8 secondary\nand 5 tertiary   hang089/P: fh 1b h. df\n. ttps://www-users.cse.umn.edu/~zhang089/Papers/youtube-tech-report.p.\nlocations. ,\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Complicated re-direction scheme to find the nearest data center to serve the video",
          "*  Minimizing Round Trip Time (RTT)",
          "*  YouTube CDN (Content Delivery Network)"
        ],
        "definitions": [
          "*  CDN (Content Delivery Network): a system of distributed servers that store and deliver content to users based on geographic proximity",
          "*  RTT (Round Trip Time): the time it takes for data to travel from a user's device to a server and back"
        ],
        "formulas": [],
        "algorithms": [
          "*  Complicated re-direction scheme (not explicitly described, but mentioned as \"complicated\")"
        ],
        "examples": [
          "*  Rare video requested in California: first request came from the Netherlands, but future requests were served from California"
        ],
        "priority": "MEDIUM",
        "slide_number": 30,
        "slide_file": "s32.png",
        "raw_text": "Ke\n7 7 ,\nSchool of Engineering\n¢ There are four research papers that investigated and discussed the YouTube\nCDN, they are:\n1. Vivisecting YouTube: An Active Measurement Study, 2012, cited by Jefay\n2. Dissecting Video Server Selection Strategies in the YouTube CDN, 2011, cited\nby Jefay\n3. YouTube Traffic Dynamics and Its InterPlay with  Tier-1 ISP, 2010\n4. https://www-users.cse.umn.edu/~zhang089/Papers/youtube-tech-report.pdf\n¢ All of the papers describe  complicated re-direction scheme to find the\nnearest data center to serve the video; they attempt to minimize Round Trip\nTime or RTT\n¢ For rarely-called-for videos the “Dissecting” paper did  study requesting in\nCalifornia  rare video and observed that the first request came from the\nNetherlands, but future requests were served from California\n— Conclusion: videos are constantly being moved around to be closer to the\nplace that is requesting the sien Ellis Horowitz, 2011-2022"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  YouTube had no way of making money in its early days",
          "*  YouTube's infrastructure was expensive to maintain",
          "*  Copyright infringement issues led to lawsuits against YouTube"
        ],
        "definitions": [],
        "formulas": [],
        "algorithms": [
          "*  Not explicitly mentioned in the content"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 31,
        "slide_file": "s33.png",
        "raw_text": "* YouTube challenges in the early days\n— YouTube had no way of making money and its infrastructure is very expensive\n— YouTube was being sued by content creators as many of YouTube’  videos were\nuploaded illegally\n— YouTube solved both problems at once, by\n* Developing  system for spotting copyrighted content\n+ Allowing the copyright owner to decide if he wants to keep the content on the\nsite and let ads appear, splitting the revenue with YouTube or taking the content\ndown\n— Here is  video that describes how YouTubers make money\n—_ https://www.youtube.com/watch?v=v8F4jrtZtNE\n— (8 min)\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Content ID: a fingerprint database of copyrighted content used by YouTube",
          "*  Copyright infringement detection: the process of identifying and addressing copyright violations on YouTube"
        ],
        "definitions": [
          "*  Fingerprinting: a technique used to identify unique characteristics of digital content, such as videos or audio files"
        ],
        "formulas": [],
        "algorithms": [
          "*  The process of checking new video uploads against the Content ID database and flagging copyright violations if a match is found"
        ],
        "examples": [
          "*  YouTube has paid $1 billion to rights holders via Content ID since 2007 (highlighting the effectiveness of the system)"
        ],
        "priority": "MEDIUM",
        "slide_number": 32,
        "slide_file": "s34.png",
        "raw_text": "* YouTube’ solution was to create  fingerprint database of copyrighted\ncontent, called Content ID\n* YouTube solicited cooperation from content owners asking them to submit\ncopies of their content so YouTube could fingerprint them\n— There are millions of reference files in YouTube’  Content ID database.\n* When  new video is uploaded, it is immediately checked against the\ndatabase, and the video is flagged as  copyright violation ifa match is found.\n¢ When this occurs, the content owner has the choice of\n1. blocking the video to make it unviewable,\n2. tracking the viewing statistics of the video, or\n3. adding advertisements to the video\nhttps://arstechnica.com/tech-policy-policy/2014/10/youtube-has-paid-1-billion-to-rights-holders-via-content-id-since-2007/\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "1. **** Content ID is based on audio and video samples uploaded by rights holders to YouTube. (Slide 33)",
          "2. **** Video processing involves transcoding into multiple formats, including HTML5, H.264, WebM VP8, HD, non-HD, and others.",
          "3. **** Content ID uses a spectrogram-based approach for audio identification."
        ],
        "definitions": [
          "1. **** Content ID: A system used by YouTube to identify copyright infringement.",
          "2. **** Transcoding: The process of converting video into multiple formats.",
          "3. **** Spectrogram: A visual representation of audio frequencies over time."
        ],
        "formulas": [],
        "algorithms": [
          "1. ****",
          "2. ****",
          "3. ****"
        ],
        "examples": [
          "1. **** The introduction of YouTube's new version of Content ID, Copyright Match (recent development)."
        ],
        "priority": "MEDIUM",
        "slide_number": 33,
        "slide_file": "s35.png",
        "raw_text": "1. Content ID is based off audio and video samples that rights holders have uploaded to YouTube\n2. User uploads  video.\n3. YouTube then queues up the video to be processed i.e. it is transcoded into multiple formats\nincluding:\n— HTML5, H.264, WebM VP8, HD, non-HD, and others\n4. If the video contains audio,  hash is then calculated based off  time frequency graph called\nspectrogram.\n— Target zones (peak points in the spectrogram) are marked, then the target area between\nthem is also taken and hashed\n5. For the video portion,  sample section of frames of the video is taken.\n—  hash is created from those sampled frames of the video\n* Note recently YouTube has introduced  new version of ContentID it calls Copyright Match\n* See the following videos for details, (2 min). https://youtu.be/5-2R-IZITZ8\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Acoustic Fingerprint: A unique representation of an audio signal",
          "*  Spectrogram: Time-frequency graph representing three dimensions of audio (frequency, amplitude, and time)"
        ],
        "definitions": [
          "*  Digitized audio signal: An audio signal converted into a digital format",
          "*  Amplitude: The magnitude or strength of a sound wave"
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  Graph with two dimensions: time and frequency; third dimension represented by intensity or color (no specific example given)"
        ],
        "priority": "MEDIUM",
        "slide_number": 34,
        "slide_file": "s36.png",
        "raw_text": "Ke\nSchool of Engineering Acoustic Fingerprint\n¢ The audio signal is digitized and converted to  spectrogram —  time-\nfrequency graph\n— The graph below plots three dimensions of audio: frequency versus amplitude\nversus time\n— Acommon format is  graph with two dimensions: one axis represents time, and\nthe other axis represents frequency;  third dimension indicating the\namplitude of  particular frequency at  particular time is represented by the\nintensity or color of each point in the image."
      },
      {
        "lecture": "youtube",
        "concepts": [
          "**Content ID**: A system used by YouTube to automatically resolve copyright issues related to sound recordings.",
          "**Hashing algorithm**: The process of identifying unique digital fingerprints to detect copyrighted content on YouTube."
        ],
        "definitions": [
          "**Finite-state transducers**: A mathematical model used for computing hash functions, suggested as the proprietary method used by YouTube (referring to Eugene Weinstein and Pedro J. Moreno's 2007 ICASSP paper)."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "**98% of copyright claims are automatically resolved**: This stat shows the effectiveness of Content ID, but it's not an example of a process or procedure."
        ],
        "priority": "MEDIUM",
        "slide_number": 35,
        "slide_file": "s37.png",
        "raw_text": "* According to stats released by YouTube 99.5 percent of all copyright issues\nspecifically related to sound recordings are automatically resolved by Content ID\n¢ In addition to music, Content ID also identifies 98% of copyright claims, including\nthose tied to film, TV, gaming\n¢ The actual hashing algorithm used by YouTube remains proprietary, but it has been\nsuggested that YouTube uses finite-state transducers to compute the hash function,\ne.g. see\n« Eugene Weinstein, Pedro J. Moreno; Music Identification with Weighted Finite-State\nTransducers, Proceedings of the International Conference in Acoustics, Speech and\nSignal Processing (ICASSP), 2007\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Exponential growth of storage capacity",
          "*  Kryder's Law (not explicitly defined)"
        ],
        "definitions": [
          "*  None mentioned, but Kryder's Law is linked to a Wikipedia article for further information."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "*  $100 can buy 5000 gigabytes of storage (illustrating exponential growth)"
        ],
        "priority": "MEDIUM",
        "slide_number": 36,
        "slide_file": "s38.png",
        "raw_text": "* The storage you can buy with $100 Gigabytes of storage you can buy with $100\nhas grown exponentially — or 5000\nequivalently, the cost of storing 1GB 4500\nof videos has decreased exponentially 4000\n1500 ‘\n7 . os .. 1 a? er on ih\nKryder’ Law\nhttps://en. wikipedia.org/wiki/Mark_Kryder#Kryder%27s_law_projection\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "*  Storage needs of YouTube",
          "*  Data compression and re-encoding for storage efficiency",
          "*  Scalability of data storage in cloud-based platforms like YouTube"
        ],
        "definitions": [
          "*  TB (Terabyte): a unit of digital information storage, equivalent to 1 trillion bytes (TB = 1,000 GB)",
          "*  PB (PetaByte): a unit of digital information storage, equivalent to 1 quadrillion bytes (PB = 1,000 TB)"
        ],
        "formulas": [
          "*  24TB * 4x (for profiles) * 365 days = 35PB/year",
          "*  86MB * 4 (for profiles) * 1,000,000,000 = 320PB"
        ],
        "algorithms": [],
        "examples": [
          "*  YouTube storing approximately 35 PB of new data every year",
          "*  Estimated total storage needs of YouTube to date (320 PB)"
        ],
        "priority": "MEDIUM",
        "slide_number": 37,
        "slide_file": "s39.png",
        "raw_text": ". An answer by Rasty Turek  1280x720 mp4 & download - 59.01 MB\nfrom Quora\n* There is roughly 24TB ofnew  & 64oxs60 webm & download - 19.07 MB\nvideos uploaded daily 400x240 flv & download - 8.51 MB\n* Each video is re-encoded based Sais = aon\n& download - 5.9\non pre-selected profiles and as ee\neach is stored as  separate file\n* Here is his computation:\n24TB * 4x (for profiles) * 365 days = 35PB/year\nSo YouTube needs to store roughly 35PB of new data every year.\nFrom multiple sources we know that there is roughly 1 Billion videos that have been uploaded\nto YouTube to date. Assuming each video has on average size of 86MB we can compute their\ntotal storage needs as:\n86MB * 4 (for profiles) * 1,000,000,000 = 320PB\nSo it is estimated that YouTube needs to have at least 320PB of storage currently and that\nthe storage needs are growing each year by 35 PetaBytes\nCopyright Ellis Horowitz, 2011-2022\nee"
      },
      {
        "lecture": "youtube",
        "concepts": [
          "1. **Content Delivery Network (CDN)** : The backbone of YouTube's rapid content delivery.",
          "2. **Load Balancer** : Efficiently distributes incoming requests across multiple servers.",
          "3. **Transcoding Servers** : Converts and optimizes video files into various formats.",
          "4. **Metadata Database** : Stores crucial metadata associated with videos.",
          "5. **Content Delivery Network (CDN)** as the backbone of YouTube's rapid content delivery, ensuring seamless streaming for users worldwide."
        ],
        "definitions": [
          "1. **Load Balancer**: A network device that distributes incoming requests across multiple servers to optimize performance and prevent bottlenecks.",
          "2. **Transcoding Server**: A server that converts and optimizes video files into various formats, accommodating diverse user devices and network conditions."
        ],
        "formulas": [],
        "algorithms": [],
        "examples": [
          "1. The example of YouTube's **Content Delivery Network (CDN)** ensuring seamless streaming for users worldwide.",
          "2. The example of **Transcoding Servers** converting and optimizing video files into various formats."
        ],
        "priority": "MEDIUM",
        "slide_number": 38,
        "slide_file": "YT_ss_design_45W.png",
        "raw_text": "YouTube System Design! &\nDiscover the intricate architecture that powers the world' leading video-sharing\nplatform:\n1. Content Delivery Network (CDN): The backbone of YouTube' rapid content\ndelivery, ensuring seamless streaming for users worldwide.\n2. Load Balancer: Efficiently distributes incoming requests across multiple servers,\noptimizing performance and preventing bottlenecks.\n3. Application Servers: The engine behind YouTube' functionality, handling user\nrequests, interactions, and serving content dynamically.\n4. User Database: The repository storing user information, preferences, and\ninteractions, ensuring personalized experiences.\n5. Transcoding Servers: Vital for converting and optimizing video files into various\nformats, accommodating diverse user devices and network conditions.\n6. Thumbnail Storage:  dedicated space for storing video thumbnails, enhancing\nvisual appeal and facilitating quick content recognition.\n7. Web Server: Facilitates user interaction, serving web pages and ensuring\nseamless browsing experience.\n8. Metadata Database: Stores crucial metadata associated with videos, enabling\nefficient content organization and retrieval.\n9. Metadata Cache: Optimizes data retrieval speed by storing frequently accessed\nmetadata, enhancing overall system efficiency.\n10. Media Storage ($3): Robust and scalable storage solution for housing the vast\nlibrary of YouTube videos, ensuring accessibility and reliabili\nyar Mess uTube, ideos, ¢ Pea Tg Me Sceested videos ty."
      },
      {
        "lecture": "youtube",
        "concepts": [
          "1. **** YouTube System Design: Understanding the architecture of a large-scale video sharing platform like YouTube is crucial for this topic.",
          "2. **** Content Delivery Network (CDN): A distributed network of servers that cache and serve content to users based on their geographical location.",
          "3. **** Load Balancer: A mechanism that distributes incoming traffic across multiple web servers to improve responsiveness, reliability, and scalability.",
          "4. **** Media Storage (S3): Amazon Simple Storage Service is a cloud-based object storage system for storing large amounts of data."
        ],
        "definitions": [
          "1. **** CDN: A network of servers that cache and serve content to users based on their geographical location.",
          "2. **** Load Balancer: A mechanism that distributes incoming traffic across multiple web servers to improve responsiveness, reliability, and scalability.",
          "3. **** Transcoding Servers: Specialized servers that convert video formats for smooth playback on different devices."
        ],
        "formulas": [],
        "algorithms": [
          "1. **** Content Delivery Network (CDN) process:",
          "2. **** Load Balancer process:"
        ],
        "examples": [],
        "priority": "MEDIUM",
        "slide_number": 39,
        "slide_file": "YT_ss_design_45W.gif",
        "raw_text": "Technical Career  BP) Hina Arora\nBranding Coach ) @hinaarora\nYoutube System Design\n= = 7S\na] | em --> RS\n5 : we\nee = = Content Delivery\nMobile Client hobs Client Web cient Network (CDN)\nc= ' fH\n— CD...\n' (“CE ----\nsot\n‘ae Dy\n| =\n4 '\n] Sacccosasce nea pte (|)\noo\nTT {\nBeeae '\nSS ss\nLoad Balancer i} Web Server\n' Media Storage (S3)\nSie\nje wm ennnnnnnnnnd\nS$ p--- GEER ----- > me !\nVE Suse 4\n' Sia es\n' Transcoding Servers\nOND!\nqinges -~~~--~- onan any\n— Thumbnail Storage\nApplication Sa\nServers ——\n- eve aaa\nHe eso\neorennn=> Ey mi\nUser Database Metadata Database Metadata Cache"
      }
    ]
  },
  "consolidated": {
    "key_concepts": [
      {
        "text": "1. **De-Duplication** -  (process of identifying and avoiding essentially identical web pages)",
        "lecture": "deduplication"
      },
      {
        "text": "2. **Locker Storage** -  (strategy where only single copy of file is stored with multiple links to the single file)",
        "lecture": "deduplication"
      },
      {
        "text": "1. **Deduplication**:  - The process of removing duplicate copies of data or URLs while maintaining their unique identities.",
        "lecture": "deduplication"
      },
      {
        "text": "2. **Virtual hosts**:  - A feature that allows multiple hostnames to share the same document folder, but have different domain names.",
        "lecture": "deduplication"
      },
      {
        "text": "3. **URL structure**:  - The components of a URL (protocol, hostname, path, page name) can be distinct yet still point to the same page.",
        "lecture": "deduplication"
      },
      {
        "text": "Deduplication: removing duplicate data or records to improve efficiency and reduce storage needs.",
        "lecture": "deduplication"
      },
      {
        "text": "Data Duplication",
        "lecture": "deduplication"
      },
      {
        "text": "Deduplication (concept of removing duplicate data)",
        "lecture": "deduplication"
      },
      {
        "text": "Similarity between web pages (differing slightly)",
        "lecture": "deduplication"
      },
      {
        "text": "Deduplication (not explicitly mentioned in this slide, but relevant to the topic)",
        "lecture": "deduplication"
      },
      {
        "text": "* Mirroring ()",
        "lecture": "deduplication"
      },
      {
        "text": "Apache mirrors",
        "lecture": "deduplication"
      },
      {
        "text": "Deduplication ( implied by the context of the slide)",
        "lecture": "deduplication"
      },
      {
        "text": "**Deduplication**: Not explicitly mentioned in the text, but implied to be related to finding software releases from Apache Software Foundation.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Deduplication**: Avoiding or minimizing duplicate results in crawling",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Smarter Crawling**: Optimizing crawling to reduce resources and increase politeness",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Better Connectivity Analysis**: Combining in-links from multiple mirror sites for accurate PageRank",
        "lecture": "deduplication"
      },
      {
        "text": "**Deduplication**: The process of identifying and removing duplicate or near-duplicate data.",
        "lecture": "deduplication"
      },
      {
        "text": "*  Duplicate Problem: Exact match vs. Near-Duplicate Problem: Approximate match",
        "lecture": "deduplication"
      },
      {
        "text": "*  Cryptographic hashing for exact match detection",
        "lecture": "deduplication"
      },
      {
        "text": "*  Syntactic similarity with edit-distance measure for near-duplicate detection",
        "lecture": "deduplication"
      },
      {
        "text": "Cryptographic hash function",
        "lecture": "deduplication"
      },
      {
        "text": "Input Digest (a fixed-size alphanumeric string)",
        "lecture": "deduplication"
      },
      {
        "text": "Hash value (also known as message digest, digital fingerprint, or checksum)",
        "lecture": "deduplication"
      },
      {
        "text": "Properties of a cryptographic hash function:",
        "lecture": "deduplication"
      },
      {
        "text": "**Cryptographic hash functions**: widely used for data deduplication and security.",
        "lecture": "deduplication"
      },
      {
        "text": "**Message-digest (MD) hash function**: produces a fixed-size string of characters that represents the original input data.",
        "lecture": "deduplication"
      },
      {
        "text": "**SHA-1**, **SHA-2**, and **SHA-3** families: types of cryptographic hash functions with different digest sizes.",
        "lecture": "deduplication"
      },
      {
        "text": "**RIPEMD-160**: family of cryptographic hash functions that produces 160-bit digests.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Deduplication**: process of identifying and eliminating duplicate documents",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Hash function**: a one-way function that takes input data and produces a fixed-size output, known as a hash value or digest",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Bucketing**: dividing documents into groups based on their hash values",
        "lecture": "deduplication"
      },
      {
        "text": "Deduplication",
        "lecture": "deduplication"
      },
      {
        "text": "Hash function",
        "lecture": "deduplication"
      },
      {
        "text": "Signature",
        "lecture": "deduplication"
      },
      {
        "text": "Fingerprint",
        "lecture": "deduplication"
      },
      {
        "text": "Near duplicates",
        "lecture": "deduplication"
      },
      {
        "text": "Distance measure must satisfy 4 properties:",
        "lecture": "deduplication"
      },
      {
        "text": "* Sets as unordered collections of objects (e.g., {a, b, c})",
        "lecture": "deduplication"
      },
      {
        "text": "* Distance between sets (d(A, B))",
        "lecture": "deduplication"
      },
      {
        "text": "* Similarity between sets (s(A, B))",
        "lecture": "deduplication"
      },
      {
        "text": "* Jaccard similarity",
        "lecture": "deduplication"
      },
      {
        "text": "* Clustering",
        "lecture": "deduplication"
      },
      {
        "text": "**Shingle**: a contiguous subsequence of words in a document",
        "lecture": "deduplication"
      },
      {
        "text": "**Similarity Measures**: Jaccard(A,B), Containment(A,B)",
        "lecture": "deduplication"
      },
      {
        "text": "**Resemblance**: similarity measure between two documents (0 <= Resemblance <= 1)",
        "lecture": "deduplication"
      },
      {
        "text": "*  Deduplication: a process of finding near duplicates in a large dataset",
        "lecture": "deduplication"
      },
      {
        "text": "*  Tropical fish example: used to illustrate deduplication concept, not essential for exam",
        "lecture": "deduplication"
      },
      {
        "text": "*  3-shingles: sets of three consecutive words used as a basis for deduplication",
        "lecture": "deduplication"
      },
      {
        "text": "*  Deduplication",
        "lecture": "deduplication"
      },
      {
        "text": "*  Jaccard similarity of sets",
        "lecture": "deduplication"
      },
      {
        "text": "*  Jaccard distance of sets",
        "lecture": "deduplication"
      },
      {
        "text": "SimHash is a method for determining near duplicates of web pages.",
        "lecture": "deduplication"
      },
      {
        "text": "Near duplicates are determined by an f-bit fingerprint, where a pair of documents are near duplicates if their fingerprints are at most k-bits apart.",
        "lecture": "deduplication"
      },
      {
        "text": "1. **** Ahash function: usually hashes different values to totally different hash values",
        "lecture": "deduplication"
      },
      {
        "text": "2. **** Simhash: a type of hashing where similar items are hashed to similar hash values (measured by bitwise Hamming distance)",
        "lecture": "deduplication"
      },
      {
        "text": "3. **** Bitwise Hamming distance: a measure of the similarity between two hash values",
        "lecture": "deduplication"
      },
      {
        "text": "1. **Simhash**: A technique for calculating a similarity score between two phrases",
        "lecture": "deduplication"
      },
      {
        "text": "2. The concept of simhash is useful for determining the similarity between two phrases",
        "lecture": "deduplication"
      },
      {
        "text": "Deduplication: process of identifying and removing duplicate data or items.",
        "lecture": "deduplication"
      },
      {
        "text": "**Deduplication**: The process of removing duplicate values from a list by sorting.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Deduplication**: reducing runtime by checking adjacent pairs of a list instead of all combinations",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Adjacent pair approach**: improves efficiency by focusing on nearby elements",
        "lecture": "deduplication"
      },
      {
        "text": "* Bitwise Hamming distance preserves its value under permutation of bits",
        "lecture": "deduplication"
      },
      {
        "text": "* Sorting by fingerprint can be used to identify pairs of identical elements",
        "lecture": "deduplication"
      },
      {
        "text": "1. **** Information Retrieval (IR) deals with indexing textual documents and retrieving relevant documents given a query.",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. **** Searching for pages on the World Wide Web has become a primary application of IR.",
        "lecture": "info_retrieval"
      },
      {
        "text": "1. **KEY CONCEPTS** : Information Retrieval",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Boolean model of retrieval",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Vector-space model of retrieval",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Prof. Salton and his students' research at Cornell University",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **Creation of large document database systems**",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Searching FTP'able documents on the Internet",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Archie",
        "lecture": "info_retrieval"
      },
      {
        "text": "* WAIS",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Lycos",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Yahoo",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Altavista",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Recommender Systems: computer programs that attempt to predict items users may be interested in, given some information about their profile.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Automated Text Categorization & Clustering Systems: useful for grouping news articles.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Link analysis for Web Search",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Extension to retrieval of multimedia (images, music, video)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Question Answering systems that return an actual answer rather than a ranked list of documents",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Information retrieval (IR) - process of searching for and retrieving relevant information from a large collection",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Data analysis - process of extracting insights and patterns from data",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Machine learning models - algorithms that enable systems to learn from data without being explicitly programmed",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Focused on structured data stored in relational tables rather than free-form text",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Efficient processing of well-defined queries in formal language (SQL)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Clearer semantics for both data and queries",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Human user aspects of interaction ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ User interface ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Visualization ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Effective categorization of human knowledge ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Citation analysis ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Bibliometrics ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "+ Digital libraries ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Representation of knowledge, reasoning, and intelligent action",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Formalisms for representing knowledge and queries",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Integration with web ontologies and intelligent information agents",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Syntactic analysis**: focuses on syntax (phrase structure) in Natural Language Processing",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Semantic analysis**: focuses on semantics to retrieve meaning based on natural language text",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Pragmatic analysis**: analyzes the relationship between language and context",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Extractive Summarization**: a method of summarizing text by extracting key information",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Abstractive Text Summarization**: a method of summarizing text by generating new content",
        "lecture": "info_retrieval"
      },
      {
        "text": "1. **Machine Learning**  - a branch of Artificial Intelligence that allows computers to evolve their behavior based on empirical data",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. Machine learning is focused on developing computational systems that improve their performance with experience",
        "lecture": "info_retrieval"
      },
      {
        "text": "1. **** Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from various types of data.",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. **** Data science is related to data mining, machine learning, and big data.",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Notion of Relevance**: Simplest notion of relevance is that the query string appears verbatim in the document.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Goes beyond using just keyword matching",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Takes into account the meaning of the words used",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Adapts to the user based on direct or indirect feedback",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Indexing: The process of creating an index from a text database to enable fast search operations.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Search Initiation: Queries are used to initiate a search on the indexed documents.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Parsing forms index words (tokens)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Stopword removal",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Stemming",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Retrieval model specifies details of document representation, query representation, and retrieval function",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Determines notion of relevance",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Three major types of retrieval models: Boolean, Vector Space, and Probabilistic",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Text Preprocessing**: The process of preparing text data for information retrieval.",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Inverted Index**: A data structure that maps keywords to documents they appear in.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Document representation as a set of keywords",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Boolean queries as expressions of keywords connected by AND, OR, and NOT operators",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Scope indication using brackets",
        "lecture": "info_retrieval"
      },
      {
        "text": "Popular retrieval model",
        "lecture": "info_retrieval"
      },
      {
        "text": "Boolean models can be extended to include ranking",
        "lecture": "info_retrieval"
      },
      {
        "text": "Reasonably efficient implementations possible for normal queries",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Rigid Boolean query model",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Difficulty in expressing complex user requests",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Information retrieval challenges: too many matches for simple queries (e.g., \"Lincoln\")",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Importance of query refinement for accurate results",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Vector Space**: A set of \"orthogonal\" terms (index terms or vocabulary) that form a vector space.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Dimension**: The size of the vocabulary, equal to the number of index terms.",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Vocabulary**: consists of 3 terms (T)",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Term weights**: coefficients",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Documents**: D1 and D2",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Query**: Q = 0T + 0T + 2T",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Term frequency in a document (more frequent terms are more important)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Normalizing term frequency across the entire corpus",
        "lecture": "info_retrieval"
      },
      {
        "text": "Terms that appear in many different documents are less indicative of overall topic.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Term frequency (df)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Inverse Document Frequency (idf)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Combined term importance indicator",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  TF-IDF (Term Frequency-Inverse Document Frequency) weight",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Query-document scoring",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **Term Frequency (tf)**: represents the frequency of a term in a document",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **Inverse Document Frequency (idf)**: measures the rarity of a term across the entire collection",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Cosine of the angle between vectors is a similarity measure (not distance measure)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Similarity measure",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Normalizing vectors (  )",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Unit circle (  )",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **Similarity between vectors for the document d; and query q can be computed as the vector inner product** ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Similarity between vectors for documents and queries",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Vector inner product for computing similarity",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** Vector space model",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** Binary vector representation",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** Weighted vector representation",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Cosine Similarity**: measures the cosine of the angle between two vectors",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Vector Lengths**: normalization factor in Cosine Similarity calculation ( Dj = sqrt(Dwj^2) )",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Inner Product**: a measure of similarity between two vectors",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Vector Space Model**: a method for representing documents and queries as vectors in a high-dimensional space.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Ranking: computing the documents in the corpus \"nearest\" to the query",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Representing query and documents as weighted vectors:",
        "lecture": "info_retrieval"
      },
      {
        "text": "Preprocessing",
        "lecture": "info_retrieval"
      },
      {
        "text": "Preferred list",
        "lecture": "info_retrieval"
      },
      {
        "text": "Cosine similarity",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Mathematically based approach",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Combination of local and global word occurrence frequencies",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Missing semantic information (e.g. word sense)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Missing syntactic information (e.g. phrase structure, word order, proximity information)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Inverted index: a data structure used to improve search efficiency",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Linked Inverted Index: an extension of inverted indexing that links words with their occurrences",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Distributed Indexing: a technique for scaling inverted indexes across multiple machines",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted index is a data structure composed of a vocabulary (vector) containing distinct words in lexicographical order",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Vocabulary includes lists of documents and text positions where each word occurs",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** Inverted Indexing: A data structure used for efficient text search.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. **** Dictionary (Index File): The part of an inverted index that stores terms in alphabetical order with pointers to their postings lists.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "3. **** Postings List: The part of an inverted index that stores a list of document IDs where a term appears.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* List of words by position",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* List of positions by word",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. ** Inverted Index**: Considered as a sparse matrix where rows represent terms and columns represent documents.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. ** Sparse Matrix**: Used to represent an inverted index, where most entries are zero.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted indexing ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Bitwise AND operation in inverted indexing ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Indexing: The process of creating a data structure that allows for efficient retrieval of specific information from a large dataset.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted indexing ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Sparsity of term-document matrix ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverted Indexing: Storing a list of all documents that contain a specific term.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverted Indexing: A data structure used to store words and their corresponding documents.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **Inverted Indexing**: An index data structure for efficient information retrieval",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. Understanding of a corpus: Knowledge of the document collection before parsing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**KEY CONCEPTS: **",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **Inverted Index**: A data structure that stores a list of documents containing each unique term ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. Importance of efficient indexing for search queries ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Inverted Indexing**: a data structure used to index and store terms in a document collection.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Dictionary file**: contains a list of unique terms (index entries) with their corresponding frequencies and document numbers.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Postings file**: contains the locations where each term appears in the documents, along with the frequency information.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Inverted Indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Dictionary (inverted index)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Postings (document ids)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted indexing ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Inverted Indexing**: The process of storing and retrieving documents based on their indices, allowing for efficient querying.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Query Processing**: The process of evaluating a query to retrieve relevant documents from the index.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Inverted Indexing**: not mentioned explicitly, but related to the technique described",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Skip Pointers**: main idea discussed in this slide",
        "lecture": "inverted_indexing"
      },
      {
        "text": "- **Skip pointers**:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "- **Posting augmentation**:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Inverted Indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Posting lists",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Successor (in the context of postings)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Skip successor (of a posting on a lower list)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1.  **Skip Pointers**: shortcuts added at indexing time to help with AND queries",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2.  **Static Corpus**: when the corpus is relatively static, skip pointers are more useful",
        "lecture": "inverted_indexing"
      },
      {
        "text": "3.  **Posting List Length (P)**: the length of a postings list affects the placement of skip pointers",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Phrase queries",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+  Biword (or 2-gram) concept: consecutive pair of terms in a text",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+  Indexing biwords to improve phrase searching",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. Inverted Indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. Biwords",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverted Indexing: a data structure used to store and retrieve documents based on their terms.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* ** Inverted Indexing**: Storing postings of the form docID: position1, position2, ..., where each position is a token index in the document.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* ** Positional Index**: Expanding required postings storage significantly, even if we compress position values/offsets.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted indexing: a method for extracting and merging doc:position lists for multiple terms ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Proximity searches: searching for adjacent words in a document ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Frequency analysis in patent data*",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. N-grams are sequences of consecutive words",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. N-grams can be identified at the time of parsing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "3. The inverted index will need pointers to all dictionary terms containing an n-gram (postings)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Inverted Indexing: A technique used to speed up information retrieval by storing an index of words in a document along with their locations.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  N-grams: A sequence of n items (e.g., words, characters) from a given text.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1.  **Inverted Indexing**: The concept of inverted indexing is not explicitly mentioned in this slide, but it can be inferred from the context.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2.  **N-gram statistics**: The study focuses on analyzing n-grams (sequences of characters or words) in English and Chinese texts.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Distributed Computing Cluster**: A pool of machines used for web-scale indexing to improve efficiency and reliability.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Fault-tolerance**: The ability of individual machines in a distributed cluster to handle unpredictable slowdowns or failures.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverted Indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Parallel tasks (Parsers and Inverters)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Document corpus splitting",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverted Indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Posting (refer to sign \"assign\")",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Inverted indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Inverted indexing: a method for searching across multiple indices",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Main index vs. auxiliary index: maintaining two separate indices for efficient search and updates",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Big main index vs. small auxiliary index: trade-off between space and search efficiency",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** Inverted Indexing: a data structure used to efficiently store and retrieve information from a document collection.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Query language processing",
        "lecture": "querying"
      },
      {
        "text": "*  Boolean operators (AND, BUT NOT)",
        "lecture": "querying"
      },
      {
        "text": "*  Search engine functionality (returns unexpected results)",
        "lecture": "querying"
      },
      {
        "text": "Boolean query",
        "lecture": "querying"
      },
      {
        "text": "Advanced Search page",
        "lecture": "querying"
      },
      {
        "text": "ANDing ( logical operator)",
        "lecture": "querying"
      },
      {
        "text": "ORing (logical operator)",
        "lecture": "querying"
      },
      {
        "text": "NOTing (logical operator)",
        "lecture": "querying"
      },
      {
        "text": "*  **Search Engine Query**: Understanding how search engines process queries is crucial in querying.",
        "lecture": "querying"
      },
      {
        "text": "*  Implicit AND: when searching for multiple keywords at once, Google/Bing will automatically search for pages that contain ALL of your keywords.",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for phrases in Google requires putting the phrase in quotes.",
        "lecture": "querying"
      },
      {
        "text": "*  Using the exact phrase within quotes will show pages that contain the exact phrase, not just the individual words.",
        "lecture": "querying"
      },
      {
        "text": "*  General principles of electricity supply systems",
        "lecture": "querying"
      },
      {
        "text": "*  Law of electricity (no clear definition provided)",
        "lecture": "querying"
      },
      {
        "text": "*  Basic principle of electric generator",
        "lecture": "querying"
      },
      {
        "text": "*  Theory of electromagnetism (implied through discussion on magnetism and electromotive force)",
        "lecture": "querying"
      },
      {
        "text": "Querying: searching for pages that contain words similar to search terms",
        "lecture": "querying"
      },
      {
        "text": "Word Variations or Automatic Stemming: feature of Google that finds pages with similar words (e.g. \"child\" and \"children\")",
        "lecture": "querying"
      },
      {
        "text": "*  Google favors results that have search terms near each other",
        "lecture": "querying"
      },
      {
        "text": "*  Google gives higher priority to pages with terms in the same order as the query",
        "lecture": "querying"
      },
      {
        "text": "*  Google is NOT case sensitive",
        "lecture": "querying"
      },
      {
        "text": "*  Case sensitivity in querying (e.g., \"bush\" vs. \"Bush\")",
        "lecture": "querying"
      },
      {
        "text": "1. **Querying**:  - The process of retrieving data from a database or system.",
        "lecture": "querying"
      },
      {
        "text": "2. **Database Management**:  - A field that deals with storing, organizing, and managing large amounts of data in databases.",
        "lecture": "querying"
      },
      {
        "text": "3. **Disney+**:  - A streaming service provided by The Walt Disney Company.",
        "lecture": "querying"
      },
      {
        "text": "* Boolean OR operator",
        "lecture": "querying"
      },
      {
        "text": "* Precedence of Boolean operators (OR > AND)",
        "lecture": "querying"
      },
      {
        "text": "* All query terms are implicitly ANDed",
        "lecture": "querying"
      },
      {
        "text": "* OR has higher precedence than AND",
        "lecture": "querying"
      },
      {
        "text": "*  Full-word wildcard query",
        "lecture": "querying"
      },
      {
        "text": "*  Google search functionality for exact phrases",
        "lecture": "querying"
      },
      {
        "text": "*  Use of quotes to search for exact phrases",
        "lecture": "querying"
      },
      {
        "text": "*  Stop words and their effect on search queries",
        "lecture": "querying"
      },
      {
        "text": "*  Query modifiers Search",
        "lecture": "querying"
      },
      {
        "text": "*  Search Operators",
        "lecture": "querying"
      },
      {
        "text": "* Restricting search results to specific file types using filetype:",
        "lecture": "querying"
      },
      {
        "text": "* Understanding that the \"dot\" in file extension is optional",
        "lecture": "querying"
      },
      {
        "text": "* `inanchor` operator restricts results to pages containing specific query terms in anchor text or links",
        "lecture": "querying"
      },
      {
        "text": "* `allinanchor` operator restricts results to pages containing all specified query terms in anchor text on links",
        "lecture": "querying"
      },
      {
        "text": "*  Query types: By text, URLs, and titles",
        "lecture": "querying"
      },
      {
        "text": "*  Importance of searching body text only",
        "lecture": "querying"
      },
      {
        "text": "* Intitle: operator restricts search results to documents containing a particular word in its title.",
        "lecture": "querying"
      },
      {
        "text": "* Restricting search results to documents containing a particular word in its URL using `inurl:*` and `inurl:disney` ()",
        "lecture": "querying"
      },
      {
        "text": "*  Restricting search results to a specific site using `site:` operator",
        "lecture": "querying"
      },
      {
        "text": "*  Importance of exact matching between `site:` and domain",
        "lecture": "querying"
      },
      {
        "text": "*  Using site: in conjunction with another search term or phrase.",
        "lecture": "querying"
      },
      {
        "text": "* Deepfaking the Mind",
        "lecture": "querying"
      },
      {
        "text": "* Brain-Computer Interfaces (BCIs)",
        "lecture": "querying"
      },
      {
        "text": "* Related lists web pages that are \"similar\" to a specified web page",
        "lecture": "querying"
      },
      {
        "text": "* Querying  (note: this may not be explicitly stated in the slide, but it's implied as the topic of discussion)",
        "lecture": "querying"
      },
      {
        "text": "*  Google's information retrieval system (implying the concept of a search engine)",
        "lecture": "querying"
      },
      {
        "text": "*  Specific information about particular web pages (suggesting the idea of searching for specific data)",
        "lecture": "querying"
      },
      {
        "text": "* Querying with specific keywords can trigger special processing in search engines",
        "lecture": "querying"
      },
      {
        "text": "* Special processing in search engines for specific keyword queries, e.g., \"stocks:\"",
        "lecture": "querying"
      },
      {
        "text": "*  Google operators: special symbols used to refine search results",
        "lecture": "querying"
      },
      {
        "text": "*  Using math expressions in searches (e.g., 12 + 34+ 10 * (150/ 7))",
        "lecture": "querying"
      },
      {
        "text": "*  Utilizing dictionary definitions in searches (e.g., define:antidisestablishmentarianism)",
        "lecture": "querying"
      },
      {
        "text": "*  Tracking numbers and airline flight numbers for specific search results",
        "lecture": "querying"
      },
      {
        "text": "**KEY CONCEPTS **",
        "lecture": "querying"
      },
      {
        "text": "1. **Google Phonebook Operators**:  - Different types of Google phonebook search operators.",
        "lecture": "querying"
      },
      {
        "text": "2. **Privacy Violations**:  - Concerns raised about Google phonebook feature.",
        "lecture": "querying"
      },
      {
        "text": "**Statistical models**: The lecture mentions that Google built a statistical model with the help of teachers to classify reading levels.",
        "lecture": "querying"
      },
      {
        "text": "**Reading level classification**: The feature is based on classifying webpages into different reading levels using statistical models.",
        "lecture": "querying"
      },
      {
        "text": "*  Google introduced the Wonder Wheel in 2009, a flash-based interface that provided possible interpretations for search queries.",
        "lecture": "querying"
      },
      {
        "text": "*  The Wonder Wheel was removed in 2011 but restored in 2012 with a renaming of the \"wheel of possible interpretations\".",
        "lecture": "querying"
      },
      {
        "text": "*  In 2014, Google re-focused the Wonder Wheel to help advertisers choose keywords.",
        "lecture": "querying"
      },
      {
        "text": "*  Google Code Search was a free beta product that allowed web users to search for open-source code on the internet.",
        "lecture": "querying"
      },
      {
        "text": "*  It used a regular expression engine to search for code in various formats, including tar.gz, CVS, and Subversion.",
        "lecture": "querying"
      },
      {
        "text": "*  The service employed a methodology that combined trigram indexing with a custom-built regular expression engine.",
        "lecture": "querying"
      },
      {
        "text": "*  It supported POSIX extended regular expression syntax.",
        "lecture": "querying"
      },
      {
        "text": "**Patent search**: The process of searching for patents using various parameters.",
        "lecture": "querying"
      },
      {
        "text": "*  Google Books: a digital database of scanned books and magazines",
        "lecture": "querying"
      },
      {
        "text": "*  Optical Character Recognition (OCR): process of converting scanned texts to digital text",
        "lecture": "querying"
      },
      {
        "text": "*  Copyright violations: potential issue with scanning and storing copyrighted materials",
        "lecture": "querying"
      },
      {
        "text": "*  Editing errors: OCR process may introduce many errors into scanned texts",
        "lecture": "querying"
      },
      {
        "text": "Querying",
        "lecture": "querying"
      },
      {
        "text": "Full View",
        "lecture": "querying"
      },
      {
        "text": "Snippet View",
        "lecture": "querying"
      },
      {
        "text": "Limited View",
        "lecture": "querying"
      },
      {
        "text": "Querying: HIGH (Understanding querying concepts is crucial for exam)",
        "lecture": "querying"
      },
      {
        "text": "*  Dynamic character grouping",
        "lecture": "querying"
      },
      {
        "text": "*  Consistency contains toporapic moos (Note: This appears to be a typo or unclear concept, but I'll keep it as is)",
        "lecture": "querying"
      },
      {
        "text": "* Relevance Feedback",
        "lecture": "querying"
      },
      {
        "text": "* Query Expansion",
        "lecture": "querying"
      },
      {
        "text": "* Peer-to-peer processing ()",
        "lecture": "querying"
      },
      {
        "text": "* Feedback mechanism for relevance of retrieved documents ()",
        "lecture": "querying"
      },
      {
        "text": "* Query processing on peer-to-peer networks ()",
        "lecture": "querying"
      },
      {
        "text": "**Enhanced Related Searches**: Google has improved its related searches feature to provide more relevant results.",
        "lecture": "querying"
      },
      {
        "text": "*  Querying",
        "lecture": "querying"
      },
      {
        "text": "*  Search engines (Yahoo!, Bing)",
        "lecture": "querying"
      },
      {
        "text": "*  Related searches",
        "lecture": "querying"
      },
      {
        "text": "**Query Optimization**: The process of improving the performance of a query by optimizing its execution plan.",
        "lecture": "querying"
      },
      {
        "text": "* Auto-completion is a form of relevance feedback",
        "lecture": "querying"
      },
      {
        "text": "* Predicting word or phrase that the user wants to type in without actually typing it in completely",
        "lecture": "querying"
      },
      {
        "text": "*  **Autocomplete**: Google's feature that provides suggestions based on user input",
        "lecture": "querying"
      },
      {
        "text": "*  **Experimental feature**: Auto-complete was initially an experimental feature in 2004",
        "lecture": "querying"
      },
      {
        "text": "* Autocomplete feature in Google search",
        "lecture": "querying"
      },
      {
        "text": "* Intellisense or suggestion feature in Google search",
        "lecture": "querying"
      },
      {
        "text": "1. **Autocomplete feature**  - The ability of a search engine to display results for a partially typed query, including links to related searches.",
        "lecture": "querying"
      },
      {
        "text": "*  Auto-complete suggestions",
        "lecture": "querying"
      },
      {
        "text": "*  Limitations of auto-complete systems (e.g., running out of alternatives)",
        "lecture": "querying"
      },
      {
        "text": "* Querying and search results (mark with )",
        "lecture": "querying"
      },
      {
        "text": "* Search engine algorithms and ranking systems (mark with )",
        "lecture": "querying"
      },
      {
        "text": "+ Query: A request for information submitted to a search engine (implied by )",
        "lecture": "querying"
      },
      {
        "text": "+ Search result: The output provided by a search engine in response to a query (implied by )",
        "lecture": "querying"
      },
      {
        "text": "1. **Auto-Completion**: Bing's use of previous queries to make suggestions before user enters a single character",
        "lecture": "querying"
      },
      {
        "text": "2. **Querying**: Process of searching for information on a search engine like Bing",
        "lecture": "querying"
      },
      {
        "text": "*  **Querying**: Bing will offer results using corrected spelling and include a link for the user to correct their query",
        "lecture": "querying"
      },
      {
        "text": "*  **Search Engine Results**: Displaying multiple sources, including images, videos, and links",
        "lecture": "querying"
      },
      {
        "text": "*  Statistical measure for evaluating processes that produce lists of possible responses to samples of queries",
        "lecture": "querying"
      },
      {
        "text": "*  Mean Reciprocal Rank (MRR) is a statistical measure",
        "lecture": "querying"
      },
      {
        "text": "1. KEY CONCEPTS:",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Evolution of Search Engines**: The development of search engines from 1991 to present day",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Non-Web Search Engines**: Early search engines that were not web-based (Gopher, Archie, Veronica)",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Web-Based Search Engines**: Transition to web-based search engines in the mid-to-late 1990s",
        "lecture": "se-basics"
      },
      {
        "text": "* The Internet's early search tools were developed in the late 1980s and early 1990s",
        "lecture": "se-basics"
      },
      {
        "text": "* Archie, Veronica, and Jughead were three major search tools that emerged during this period",
        "lecture": "se-basics"
      },
      {
        "text": "* The Gopher protocol was a TCP/IP application layer protocol designed for distributing, searching, and retrieving documents over the Internet",
        "lecture": "se-basics"
      },
      {
        "text": "*  Statistical analysis of word relationships to make searching more efficient",
        "lecture": "se-basics"
      },
      {
        "text": "*  Use of statistical analysis in search software development",
        "lecture": "se-basics"
      },
      {
        "text": "* World Wide Web Wanderer (marked as )",
        "lecture": "se-basics"
      },
      {
        "text": "* Potential for general-purpose WWW search engine (marked as )",
        "lecture": "se-basics"
      },
      {
        "text": "*  ALIWEB (Archie-Like Indexing of the Web) - a Web search engine created in 1993",
        "lecture": "se-basics"
      },
      {
        "text": "*  Importance of meta information in indexing web pages",
        "lecture": "se-basics"
      },
      {
        "text": "**AltaVista**: A significant search engine in the early days of the web.",
        "lecture": "se-basics"
      },
      {
        "text": "**Natural Language Queries**: Allowing users to search using everyday language, rather than specific keywords or syntax.",
        "lecture": "se-basics"
      },
      {
        "text": "**Advanced Searching Techniques**: Features that enable more precise and effective searching, such as filtering and sorting results.",
        "lecture": "se-basics"
      },
      {
        "text": "Lycos search engine",
        "lecture": "se-basics"
      },
      {
        "text": "Ranked relevance retrieval",
        "lecture": "se-basics"
      },
      {
        "text": "Prefix matching",
        "lecture": "se-basics"
      },
      {
        "text": "Word proximity bonuses",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Hierarchical listing**: A way to organize links into a topical hierarchy",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Portal**: A website that acts as an entry point for other websites or services (e.g. Email, Finance, Groups)",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Search feature**: A way to search through all of the links on the Yahoo homepage",
        "lecture": "se-basics"
      },
      {
        "text": "LookSmart was a search engine that competed with Yahoo! Directory in terms of inclusion rates.",
        "lecture": "se-basics"
      },
      {
        "text": "Pay-per-click (PPC) business model, where listed sites pay a flat fee per click.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Inktomi Corporation: A search engine company that came into existence in 1996",
        "lecture": "se-basics"
      },
      {
        "text": "*  Hotbot: The search engine developed by Inktomi",
        "lecture": "se-basics"
      },
      {
        "text": "*  Paid inclusion model: A business model where websites pay a fee to guarantee display on certain search terms",
        "lecture": "se-basics"
      },
      {
        "text": "* Natural language search engine",
        "lecture": "se-basics"
      },
      {
        "text": "* Human editors matching search queries",
        "lecture": "se-basics"
      },
      {
        "text": "* Subject Specific Popularity",
        "lecture": "se-basics"
      },
      {
        "text": "* Google is a play on the word Googol, which refers to 1 followed by 100 zeros ()",
        "lecture": "se-basics"
      },
      {
        "text": "* A googol is bigger than the number of atoms in the universe ()",
        "lecture": "se-basics"
      },
      {
        "text": "* Algorithmic Yahoo",
        "lecture": "se-basics"
      },
      {
        "text": "* Search Era Lycos",
        "lecture": "se-basics"
      },
      {
        "text": "* Paid Search Era",
        "lecture": "se-basics"
      },
      {
        "text": "*  Search Engine Basic Behavior",
        "lecture": "se-basics"
      },
      {
        "text": "*  Providing access to heterogeneous, distributed information that is publicly available on the World Wide Web",
        "lecture": "se-basics"
      },
      {
        "text": "*  Information comes in many different formats",
        "lecture": "se-basics"
      },
      {
        "text": "*  Most of the information has not been screened for accuracy",
        "lecture": "se-basics"
      },
      {
        "text": "*  The World Wide Web as a source of new opportunities in marketing",
        "lecture": "se-basics"
      },
      {
        "text": "Search engine: program designed to help find information stored on computer systems.",
        "lecture": "se-basics"
      },
      {
        "text": "* The User  (element 1)",
        "lecture": "se-basics"
      },
      {
        "text": "* The Web  (element 2)",
        "lecture": "se-basics"
      },
      {
        "text": "* The Crawler/Spider  (element 3)",
        "lecture": "se-basics"
      },
      {
        "text": "* The Indexer  (element 4)",
        "lecture": "se-basics"
      },
      {
        "text": "* The Query Processor  (element 6)",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Spider (a.k.a. crawler/robot)**: builds corpus",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Indexer**: creates inverted indexes",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Query processor**: serves query results",
        "lecture": "se-basics"
      },
      {
        "text": "* Distributed content creation and linking",
        "lecture": "se-basics"
      },
      {
        "text": "* Truth, lies, obsolete information, contradictions on the web",
        "lecture": "se-basics"
      },
      {
        "text": "* Diverse user backgrounds and training methods",
        "lecture": "se-basics"
      },
      {
        "text": "* Users' difficulty in distinguishing between search bar and URL address field",
        "lecture": "se-basics"
      },
      {
        "text": "* Importance of key results being at or near the top due to users rarely using scroll bars",
        "lecture": "se-basics"
      },
      {
        "text": "* Diverse access methodologies (high bandwidth connectivity, mobile limitations)",
        "lecture": "se-basics"
      },
      {
        "text": "* Poor comprehension of syntax in current search engines",
        "lecture": "se-basics"
      },
      {
        "text": "Note that some content might overlap between categories. For instance, \"Diverse user backgrounds and training methods\" could be both a  and a [PRIORITY] item. However, I've tried to categorize each point according to its primary significance for studying.",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Types of user intentions** (Informational, Navigational, Transactional)",
        "lecture": "se-basics"
      },
      {
        "text": "1. Query processing involves more than just matching query terms with document terms",
        "lecture": "se-basics"
      },
      {
        "text": "2. Semantic analysis of queries is a crucial step in search engine functionality",
        "lecture": "se-basics"
      },
      {
        "text": "*  - User interface customization (e.g., \"moar Nsagauavesane\")",
        "lecture": "se-basics"
      },
      {
        "text": "*  Search Engine Basics (SEB) - a fundamental concept in understanding how search engines work",
        "lecture": "se-basics"
      },
      {
        "text": "*  Google - one of the most popular search engines",
        "lecture": "se-basics"
      },
      {
        "text": "*  Las Vegas is the most populous city in the U.S. state of Nevada.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Las Vegas is officially known as the City of Las Vegas and has a population.",
        "lecture": "se-basics"
      },
      {
        "text": "*  The official website for Las Vegas travel information is VEGAS.com.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Las Vegas has various hotels, shows, casinos, restaurants, maps, and attractions.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Hotel website structure: main pages include map, address, phone number, price of room, photos, features & amenities, directions, make reservation, special offers.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Online booking process: includes searching hotel website, selecting dates, and making a reservation.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Google Web History: a feature that allows users to view their search history",
        "lecture": "se-basics"
      },
      {
        "text": "*  Personalization of search results: only the user can see their own search history",
        "lecture": "se-basics"
      },
      {
        "text": "* Google's dominance in the search engine revenue market",
        "lecture": "se-basics"
      },
      {
        "text": "* Baidu, Yahoo, and Bing's revenue trends over the years",
        "lecture": "se-basics"
      },
      {
        "text": "* Google's dominance in search engine market",
        "lecture": "se-basics"
      },
      {
        "text": "* Anti-Trust violations",
        "lecture": "se-basics"
      },
      {
        "text": "* Importance of maintaining a large index of the web",
        "lecture": "se-basics"
      },
      {
        "text": "Search Engine Evaluation",
        "lecture": "se-evaluation"
      },
      {
        "text": "Information Retrieval (IR)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Query Understanding",
        "lecture": "se-evaluation"
      },
      {
        "text": "Ranking Algorithms",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Evaluation metrics for search engines",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Search result quality guidelines",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Using log files for evaluation",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Elements of good search results",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Search Engine Evaluation",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. Measuring search engine quality",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. Importance of evaluating search engines",
        "lecture": "se-evaluation"
      },
      {
        "text": "Relevant vs. Irrelevant information",
        "lecture": "se-evaluation"
      },
      {
        "text": "True Positive (tp), False Positive (fp), True Negative (tn), and False Negative (fn)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Precision, Recall, and Accuracy as measures of classification performance",
        "lecture": "se-evaluation"
      },
      {
        "text": "Set of relevant documents",
        "lecture": "se-evaluation"
      },
      {
        "text": "Set of retrieved documents",
        "lecture": "se-evaluation"
      },
      {
        "text": "Precision and Recall metrics",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Recall vs. Precision**: High recall can lead to low precision ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. The relationship between precision and number of documents retrieved: \"precision decreases as the number of docs retrieved (or recall) increases\" ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "* There are three Pythagorean means: arithmetic mean, geometric mean, harmonic mean",
        "lecture": "se-evaluation"
      },
      {
        "text": "* These means are useful in analyzing data, with each having different applications and interpretations",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** F-score (or F-measure): a harmonic mean of precision and recall used to evaluate algorithms and systems",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Harmonic mean: emphasizes the importance of small values, unlike arithmetic mean which is affected by outliers",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Recall**: The ratio of relevant items retrieved to all relevant items.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Precision**: The ratio of relevant items retrieved to all items retrieved.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Relevant documents**: Documents that are actually relevant to the query or task at hand.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Ranking #1 and #2:",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Recall and Precision:",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Average precision across multiple queries for relevant documents (  )",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Ranking #1 docs: average precision calculation (  )",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Mean Average Precision (MAP)** : A measure of the effectiveness of a search system that averages the average precision scores for each query.",
        "lecture": "se-evaluation"
      },
      {
        "text": "Information Retrieval Evaluation",
        "lecture": "se-evaluation"
      },
      {
        "text": "Recall and Precision metrics for evaluating search results",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Mean Average Precision (MAP)",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** Average over large document collection: The idea of using a large collection of documents to determine relevance.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Query ensembles: Using multiple queries or search terms to assess relevance.",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. **** Human relevance assessments: Assessing the relevance of search results based on human judgment.",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** Discounted Cumulative Gain (DCG) is a measure that penalizes highly relevant documents appearing lower in search result lists.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Graded Relevance value is reduced logarithmically proportional to the position of the result.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Discounting in Document Ranking**: We want high weights for high-ranked documents because searchers are likely to inspect them, and low weights for low-ranked documents.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Information Retrieval evaluation measures (e.g., Precision, Recall, F-score)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Gridded results display vs. sequential scanning of results",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Different types of metrics (Binary, Graded, Cumulative)",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** Search engines have test collections of queries and hand-ranked results",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Recall is difficult to measure on the web",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. **** Precision at top positions (e.g., top 10) is a common evaluation metric",
        "lecture": "se-evaluation"
      },
      {
        "text": "1.  Google relies on raters to evaluate search results and search experience. (HIGH)",
        "lecture": "se-evaluation"
      },
      {
        "text": "2.  Data generated by raters is statistically analyzed to give a view of the quality of search results and search experience. (HIGH)",
        "lecture": "se-evaluation"
      },
      {
        "text": "3.  Ability to measure the effect of proposed changes to Google's search algorithms is crucial. (MEDIUM)",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Search Quality Evaluator Guidelines**  - The document provides guidelines for evaluators to rate search results.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **Rating Scale Categories**  - There are six categories used to evaluate search result relevance.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* : Method for evaluating search result quality (HIGH)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* : Method for testing search algorithm changes on a small scale (MEDIUM)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* : Final evaluation and release of improved search results (HIGH)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* A/B testing is comparing two versions of a web page to see which one performs better ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Single innovation test ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Using user clicks for evaluation**: This concept highlights the idea that user interactions (clicks) can be used to assess website performance, usability, and overall effectiveness.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Conference on Information and Knowledge Management (CIKM)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* International forum for presentation and discussion of research on information and knowledge management",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  **Information and Knowledge Management**: Provides an international forum for presentation and discussion of research on information and knowledge management.",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  **CIKM Conference**: A conference that brings together researchers to present and discuss research on information and knowledge management.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Query log files used for both tuning and evaluating search engines",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Query logs contain various techniques such as query suggestion",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Clicks are not relevance judgments",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Display Improvements** () - The concept of improving search results display to provide more relevant information to users.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **autocomplete anticipations** () - A feature that suggests possible search queries based on a user's input.",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. **Extensions to More Data** () - The idea of incorporating additional data sources into search results, such as books, news, images, patents, and air schedules.",
        "lecture": "se-evaluation"
      },
      {
        "text": "4. **Featured Snippets** () - A summary of the most relevant information from a webpage, displayed at the top of the search results page.",
        "lecture": "se-evaluation"
      },
      {
        "text": "5. **Knowledge Graph** () - A knowledge base that provides additional information about entities such as people, places, and organizations.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Comparison of web search engines (marked as )",
        "lecture": "se-evaluation"
      },
      {
        "text": "Information Retrieval (IR) vs. text classification",
        "lecture": "text_processing"
      },
      {
        "text": "Standing queries: periodic search for new relevant documents",
        "lecture": "text_processing"
      },
      {
        "text": "Relevant vs. not relevant classification",
        "lecture": "text_processing"
      },
      {
        "text": "**Tokenization**: The process of breaking down text into individual tokens or words (implied in the first tweet by @Robertoross)",
        "lecture": "text_processing"
      },
      {
        "text": "**Natural Language Processing (NLP)**: A field of study that focuses on interactions between computers and human language (mentioned in various parts of the email)",
        "lecture": "text_processing"
      },
      {
        "text": "- Real estate investment ()",
        "lecture": "text_processing"
      },
      {
        "text": "- No-money-down property purchase ()",
        "lecture": "text_processing"
      },
      {
        "text": "+  Real estate investment",
        "lecture": "text_processing"
      },
      {
        "text": "+  No-money-down property purchase",
        "lecture": "text_processing"
      },
      {
        "text": "**Representation of Text Documents**: How to represent text documents in a way that can be processed by computers.",
        "lecture": "text_processing"
      },
      {
        "text": "**Bag of Words**: A type of high-dimensional space used to represent text documents.",
        "lecture": "text_processing"
      },
      {
        "text": "**Classification Functions**: Also known as \"classifiers\", these are functions that determine the category of a document.",
        "lecture": "text_processing"
      },
      {
        "text": "1. **Test language**",
        "lecture": "text_processing"
      },
      {
        "text": "2. **Data: Cet proof** (likely referring to \"CET\" or \"Certification\")",
        "lecture": "text_processing"
      },
      {
        "text": "3. **Artificial Intelligence (AI)**",
        "lecture": "text_processing"
      },
      {
        "text": "4. **Multimedia, Machine Learning, Programming, and Intelligence**",
        "lecture": "text_processing"
      },
      {
        "text": "Rule-based classifiers",
        "lecture": "text_processing"
      },
      {
        "text": "Hand-coded rule-based classifiers",
        "lecture": "text_processing"
      },
      {
        "text": "Text processing",
        "lecture": "text_processing"
      },
      {
        "text": "* Hand-coded rule-based classifiers",
        "lecture": "text_processing"
      },
      {
        "text": "**Text Classification**: The process of assigning a class label to a document based on its content.",
        "lecture": "text_processing"
      },
      {
        "text": "**Supervised Learning**: requires hand-classified training data",
        "lecture": "text_processing"
      },
      {
        "text": "**No Free Lunch**: implies that each learning method has its own strengths and weaknesses",
        "lecture": "text_processing"
      },
      {
        "text": "**Mixture of Methods**: commercial systems often use a combination of different machine learning methods",
        "lecture": "text_processing"
      },
      {
        "text": "*  Supervised learning classifiers can use various features in text processing",
        "lecture": "text_processing"
      },
      {
        "text": "*  Bag of words view of documents",
        "lecture": "text_processing"
      },
      {
        "text": "* Text collections have a large number of features",
        "lecture": "text_processing"
      },
      {
        "text": "* Selection can make particular classifiers feasible",
        "lecture": "text_processing"
      },
      {
        "text": "* Reduces training time",
        "lecture": "text_processing"
      },
      {
        "text": "* Makes runtime models smaller and faster",
        "lecture": "text_processing"
      },
      {
        "text": "* Can improve generalization (performance)",
        "lecture": "text_processing"
      },
      {
        "text": "1.  **Simplest Feature Selection Method**: The most common terms can be used for feature selection with no particular foundation.",
        "lecture": "text_processing"
      },
      {
        "text": "2.  **Well-Estimatable Terms**: Words that can be well-estimated are often available as evidence.",
        "lecture": "text_processing"
      },
      {
        "text": "Naive Bayes has found a home in spam filtering",
        "lecture": "text_processing"
      },
      {
        "text": "Spam filters often use features beyond just words",
        "lecture": "text_processing"
      },
      {
        "text": "Document as a vector",
        "lecture": "text_processing"
      },
      {
        "text": "High-dimensional vector space",
        "lecture": "text_processing"
      },
      {
        "text": "Classification in high-dimensional space",
        "lecture": "text_processing"
      },
      {
        "text": "* Vector space classification",
        "lecture": "text_processing"
      },
      {
        "text": "* Labeled set of points (equivalently, vectors)",
        "lecture": "text_processing"
      },
      {
        "text": "* Documents in the same class form contiguous region of space",
        "lecture": "text_processing"
      },
      {
        "text": "* Documents from different classes don't overlap (much)",
        "lecture": "text_processing"
      },
      {
        "text": "Text classification or categorization",
        "lecture": "text_processing"
      },
      {
        "text": "+  Text categorization ( Government, Science, Arts)",
        "lecture": "text_processing"
      },
      {
        "text": "*  Centroid: the vector space representation of a set of documents",
        "lecture": "text_processing"
      },
      {
        "text": "*  Rocchio forms a simple representative for each class: the centroid/prototype",
        "lecture": "text_processing"
      },
      {
        "text": "*  Classification: nearest prototype/centroid",
        "lecture": "text_processing"
      },
      {
        "text": "*  kNN (k-Nearest Neighbor) algorithm",
        "lecture": "text_processing"
      },
      {
        "text": "*  Classification of documents using nearest neighbors",
        "lecture": "text_processing"
      },
      {
        "text": "**Voronoi Diagram**: A way of partitioning a plane into regions based on proximity to points.",
        "lecture": "text_processing"
      },
      {
        "text": "* **Just store the labeled training examples**",
        "lecture": "text_processing"
      },
      {
        "text": "* **kNN (k-Nearest Neighbors)**",
        "lecture": "text_processing"
      },
      {
        "text": "* **Contiguity hypothesis**",
        "lecture": "text_processing"
      },
      {
        "text": "*  1-Nearest Neighbor (1NN) algorithm",
        "lecture": "text_processing"
      },
      {
        "text": "*  Robustness of k-Nearest Neighbors algorithm",
        "lecture": "text_processing"
      },
      {
        "text": "*  No feature selection necessary",
        "lecture": "text_processing"
      },
      {
        "text": "*  Scales well with large number of classes",
        "lecture": "text_processing"
      },
      {
        "text": "* Categorize them accordingly with the corresponding marks (, [DEFINITION], [FORMULA], [ALGORITHM], [EXAMPLE], and [PRIORITY])",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Web crawler: a computer program that visits web pages in an organized way",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Web crawlers are sometimes called spiders or robots",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Importance of understanding web crawlers for web crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **Quality in web crawling**: finding the \"Best\" pages first",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **Efficiency in web crawling**: avoiding duplication or near duplication",
        "lecture": "web_crawling"
      },
      {
        "text": "3. **Etiquette in web crawling**: behaving politely to not disturb website performance",
        "lecture": "web_crawling"
      },
      {
        "text": "Web Crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "Seed pages (known pages to start with)",
        "lecture": "web_crawling"
      },
      {
        "text": "Fetching and parsing web pages",
        "lecture": "web_crawling"
      },
      {
        "text": "Database storage of crawled pages",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Web crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Indexes",
        "lecture": "web_crawling"
      },
      {
        "text": "1.  - Crawling the entire web is not feasible with one machine (distributed processing)",
        "lecture": "web_crawling"
      },
      {
        "text": "2.  - Handling/Avoiding malicious pages",
        "lecture": "web_crawling"
      },
      {
        "text": "3.  - Latency/bandwidth to remote servers can vary widely",
        "lecture": "web_crawling"
      },
      {
        "text": "Protocol for web crawler limitations",
        "lecture": "web_crawling"
      },
      {
        "text": "Robots.txt file defines crawling permissions",
        "lecture": "web_crawling"
      },
      {
        "text": "**robots.txt directives**: rules that control which pages a robot can crawl on a website",
        "lecture": "web_crawling"
      },
      {
        "text": "**User-agent**: identifies the type of robot visiting the domain (e.g., Slurp)",
        "lecture": "web_crawling"
      },
      {
        "text": "**Disallow**: specifies URLs that should not be crawled",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Robots.txt files",
        "lecture": "web_crawling"
      },
      {
        "text": "*  User-agent directives",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **Determining bias to search engines from robots.txt **",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **Favored and disfavored robots **",
        "lecture": "web_crawling"
      },
      {
        "text": "3. **Robot names in robots.txt files **",
        "lecture": "web_crawling"
      },
      {
        "text": "4. **Association between robot names and AP(r) values **",
        "lecture": "web_crawling"
      },
      {
        "text": "+  BREADTH-FIRST SEARCH: A web crawling algorithm that examines all pages at a given level before moving on to the next level.",
        "lecture": "web_crawling"
      },
      {
        "text": "**Depth-first Search**: A traversal algorithm used to explore a graph or tree data structure.",
        "lecture": "web_crawling"
      },
      {
        "text": "Web Wide Crawl",
        "lecture": "web_crawling"
      },
      {
        "text": "PageRank algorithm developed by Google for determining page value",
        "lecture": "web_crawling"
      },
      {
        "text": "Breadth-First Search (BFS) crawling brings in high-quality pages early",
        "lecture": "web_crawling"
      },
      {
        "text": "* Web crawling process",
        "lecture": "web_crawling"
      },
      {
        "text": "* Crawling a webpage involves downloading its contents and extracting links to other relevant pages",
        "lecture": "web_crawling"
      },
      {
        "text": "How new links are added to the queue determines the search strategy",
        "lecture": "web_crawling"
      },
      {
        "text": "Search strategies can be breadth-first (FIFO) or depth-first (LIFO)",
        "lecture": "web_crawling"
      },
      {
        "text": "Focused crawlers direct their search towards \"interesting\" pages",
        "lecture": "web_crawling"
      },
      {
        "text": "Heuristic ordering of URLs in the queue can improve crawling efficiency",
        "lecture": "web_crawling"
      },
      {
        "text": "*  The web is a graph, not a tree, which means that links can be bidirectional and cyclic.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  An Accrawler must efficiently index URLs as well as already visited pages.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  To determine if a URL has already been seen, the crawler must store URLs in a standard format and develop a fast way to check if the URL has already been seen.",
        "lecture": "web_crawling"
      },
      {
        "text": "Finding all links in a page and extracting URLs is crucial for web crawling.",
        "lecture": "web_crawling"
      },
      {
        "text": "* URLs are often long and storing all unique URLs can require large amounts of storage space",
        "lecture": "web_crawling"
      },
      {
        "text": "* Hashing on host/domain name to determine uniqueness",
        "lecture": "web_crawling"
      },
      {
        "text": "* Trie data structure for efficient lookup",
        "lecture": "web_crawling"
      },
      {
        "text": "* Delta-encoded text file for compact storage",
        "lecture": "web_crawling"
      },
      {
        "text": "**Tries**: A data structure that can store multiple \"words\" with the same prefix.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **URL Normalization**: The process of standardizing URLs to eliminate variations.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  URL Normalization",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Scheme and host components of a URL are case-insensitive",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Percent-encoding triplet is case-insensitive",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Default port (port 80 for the \"http\" scheme) can be removed from or added to a URL",
        "lecture": "web_crawling"
      },
      {
        "text": "Spider Trap: A situation where a crawler re-visits the same page over and over again.",
        "lecture": "web_crawling"
      },
      {
        "text": "Session ID Management: The use of unique IDs to keep track of visitors, often used in J2EE, ASP, .NET, and PHP.",
        "lecture": "web_crawling"
      },
      {
        "text": "URL Length Monitoring: A technique to avoid spider traps by monitoring the length of URLs and stopping if it gets too long.",
        "lecture": "web_crawling"
      },
      {
        "text": "* First generation of spam web pages (use of repeated terms)",
        "lecture": "web_crawling"
      },
      {
        "text": "* Second generation of spam web pages (cloaking)",
        "lecture": "web_crawling"
      },
      {
        "text": "* Third generation of spam web pages (doorway pages)",
        "lecture": "web_crawling"
      },
      {
        "text": "**DNS Caching Server**: A server that stores DNS responses to reduce the number of requests made to the DNS resolver.",
        "lecture": "web_crawling"
      },
      {
        "text": "**UDP for DNS**: The use of User Datagram Protocol (UDP) for transmitting DNS requests and responses.",
        "lecture": "web_crawling"
      },
      {
        "text": "**Parallel Threads Waiting**: The ability of a web crawler to perform multiple tasks concurrently using parallel threads.",
        "lecture": "web_crawling"
      },
      {
        "text": "Measuring and tuning a crawler for peak performance involves improving URL parsing speed, network bandwidth speed, and fault tolerance.",
        "lecture": "web_crawling"
      },
      {
        "text": "DNS lookup implementation",
        "lecture": "web_crawling"
      },
      {
        "text": "DNS caching server",
        "lecture": "web_crawling"
      },
      {
        "text": "* Network delay in downloading individual pages is a bottleneck in web crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "* Having multiple threads running in parallel can improve throughput",
        "lecture": "web_crawling"
      },
      {
        "text": "* A thread of execution is the smallest sequence of programmed instructions that can be managed independently by the scheduler",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Centralized crawler control**: a system where one main crawler controls multiple parallel crawlers running on a LAN",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Distributed crawling**: a system where multiple crawlers run on widely distributed machines with or without cross communication",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Distributed crawlers must periodically update master index",
        "lecture": "web_crawling"
      },
      {
        "text": "* **Scalability**: ability to handle large-scale web-crawls",
        "lecture": "web_crawling"
      },
      {
        "text": "* **Network-load dispersion and reduction**: distributing network load by dividing the web into regions",
        "lecture": "web_crawling"
      },
      {
        "text": "* **** Three strategies for web crawling: Independent, Dynamic assignment, Static assignment",
        "lecture": "web_crawling"
      },
      {
        "text": "Inter-partition links",
        "lecture": "web_crawling"
      },
      {
        "text": "Handling inter-partition links in web crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "Firewall mode, Cross-over mode, Exchange mode for handling inter-partition links",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Exchange mode limitations",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Batch communication",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Replication in web crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "*  URL-hash based partitioning",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Site-hash based partitioning",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Hierarchical partitioning (e.g., by TLD)",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Firewall crawlers and their benefits",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Cross-over approach for 100% quality",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Combination of policies for Web crawler behavior",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Selection policy",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Re-visit policy",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Politeness policy",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Parallelization policy",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Dynamic Web**: The web is dynamic, with many new pages, updated pages, deleted pages, etc.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Page Update Tracking**: Periodically check crawled pages for updates and deletions to maintain freshness.",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **** Steady crawler: runs continuously without pause",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **** Shadowing: implies new set of pages are collected and stored separately",
        "lecture": "web_crawling"
      },
      {
        "text": "3. **** In-place updating: updates index current by replacing old versions with new ones",
        "lecture": "web_crawling"
      },
      {
        "text": "4. **** Multiple crawlers: typically used by search engines to improve crawling efficiency",
        "lecture": "web_crawling"
      },
      {
        "text": "* Re-visiting policies ()",
        "lecture": "web_crawling"
      },
      {
        "text": "* Uniform policy ()",
        "lecture": "web_crawling"
      },
      {
        "text": "* Proportional policy ()",
        "lecture": "web_crawling"
      },
      {
        "text": "* Average freshness ()",
        "lecture": "web_crawling"
      },
      {
        "text": "* Cho and Garcia-Molina's result that uniform policy outperforms proportional policy in terms of average freshness ()",
        "lecture": "web_crawling"
      },
      {
        "text": "* Sitemap is a list of pages accessible to crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "* Helps search engine crawlers find pages on the site",
        "lecture": "web_crawling"
      },
      {
        "text": "Use consistent, fully-qualified URLs to ensure Google crawls your site correctly.",
        "lecture": "web_crawling"
      },
      {
        "text": "Sitemaps can be posted anywhere on your site but only affect descendants of the parent directory.",
        "lecture": "web_crawling"
      },
      {
        "text": "Session IDs should not be included in sitemap URLs.",
        "lecture": "web_crawling"
      },
      {
        "text": "* Multiple crawlers used by Google (e.g., Googlebot, AdsBot)",
        "lecture": "web_crawling"
      },
      {
        "text": "* Different types of crawlers for specific tasks (e.g., images, news, video)",
        "lecture": "web_crawling"
      },
      {
        "text": "* Importance of understanding how Googlebot sees a website",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **Multiple Crawlers Used by Google**  (HIGH)",
        "lecture": "web_crawling"
      },
      {
        "text": "3. **Importance of Understanding How Googlebot Sees a Website**  (HIGH)",
        "lecture": "web_crawling"
      },
      {
        "text": "* Googlebot cannot see within Flash files, audio/video tracks, and content within programs",
        "lecture": "web_crawling"
      },
      {
        "text": "* Many versions of Googlebot are run on multiple machines located near the site they are indexing",
        "lecture": "web_crawling"
      },
      {
        "text": "* Importance of creating an empty robots.txt file to prevent \"File not found\" in website error log",
        "lecture": "web_crawling"
      },
      {
        "text": "**Web Crawling**: The process of automatically scanning and indexing web pages to gather information.",
        "lecture": "web_crawling"
      },
      {
        "text": "**IP Address Verification**: Verifying the IP address of a web crawler to ensure it is legitimate.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Crawl rate: The number of requests per second Googlebot makes to a site when crawling it.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Googlebot must understand and execute JavaScript code to extract meaningful features from web pages.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Browsers render HTML hierarchy, but also include transformations via CSS and JavaScript.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Web Trends",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Measurements",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ The web has undergone significant changes over the last 30+ years",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Understanding the different dimensions of the web is crucial for building a web search engine today",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Scale, complexity, and growth are important factors to consider",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The internet is used by a significant portion of the world's population ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The internet penetration rate can be expressed as a percentage of the total population ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. Global Internet Properties () - refers to popular websites with a global reach",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. China's growing influence on the internet ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. Top 10 Internet Properties by Global Monthly Unique Visitors ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Mobile internet users growth rate in China",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Year-over-year (Y/Y) growth comparison",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** China Mobile Internet Usage Leaders: Tencent, Alibaba, Baidu dominate 71% of mobile time spent",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** Share of Mobile Time Spent: WeChat leads with ~200 minutes per user, average QQ",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The amount of global digital information created and shared is increasing exponentially",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Digital info has grown 9x in five years to nearly 2 zettabytes in 2011, per IDC.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The growth of photo sharing remains robust despite new platforms emerging",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Photos uploaded and shared daily number is doubling every year (since 2005-2014)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Online Video & Entertainment",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Hours of video uploaded to YouTube every minute (as a key metric)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Growth rate of online video content between 2014 and 2020",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Mobile devices (excluding tablets) generate a significant portion of global website traffic.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The percentage of mobile device-generated traffic has consistently hovered around 50% since 2017.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** Tablet growth is more rapid than smartphone growth, specifically iPad growth is ~3x iPhone growth.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** The rate of tablet adoption (iPad) vs. smartphone adoption (iPhone) can be compared using cumulative unit shipments.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Product Finding = Often Starts @ Search (Amazon + Google...)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Technology Cycles: refers to the pattern of technological innovation and adoption in computing devices",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  10-Year Cycle: a common trend where new technologies tend to last about 10 years before being replaced or surpassed",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. Re-Imagination of Computing Operating Systems ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. Global Market Share of Personal Computing Platforms by Operating System Shipments ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Market Share of Cloud Hosting Providers",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Leading Cloud Hosting Providers (Microsoft Azure, Alibaba Group, Google Cloud Platform, etc.)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Amazon Web Services (AWS) is leading the cloud charge",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Cloud Revenue Re-Accelerating",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Volume Effects",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Voice-related commands have increased significantly since 2008 after the launch of iPhone and Google Voice Search.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Voice-Based Mobile Platform Front-Ends: The idea that voice can replace typing in mobile interactions",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Natural / Conversational Language: The use of natural language processing (NLP) to understand user requests",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Voice assistants (e.g., Amazon Echo)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Growth in number of users connected",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Transition from desktop/laptop use to mobile",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Move away from server farms to cloud computing",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Decreased dominance of Microsoft Windows",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The World Wide Web is dynamic and hard to describe accurately over time",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The total number of websites on the internet (approximately 1.7 billion)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The popularity of different web servers (Apache, nginx, Microsoft IIS)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The percentage of inactive/parked websites (around 75%)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The number of websites in the world has been growing rapidly over the years ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Websites growth rate from 1991 to 2021 is significant ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Domain Count Statistics for TLDs (TLD = Top-Level Domains)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The importance of having a record count that represents all domains known about, which is usually more accurate than other sources",
        "lecture": "web_serving_basics"
      },
      {
        "text": "**KEY CONCEPTS **",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **Language diversity**: Estimated 40,000 languages created by humans, with only 6,000-9,000 still in use.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **Internet language shift**: Decline of English as primary language among internet users from 80% to 40%.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "**Web Serving Basics**: The fundamental principles of serving web content.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "**Java Applet/Servlet**: Java-based components used to extend the functionality of web servers.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Content types have a wide range (16,000 to 51,000)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Parsing content types is necessary for various applications",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The Web has approximately 86 billion websites",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The distribution of websites across TLDs (e.g., .com) is uneven, with 72% in .com",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The number of web pages is estimated to be around 30 trillion unique URLs",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  HTML, PDF, Word, Excel, PPT, and others are examples of content types",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The Web graph follows a power law distribution for in-degree and out-degree",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ The Web has approximately 86 billion websites ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Distribution of websites across TLDs is uneven ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Number of web pages estimated to be around 30 trillion unique URLs ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ HTML, PDF, Word, Excel, PPT, and others are examples of content types ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Storage required to hold a single snapshot of the Web: 100 petabytes ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Human editors were used by Yahoo! in the past to assemble large directories.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "The European Car Webzine focuses on prestige marques, includes articles, web broadcasting, screen savers, dealer business & economy.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **ODP - Open Directory Project**:  A collaborative effort to organize the web (HIGH)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **Ontology**:  A systematic arrangement of concepts or entities in a particular field or domain (MEDIUM)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. **Distributed directory**:  A system where data is stored and managed across multiple locations or servers (MEDIUM)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "4. **RDF format**:  A standard format for representing data on the web using resource description framework (LOW)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Open Directory - a online directory of web content",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Mozilla Firefox - a web browser",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The Internet Archive's snapshot of the World Wide Web",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Apache Nutch",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Wayback Machine",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Crawler algorithm",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** Multilingual Databases: Databases that store information in multiple languages to cater to diverse user needs.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** Deep Web: A part of the internet that is not indexed by search engines and can only be accessed through specific browsers or tools, designed for anonymity (e.g., Tor).",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. **** Dark Web: A subset of the Deep Web, often associated with illicit activities.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "4. **** Government Resources: Information stored in databases that are restricted to authorized personnel or require special access.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1.  **Video Search Engine**: A web-based search engine that crawls the web primarily for video content. (Slide 1)",
        "lecture": "youtube"
      },
      {
        "text": "2.  **Indexing of Video Content**: The process of acquiring metadata associated with a video, such as author, title, creation date, duration, coding quality, tags, description, subtitles, and transcription.",
        "lecture": "youtube"
      },
      {
        "text": "3.  **Ranking of Videos**: The process of ordering videos under a query based on relevance, user preferences, date of upload, number of views, or user rating.",
        "lecture": "youtube"
      },
      {
        "text": "+  Video search engines",
        "lecture": "youtube"
      },
      {
        "text": "+  Web-wide video search engine",
        "lecture": "youtube"
      },
      {
        "text": "+  All-content search engine",
        "lecture": "youtube"
      },
      {
        "text": "+  Integrated universal search engine for science-oriented videos",
        "lecture": "youtube"
      },
      {
        "text": "* Video hosting is highly concentrated on a small number of websites due to large file sizes involved",
        "lecture": "youtube"
      },
      {
        "text": "* YouTube.com has become the defacto site for uploading videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Subscription video on demand services (e.g., Hulu, Netflix)",
        "lecture": "youtube"
      },
      {
        "text": "*  Ownership structure of Hulu (jointly owned by major media companies)",
        "lecture": "youtube"
      },
      {
        "text": "**Video Subtitling Services**: There are three main types of video subtitling services: open caption, closed caption, and SDH (Subtitles for the Deaf and Hard of Hearing).",
        "lecture": "youtube"
      },
      {
        "text": "**Speech Recognition**: used to extract phrases from audio transcripts for better indexing.",
        "lecture": "youtube"
      },
      {
        "text": "**Text Recognition**: uses OCR on video slides to detect words.",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube is a video hosting website",
        "lecture": "youtube"
      },
      {
        "text": "*  The site allows users to upload, view, rate, share, add to favorites, report, and comment on videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Google acquired YouTube in 2006 for $1.65 billion",
        "lecture": "youtube"
      },
      {
        "text": "* YouTube is a search engine ()",
        "lecture": "youtube"
      },
      {
        "text": "* YouTube processes more than 3 billion searches per month ()",
        "lecture": "youtube"
      },
      {
        "text": "* YouTube is transforming and has surpassed other search engines like Bing ()",
        "lecture": "youtube"
      },
      {
        "text": "1. **** YouTube traffic growth rate: 60 hours of video uploaded every minute",
        "lecture": "youtube"
      },
      {
        "text": "2. **** Large-scale data storage capacity: estimated to be 1 sextillion gigabytes",
        "lecture": "youtube"
      },
      {
        "text": "YouTube's major hurdles (beyond crawling, indexing, and ranking)",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube requires a registered user to upload videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Channels include thumbnails of uploaded videos, members subscribed, favorite videos, friends' lists",
        "lecture": "youtube"
      },
      {
        "text": "*  Having 1 million subscribers as a YouTuber can earn between $300,000 - $2 million per year",
        "lecture": "youtube"
      },
      {
        "text": "*  To be in the top 1000 YouTubers, you must have approximately 1.8 million subscribers",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube captures videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Upload process on YouTube (making a video live)",
        "lecture": "youtube"
      },
      {
        "text": "*  Video management on YouTube (adding more videos, custom thumbnail)",
        "lecture": "youtube"
      },
      {
        "text": "*  Video/Audio quality",
        "lecture": "youtube"
      },
      {
        "text": "*  Encoding into streamable file format for faster video/audio quality",
        "lecture": "youtube"
      },
      {
        "text": "**Monetization**: YouTube allows creators to specify how they want to be monetized (paid promotions, sponsorships, etc.)",
        "lecture": "youtube"
      },
      {
        "text": "**Age Restrictions**: YouTube enables age restrictions on videos, allowing creators to control who can view their content",
        "lecture": "youtube"
      },
      {
        "text": "**License and Ownership**: The Standard YouTube License governs the use of licensed content on the platform",
        "lecture": "youtube"
      },
      {
        "text": "1. **YouTube Search Algorithm**",
        "lecture": "youtube"
      },
      {
        "text": "2. **Vevo**",
        "lecture": "youtube"
      },
      {
        "text": "Computer Science Education: The importance of computer science education and its challenges.",
        "lecture": "youtube"
      },
      {
        "text": "Online Resources for Learning: The role of online platforms like YouTube in providing educational resources for computer science students.",
        "lecture": "youtube"
      },
      {
        "text": "+ FEATURES",
        "lecture": "youtube"
      },
      {
        "text": "1.  **FILTERS**: YouTube's search filters",
        "lecture": "youtube"
      },
      {
        "text": "2.  **FEATURES ALGORITHMS**: Mentioned but not explicitly explained",
        "lecture": "youtube"
      },
      {
        "text": "YouTube uses a set of metrics to rank search results",
        "lecture": "youtube"
      },
      {
        "text": "Video quality, metadata, views, likes, shares, links, subtitles/closed captions are all ranking factors",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube supports multiple video formats for uploading (MOV, MP4, AVI, WMV, FLV, 3GP, MPEGPS, WebM)",
        "lecture": "youtube"
      },
      {
        "text": "*  Aspect Ratio matters when uploading videos to YouTube",
        "lecture": "youtube"
      },
      {
        "text": "*  Video file size limit on YouTube is 128GB",
        "lecture": "youtube"
      },
      {
        "text": "*  Default video length limit on YouTube is 15 minutes (can be extended)",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube videos are played in the browser, assuming it supports HTML5",
        "lecture": "youtube"
      },
      {
        "text": "*  No native support for running YouTube videos on some devices (e.g. Apple products), requires separate app or transcoding",
        "lecture": "youtube"
      },
      {
        "text": "*  Different video standards used by various devices (e.g. H.264)",
        "lecture": "youtube"
      },
      {
        "text": "*  Computer algorithms play a crucial role in YouTube's recommendation system to maximize watch time.",
        "lecture": "youtube"
      },
      {
        "text": "* OUTube Recommendation Algorithm ()",
        "lecture": "youtube"
      },
      {
        "text": "* Query ()",
        "lecture": "youtube"
      },
      {
        "text": "* Selection ()",
        "lecture": "youtube"
      },
      {
        "text": "Association Rule Mining",
        "lecture": "youtube"
      },
      {
        "text": "Co-visitation counts",
        "lecture": "youtube"
      },
      {
        "text": "Relatedness (r(vi,vj))",
        "lecture": "youtube"
      },
      {
        "text": "Normalization function (f(v; vj))",
        "lecture": "youtube"
      },
      {
        "text": "* Video-rich snippet",
        "lecture": "youtube"
      },
      {
        "text": "* YouTube's dominance in video-rich snippets",
        "lecture": "youtube"
      },
      {
        "text": "A Content Distribution Network (CDN) consists of a large set of content servers and means for dynamically selecting servers based on knowledge of the location of the user and possibly the content being requested.",
        "lecture": "youtube"
      },
      {
        "text": "+  Identifying billions of videos",
        "lecture": "youtube"
      },
      {
        "text": "+  Efficiently delivering video to desktop/mobile device",
        "lecture": "youtube"
      },
      {
        "text": "YouTube Video Cache Locations",
        "lecture": "youtube"
      },
      {
        "text": "Geographical distribution of video content",
        "lecture": "youtube"
      },
      {
        "text": "**Video Transcoding**: The technique of converting video into multiple different formats and resolutions to make it playable across different devices and bandwidths.",
        "lecture": "youtube"
      },
      {
        "text": "**Content Distribution Network (CDN)**: A system that sends transcoded copies of videos to various locations for faster playback.",
        "lecture": "youtube"
      },
      {
        "text": "[DEFINITION] **Video Transcoding**:",
        "lecture": "youtube"
      },
      {
        "text": "[DEFINITION] **Content Distribution Network (CDN)**:",
        "lecture": "youtube"
      },
      {
        "text": "*  **DNS Resolution**: Local DNS server resolves domain name (www.youtube.com) to IP address",
        "lecture": "youtube"
      },
      {
        "text": "*  **YouTube Video Delivery Process**: 4-step process for delivering YouTube video:",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube video delivery system design consists of three components: flat video ID space, multi-layered logical server organization, and 3-tiered physical cache hierarchy.",
        "lecture": "youtube"
      },
      {
        "text": "*  Video ID space is \"flat\" and has five \"anycast namespaces\" (with two unicast namespaces).",
        "lecture": "youtube"
      },
      {
        "text": "*  Complicated re-direction scheme to find the nearest data center to serve the video",
        "lecture": "youtube"
      },
      {
        "text": "*  Minimizing Round Trip Time (RTT)",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube CDN (Content Delivery Network)",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube had no way of making money in its early days",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube's infrastructure was expensive to maintain",
        "lecture": "youtube"
      },
      {
        "text": "*  Copyright infringement issues led to lawsuits against YouTube",
        "lecture": "youtube"
      },
      {
        "text": "*  Content ID: a fingerprint database of copyrighted content used by YouTube",
        "lecture": "youtube"
      },
      {
        "text": "*  Copyright infringement detection: the process of identifying and addressing copyright violations on YouTube",
        "lecture": "youtube"
      },
      {
        "text": "1. **** Content ID is based on audio and video samples uploaded by rights holders to YouTube. (Slide 33)",
        "lecture": "youtube"
      },
      {
        "text": "2. **** Video processing involves transcoding into multiple formats, including HTML5, H.264, WebM VP8, HD, non-HD, and others.",
        "lecture": "youtube"
      },
      {
        "text": "3. **** Content ID uses a spectrogram-based approach for audio identification.",
        "lecture": "youtube"
      },
      {
        "text": "*  Acoustic Fingerprint: A unique representation of an audio signal",
        "lecture": "youtube"
      },
      {
        "text": "*  Spectrogram: Time-frequency graph representing three dimensions of audio (frequency, amplitude, and time)",
        "lecture": "youtube"
      },
      {
        "text": "**Content ID**: A system used by YouTube to automatically resolve copyright issues related to sound recordings.",
        "lecture": "youtube"
      },
      {
        "text": "**Hashing algorithm**: The process of identifying unique digital fingerprints to detect copyrighted content on YouTube.",
        "lecture": "youtube"
      },
      {
        "text": "*  Exponential growth of storage capacity",
        "lecture": "youtube"
      },
      {
        "text": "*  Kryder's Law (not explicitly defined)",
        "lecture": "youtube"
      },
      {
        "text": "*  Storage needs of YouTube",
        "lecture": "youtube"
      },
      {
        "text": "*  Data compression and re-encoding for storage efficiency",
        "lecture": "youtube"
      },
      {
        "text": "*  Scalability of data storage in cloud-based platforms like YouTube",
        "lecture": "youtube"
      },
      {
        "text": "1. **Content Delivery Network (CDN)** : The backbone of YouTube's rapid content delivery.",
        "lecture": "youtube"
      },
      {
        "text": "2. **Load Balancer** : Efficiently distributes incoming requests across multiple servers.",
        "lecture": "youtube"
      },
      {
        "text": "3. **Transcoding Servers** : Converts and optimizes video files into various formats.",
        "lecture": "youtube"
      },
      {
        "text": "4. **Metadata Database** : Stores crucial metadata associated with videos.",
        "lecture": "youtube"
      },
      {
        "text": "5. **Content Delivery Network (CDN)** as the backbone of YouTube's rapid content delivery, ensuring seamless streaming for users worldwide.",
        "lecture": "youtube"
      },
      {
        "text": "1. **** YouTube System Design: Understanding the architecture of a large-scale video sharing platform like YouTube is crucial for this topic.",
        "lecture": "youtube"
      },
      {
        "text": "2. **** Content Delivery Network (CDN): A distributed network of servers that cache and serve content to users based on their geographical location.",
        "lecture": "youtube"
      },
      {
        "text": "3. **** Load Balancer: A mechanism that distributes incoming traffic across multiple web servers to improve responsiveness, reliability, and scalability.",
        "lecture": "youtube"
      },
      {
        "text": "4. **** Media Storage (S3): Amazon Simple Storage Service is a cloud-based object storage system for storing large amounts of data.",
        "lecture": "youtube"
      }
    ],
    "definitions": [
      {
        "text": "1. **De-Duplication** -  (identifying and eliminating duplicate web pages)",
        "lecture": "deduplication"
      },
      {
        "text": "2. **Locker Storage** -  (storage strategy for maintaining a single copy of a file)",
        "lecture": "deduplication"
      },
      {
        "text": "1. **Virtual hosts**:  - A method for hosting multiple websites on a single server, where each website has its own domain name and is accessible through different hostnames.",
        "lecture": "deduplication"
      },
      {
        "text": "2. **Deduplication**:  - The process of eliminating duplicate data or URLs while preserving their unique characteristics.",
        "lecture": "deduplication"
      },
      {
        "text": "**SCOP (Structural Classification of Proteins)**: a database that classifies proteins based on their structural characteristics.",
        "lecture": "deduplication"
      },
      {
        "text": "**Domain**: a unit of protein structure that performs a specific function.",
        "lecture": "deduplication"
      },
      {
        "text": "**Fold**: a common three-dimensional structure found in multiple proteins.",
        "lecture": "deduplication"
      },
      {
        "text": "**Superfamily**: a group of related folds with similar structures.",
        "lecture": "deduplication"
      },
      {
        "text": "**Family**: a group of related superfamilies with similar structures.",
        "lecture": "deduplication"
      },
      {
        "text": "Deduplication: The process of identifying and eliminating duplicate copies of data.",
        "lecture": "deduplication"
      },
      {
        "text": "Snapshot: A copy of a webpage at a particular point in time.",
        "lecture": "deduplication"
      },
      {
        "text": "**Document Object Model (DOM)**: a tree-based structure for representing an HTML document",
        "lecture": "deduplication"
      },
      {
        "text": "* Mirroring: systematic replication of web pages across hosts ()",
        "lecture": "deduplication"
      },
      {
        "text": "Apache mirrors: a collection of servers that mirror or replicate content from a central server (implied by the context)",
        "lecture": "deduplication"
      },
      {
        "text": "Regions: geographic areas where Apache mirrors are located (listed in the \"regions\" section)",
        "lecture": "deduplication"
      },
      {
        "text": "*  **PageRank (measure of importance)**: A measure of a webpage's importance based on its in-links",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Mirror Sites**: Copies of websites hosted on different servers",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Clustering**: Grouping similar documents together.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Plagiarism detection**: Identifying pairs of documents that have significantly borrowed from each other.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Spam detection**: Using near-similarity techniques to identify spam emails.",
        "lecture": "deduplication"
      },
      {
        "text": "*  Duplicate Problem: A problem where duplicate data needs to be identified.",
        "lecture": "deduplication"
      },
      {
        "text": "*  Near-Duplicate Problem: A problem where approximate matching is required, e.g., detecting similar documents.",
        "lecture": "deduplication"
      },
      {
        "text": "*  Cryptographic hashing: A technique used for generating unique digital fingerprints of data.",
        "lecture": "deduplication"
      },
      {
        "text": "Cryptographic hash function: A hash function that takes an input (message) and returns a fixed-size alphanumeric string (hash value)",
        "lecture": "deduplication"
      },
      {
        "text": "Hash value: Also known as message digest, digital fingerprint, or checksum",
        "lecture": "deduplication"
      },
      {
        "text": "Input Digest: A fixed-size alphanumeric string resulting from a cryptographic hash function",
        "lecture": "deduplication"
      },
      {
        "text": "**MD5 (message-digest)**: a widely used cryptographic hash function producing a 128-bit (16-byte) hash value.",
        "lecture": "deduplication"
      },
      {
        "text": "**SHA-1**: a type of cryptographic hash function producing a 160-bit (20-byte) hash value.",
        "lecture": "deduplication"
      },
      {
        "text": "**SHA-2**: a family of algorithms that produce digests of size 224, 256, 384, and 512 bits.",
        "lecture": "deduplication"
      },
      {
        "text": "Distance measure: a method to compute similarity between two objects or documents.",
        "lecture": "deduplication"
      },
      {
        "text": "Euclidean distance: a type of distance measure that calculates the straight-line distance between two points in n-dimensional space.",
        "lecture": "deduplication"
      },
      {
        "text": "Jaccard distance: a type of distance measure that calculates the ratio of the sizes of the intersection and union of sets.",
        "lecture": "deduplication"
      },
      {
        "text": "Cosine distance: a type of distance measure that calculates the angle between two vectors.",
        "lecture": "deduplication"
      },
      {
        "text": "Edit distance: a type of distance measure that calculates the minimum number of insertions and deletions needed to transform one string into another.",
        "lecture": "deduplication"
      },
      {
        "text": "Hamming distance: a type of distance measure that calculates the number of components in which two Boolean vectors differ.",
        "lecture": "deduplication"
      },
      {
        "text": "* d(A, B) = distance between two sets A and B",
        "lecture": "deduplication"
      },
      {
        "text": "* s(A, B) = similarity between two sets A and B",
        "lecture": "deduplication"
      },
      {
        "text": "* Intersection: size of intersection between two sets / size of union",
        "lecture": "deduplication"
      },
      {
        "text": "* Union: size of intersection between two sets / size of union  (same as above, but note that it's not a standard definition)",
        "lecture": "deduplication"
      },
      {
        "text": "**Shingle Set (S(D,w))**: a set of shingles for a document D with width w",
        "lecture": "deduplication"
      },
      {
        "text": "**Jaccard(A,B)**: similarity measure defined as size of (S(A,w) intersect S(B,w)) / size of (S(A,w) union S(B,w))",
        "lecture": "deduplication"
      },
      {
        "text": "**Containment(A,B)**: similarity measure defined as size of (S(A,w) intersect S(B,w)) / size of (S(A,w))",
        "lecture": "deduplication"
      },
      {
        "text": "1.  **Shingle**: a fixed-size substring of a document",
        "lecture": "deduplication"
      },
      {
        "text": "2.  **Collision**: when two different documents match each other's shingles (exact wording preserved)",
        "lecture": "deduplication"
      },
      {
        "text": "3.  **Stop words**: common words that are typically omitted in deduplication",
        "lecture": "deduplication"
      },
      {
        "text": "*  Hash value: a unique numerical representation of a string or sequence",
        "lecture": "deduplication"
      },
      {
        "text": "*  Fingerprints: selected hash values that are considered representative of the original data",
        "lecture": "deduplication"
      },
      {
        "text": "*  Jaccard similarity (J(A,B)) = |A ∩ B| / |A ∪ B|",
        "lecture": "deduplication"
      },
      {
        "text": "*  Jaccard distance = 1 - J(A,B)",
        "lecture": "deduplication"
      },
      {
        "text": "*  k-shingles",
        "lecture": "deduplication"
      },
      {
        "text": "*  Fingerprint",
        "lecture": "deduplication"
      },
      {
        "text": "**SimHash**: a method developed by Moses Charikar for determining near duplicates of web pages.",
        "lecture": "deduplication"
      },
      {
        "text": "**f-bit fingerprint**: a fingerprint obtained for each document using the SimHash method.",
        "lecture": "deduplication"
      },
      {
        "text": "**k-bits apart**: a measure of the distance between two fingerprints, where a pair of documents are near duplicates if their fingerprints are at most k-bits apart.",
        "lecture": "deduplication"
      },
      {
        "text": "1. **** Ahash function: a type of hash function that produces unique hash values for different inputs",
        "lecture": "deduplication"
      },
      {
        "text": "2. **** Simhash: a type of hashing that measures similarity between items using bitwise Hamming distance",
        "lecture": "deduplication"
      },
      {
        "text": "1. **Shingles**: Breaking down an input phrase into smaller sub-phrases (features)",
        "lecture": "deduplication"
      },
      {
        "text": "2. **Hash function**: A mathematical function that takes a feature as input and produces a fixed-size hash value",
        "lecture": "deduplication"
      },
      {
        "text": "3. **Bitwise Hamming distance**: The number of positions at which two binary strings differ",
        "lecture": "deduplication"
      },
      {
        "text": "8-bit hash values: a way to represent text as a binary number using 8 bits (0s and 1s).",
        "lecture": "deduplication"
      },
      {
        "text": "Fingerprint formed from 8-bit hash values: a unique representation of the original text.",
        "lecture": "deduplication"
      },
      {
        "text": "Weights: frequencies or importance of each word in the original text.",
        "lecture": "deduplication"
      },
      {
        "text": "**Bitwise Hamming Distance (hdist)**: A measure of the difference between two numbers represented in binary, calculated as the number of positions at which the corresponding bits are different.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Hamming distance**: the number of positions at which two strings are different (implied by \"low Hamming distance\")",
        "lecture": "deduplication"
      },
      {
        "text": "* Bitwise Hamming distance: a measure of the number of different bits between two numbers",
        "lecture": "deduplication"
      },
      {
        "text": "* Permutation: rearrangement of bits or elements in a specific order",
        "lecture": "deduplication"
      },
      {
        "text": "* Rotating bits: shifting bits left and replacing lowest order bit with highest order bit",
        "lecture": "deduplication"
      },
      {
        "text": "1. **** Corpus: A set of documents to be indexed.",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. **** Indexing: The process of creating an organized system of information from a corpus.",
        "lecture": "info_retrieval"
      },
      {
        "text": "- Publishes legal, tax and regulatory information for legal, corporate, government and academic markets",
        "lecture": "info_retrieval"
      },
      {
        "text": "- Contains data from more than 1.4 billion unique records of key information",
        "lecture": "info_retrieval"
      },
      {
        "text": "- National Library of Medicine health information database",
        "lecture": "info_retrieval"
      },
      {
        "text": "* FTP'able documents: Documents that can be accessed via File Transfer Protocol (FTP)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Archie: A search engine developed in 1990 to index and search FTP sites",
        "lecture": "info_retrieval"
      },
      {
        "text": "* WAIS: An information retrieval system that uses a database of articles and documents",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  TREC (Text REtrieval Conferences): a series of organized competitions sponsored by NIST to evaluate IR systems.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Collaborative filtering algorithm: an approach used in recommender systems to predict user preferences based on the behavior of similar users.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Document Object Model (DOM): provides some clues about web page structure",
        "lecture": "info_retrieval"
      },
      {
        "text": "-  Human user aspects: The focus on how humans interact with information systems.",
        "lecture": "info_retrieval"
      },
      {
        "text": "-  User interface: The means by which users interact with a system or application.",
        "lecture": "info_retrieval"
      },
      {
        "text": "-  Visualization: The use of visual elements to represent and communicate information.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **First-order Predicate Logic**: a formal system that uses quantified variables over a specified domain of discourse",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Bayesian Networks**: directed acyclic graph model that represents a set of random variables and their dependencies",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Web Ontology Language (OWL)**: a family of knowledge representation languages for authoring ontologies",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Entity Recognition**: the process of identifying specific entities in natural language text (e.g. people, places, organizations)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Natural Language Processing (NLP)**: a field of study that focuses on the interaction between computers and human language",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Natural Language Generation (NLG)**: the process of generating human-like language from a computer system",
        "lecture": "info_retrieval"
      },
      {
        "text": "1. **Supervised Learning**  - automated classification of examples based on learning concepts from labeled training examples",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. **Unsupervised Learning**  - automated methods for clustering unlabeled examples into meaningful groups",
        "lecture": "info_retrieval"
      },
      {
        "text": "3. **Data Mining**  - the discovery of previously unknown properties of given data",
        "lecture": "info_retrieval"
      },
      {
        "text": "1. **** A data scientist is someone who knows how to extract meaning from and interpret data.",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. **** The role of a data scientist requires both tools and methods from statistics and machine learning, as well as human review.",
        "lecture": "info_retrieval"
      },
      {
        "text": "None explicitly stated",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Text Database: A collection of unstructured text data that is used as input for information retrieval systems.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  User Interface: The part of the system that allows users to interact with the search engine, submit queries, and view results.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Inverted index",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Query token",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Relevance metric",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Binary relevance: a binary (yes/no) notion of relevance",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Continuous relevance: ranked retrieval, where documents are ranked in order of relevance",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Set theoretic model: Boolean model (chapter 1 in Manning et al)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Statistical/algebraic model: Vector Space model (chapter 2 in Manning et al)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Chapter 11 in Manning et al refers to Probabilistic models",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Tokenization**: Breaking down text into individual words or tokens.",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Stemming**: Reducing words to their root form (e.g. \"running\" becomes \"run\").",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Stopwords**: Common words that do not carry much meaning, such as \"a\", \"the\", etc.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Keyword: a word representing a document",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Query: a request for information, expressed as a Boolean expression of keywords",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  AND operator: combines two or more keywords to search for documents containing all specified keywords",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  OR operator: combines two or more keywords to search for documents containing at least one of the specified keywords",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  NOT operator: excludes specific keyword from search results",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  AND: means all; OR: means any (in the context of Boolean queries)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Index Terms**: Distinct terms remaining after preprocessing.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Vocabulary**: Set of index terms.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Document Vector**: A vector representing a document in the vector space, with each element being the weight of an index term.",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Vocabulary**: a set of terms used to represent documents or queries",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Term weights**: numerical values assigned to each term in the vocabulary",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Document**: a unit of information represented as a set of weighted terms",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Query**: a request for information, represented as a set of weighted terms",
        "lecture": "info_retrieval"
      },
      {
        "text": "* tf: term frequency in a document (number of times a term appears in a document)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  df; = document frequency of term",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  = number of documents containing term i",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  N: total number of documents",
        "lecture": "info_retrieval"
      },
      {
        "text": "+  df; = document frequency of term",
        "lecture": "info_retrieval"
      },
      {
        "text": "+  = number of documents containing term i",
        "lecture": "info_retrieval"
      },
      {
        "text": "+  N: total number of documents",
        "lecture": "info_retrieval"
      },
      {
        "text": "* df: document frequency, number of documents containing a term",
        "lecture": "info_retrieval"
      },
      {
        "text": "* idf: inverse document frequency, measures importance of a term in the collection",
        "lecture": "info_retrieval"
      },
      {
        "text": "Note that I didn't mark any of these examples as [EXAMPLE] since they are not concrete examples, but rather illustrative cases. Instead, I categorized them under .",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  tf-idf: a method for calculating the importance of a term in a document",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  N/df: the number of documents divided by the frequency of the term (denominator)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  IDF: Inverse Document Frequency, a measure of how rare a term is across all documents",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Term Frequency (tf): ratio of the frequency of a term in a document to the total number of terms in the document",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Inverse Document Frequency (idf): logarithm of the ratio of the total number of documents to the number of documents containing the term",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Distance between vectors d, and d, captured by cosine of the angle between them",
        "lecture": "info_retrieval"
      },
      {
        "text": "* A similarity measure is a function that computes the degree of similarity between two vectors",
        "lecture": "info_retrieval"
      },
      {
        "text": "* For binary vectors, the inner product is the number of matched query terms in the document (size of intersection) (Hamming distance) ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "* For weighted term vectors, it is the sum of the products of the weights of the matched terms. ()",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Binary vectors: vectors where each element is either 0 or 1 (representing matched or non-matched terms)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Weighted term vectors: vectors where each element represents the weight of a term",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Hamming distance: the number of matched query terms in the document",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** o: term in the document or query (denoted by 1 if present, 0 if not)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** S: size of vocabulary (number of unique terms)",
        "lecture": "info_retrieval"
      },
      {
        "text": "None extracted, but note that \"Cosine Similarity\" and \"Vector Lengths\" are key concepts with implications for understanding the definition.",
        "lecture": "info_retrieval"
      },
      {
        "text": "**idf (Inverse Document Frequency)**: a measure of how rare a word is in the document collection.",
        "lecture": "info_retrieval"
      },
      {
        "text": "**cosSim (Cosine Similarity)**: a measure of similarity between two vectors, used to compute the score of each document.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **Weighted vector**: A vector where each component represents the importance of a particular feature or term, often calculated using techniques like TF-IDF.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Term independence: the assumption that terms in a query are independent of each other",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Inverted index: an index data structure used to store the location(s) of each word in a document collection",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Linked Inverted Index: an extension of inverted indexing that stores pointers to linked words with their occurrences",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Skip Pointers: pointers that allow for efficient merging of sorted lists",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Case folding: converting all uppercase letters to lowercase",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Stemming: reducing words to their morphological roots",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Stop words: words that are so common they provide no information",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** Term Frequency: Not explicitly defined, but implied as the frequency of a term in a particular document (e.g., \"ae\" has a certain frequency in the system 1 document).",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. **** Document Frequency (df): The number of documents that contain a particular term.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "3. **** Inverted Index: A data structure that stores terms and their corresponding postings lists.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* *Inverted file*: A list of positions by word",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Each entry in the inverted file represents a word and its corresponding positions",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. ** Term**: A single word or phrase in a document (e.g., \"Antony\", \"Brutus\", etc.)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. ** Document**: A unit of text that is indexed by the inverted index (e.g., \"Antony and Cleopatra\", \"Julius Caesar\", etc.)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Complemented vector (implied as \"complemented\" in the text) (): a vector that has been modified to represent the absence of certain terms.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Term-document matrix: a matrix representing the presence or absence of terms in documents ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Linked lists: Data structures that allow for dynamic space allocation and easy insertion of terms into documents.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverted indexing: A technique used to efficiently store and retrieve information in search engines and databases.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "[PRIORITY] HIGH for Inverted Indexing concept and  Linked lists",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Document ID**: Unique identifier assigned to each document.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Modified Token**: Modified form of a word, e.g., \"Caesar\" becomes \"caesar\".",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Sequence of (Modified token, Document ID) pairs**: A list of tuples containing the modified word and its corresponding document ID.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. Inverted File: A sorted list of terms with their corresponding documents",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**DEFINITIONS: **",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **Term**: A word or phrase in a document ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. **Document Frequency (DF)**: The number of documents containing a particular term ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "3. **Term Frequency (TF)**: The frequency of a term within a document ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Term Frequency (TF)**: the number of times a term appears in a single document.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Document Frequency (DF)**: the number of documents that contain a particular term.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Inverted Index**: a data structure used to efficiently retrieve the locations and frequencies of terms in a document collection.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Dictionary: a data structure that stores words and their corresponding postings",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Postings: the document ids associated with a word in the dictionary",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Postings**: A collection of document IDs that contain a specific term.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **AND Operator**: A logical operator that combines two or more terms, returning only documents that contain all specified terms.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Inverted Indexing: A data structure that stores references to words in documents, allowing for efficient retrieval.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Posting lists: Lists of postings that contain specific word or phrase occurrences.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Successor (in the context of postings): The next posting after a given posting.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1.  **Skip Pointers**: shortcuts that help for AND queries and are useful when the corpus is relatively static",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2.  **Posting List**: a list of documents containing a particular term",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Biword (or 2-gram):  A consecutive pair of terms in a text",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Dictionary term:  Each bi-word is treated as a single dictionary term",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. Vocabulary database : a collection of words or phrases used in indexing",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. Boolean query on biwords : a search query broken down into smaller segments (biwords)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Inverted Entry**: an entry in the inverted index containing information about a specific term, including:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* ** Posting**: A record of the occurrences of a term in a document, including its frequency and positions.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* ** Token index**: The index of a token (word) within a document.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* POS tagging: Part-of-Speech tagging, a process of automatically assigning grammatical categories to words in a text*",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. N-grams: any sequence of consecutive words",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. Postings: pointers to all dictionary terms containing a given n-gram",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Token: An individual item in a dataset (e.g., word, character).",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Bigram: A sequence of two items (e.g., words).",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Trigram: A sequence of three items (e.g., words).",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Four-gram: A sequence of four items (e.g., words).",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1.  **N-gram**: A sequence of n items (characters, words, etc.) from a text.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2.  **Uni-gram**, **Bi-gram**, **3-gram**, **4-gram**: Specific types of n-grams with different lengths.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Master Machine**: A central machine that directs the indexing job and assigns tasks to idle machines in the pool.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Indexing Job**: The process of creating an inverted index for efficient searching and retrieval of data.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Parser: reads documents and emits (term, doc) pairs",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverter: complements the parser to complete index inversion",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Split: a subset of documents assigned to an idle parser machine",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Posting: a data structure that stores the locations of words in a document",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Inverted Index: an index data structure used for fast and efficient searching of text documents",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Partition: a subset of documents or terms (not explicitly defined, but implied)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Invalidating bit-vector: a data structure used to mark deleted documents in the auxiliary index",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** Inverted Index: a mapping of keywords or phrases in a document collection to the documents that contain them.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. **** Document Collection: a set of documents that are being indexed and searched.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Query box: A text field in a web search engine where users can enter keywords or phrases to initiate a search.",
        "lecture": "querying"
      },
      {
        "text": "Boolean query: a search method that allows users to enter specific keywords and operators to refine their search results",
        "lecture": "querying"
      },
      {
        "text": "*  **SOY cose som** ( acronym or term not explicitly defined)",
        "lecture": "querying"
      },
      {
        "text": "*  **Google apple orchard computer**: Example of a query",
        "lecture": "querying"
      },
      {
        "text": "*  AND operator: used to combine two or more keywords in a search query (e.g., \"disney AND disneyland AND pirates\").",
        "lecture": "querying"
      },
      {
        "text": "*  Electromotive force (EMF) - \"The pressure that is put on free electrons that causes them to flow\"",
        "lecture": "querying"
      },
      {
        "text": "*  Magnetism",
        "lecture": "querying"
      },
      {
        "text": "*  Electric generator",
        "lecture": "querying"
      },
      {
        "text": "Stop Words: common words like \"the,\" \"and,\" etc. that are ignored in searches",
        "lecture": "querying"
      },
      {
        "text": "*  Case sensitivity: Google's ability to treat uppercase and lowercase letters the same way",
        "lecture": "querying"
      },
      {
        "text": "*  None explicitly stated",
        "lecture": "querying"
      },
      {
        "text": "1. **Streaming Service**:  - A type of service that provides access to content on-demand over the internet.",
        "lecture": "querying"
      },
      {
        "text": "2. **Database**:  - A collection of organized data stored in a way that allows for efficient retrieval and manipulation.",
        "lecture": "querying"
      },
      {
        "text": "* Boolean OR: an operator used to combine keywords in a search query, allowing for multiple possible matches",
        "lecture": "querying"
      },
      {
        "text": "* OR always in all caps",
        "lecture": "querying"
      },
      {
        "text": "* Implicit ANDing: The default behavior of search engines to treat all query terms as ANDed, unless specified otherwise",
        "lecture": "querying"
      },
      {
        "text": "* Precedence: The order in which operations or operators are performed in a query",
        "lecture": "querying"
      },
      {
        "text": "*  **Wildcard**: a symbol used in search queries to represent one or more characters (e.g. +)",
        "lecture": "querying"
      },
      {
        "text": "*  **Stop word**: a common word that is ignored by the search engine (e.g. \"the\", \"and\")",
        "lecture": "querying"
      },
      {
        "text": "*  daterange: Service - no definition provided ( likely a search operator)",
        "lecture": "querying"
      },
      {
        "text": "*  filetype: allinanchor:, allintext:, allintitle:, allinurl:, cache:, define:, jnanchor: - no clear definition, appears to be list of search operators",
        "lecture": "querying"
      },
      {
        "text": "* filetype: restricts search results to files with a specific suffix",
        "lecture": "querying"
      },
      {
        "text": "* Filetype: should not have spaces between filetype and the file extension",
        "lecture": "querying"
      },
      {
        "text": "* Optional inclusion of the dot (.) in file extensions",
        "lecture": "querying"
      },
      {
        "text": "* Anchor text: the text on links to a page",
        "lecture": "querying"
      },
      {
        "text": "*  Intext: A query type that searches for the exact phrase within the body text (e.g., \"Pirates of the Caribbean\")",
        "lecture": "querying"
      },
      {
        "text": "* intitle: - an operator that restricts search results to documents containing a particular word in its title.",
        "lecture": "querying"
      },
      {
        "text": "*  `site:` operator - restricts search results to a specific website or domain",
        "lecture": "querying"
      },
      {
        "text": "*  Domain - the name of a website (e.g. google.com, cs.stanford.edu)",
        "lecture": "querying"
      },
      {
        "text": "* None in this specific content, but it's worth noting that \"web cache\" is a term related to web searching and caching. However, since there are no specific definitions provided, I won't mark it as .",
        "lecture": "querying"
      },
      {
        "text": "* Qos: aan + There can be no space between related: and the URL.",
        "lecture": "querying"
      },
      {
        "text": "* \"Related:\" is a search operator that lists web pages similar to a specified page",
        "lecture": "querying"
      },
      {
        "text": "* Stock ticker symbols",
        "lecture": "querying"
      },
      {
        "text": "*  Antidisestablishmentarianism: a term used to demonstrate the limits of the English language",
        "lecture": "querying"
      },
      {
        "text": "*  Tracking number: a unique code provided by shipping companies like FedEx or UPS",
        "lecture": "querying"
      },
      {
        "text": "**DEFINITIONS **",
        "lecture": "querying"
      },
      {
        "text": "1. **Phonebook:**  - Searches the entire Google phonebook.",
        "lecture": "querying"
      },
      {
        "text": "2. **rphonebook:**  - Searches residential listings only.",
        "lecture": "querying"
      },
      {
        "text": "3. **bphonebook:**  - Searches business listings only.",
        "lecture": "querying"
      },
      {
        "text": "**Statistical model**: A mathematical representation of a system or process that uses statistical methods to make predictions or classify data.",
        "lecture": "querying"
      },
      {
        "text": "**Reading level**: The complexity or difficulty of written content, often measured by factors such as vocabulary, sentence structure, and grammar.",
        "lecture": "querying"
      },
      {
        "text": "*  Trigram index: A method of indexing data to enable fast searching.",
        "lecture": "querying"
      },
      {
        "text": "*  Regular expression engine: A software component that searches for patterns in text or code using regular expressions.",
        "lecture": "querying"
      },
      {
        "text": "*  POSIX extended regular expression syntax: A standard for regular expressions used in Unix-like operating systems.",
        "lecture": "querying"
      },
      {
        "text": "*  OCR: Optical Character Recognition, the process of recognizing and extracting text from images or scans",
        "lecture": "querying"
      },
      {
        "text": "Querying: The process of searching for specific information in a database or repository.",
        "lecture": "querying"
      },
      {
        "text": "Full View: A mode of viewing search results that displays the entire content of each result.",
        "lecture": "querying"
      },
      {
        "text": "Snippet View: A mode of viewing search results that displays a brief summary or excerpt of each result.",
        "lecture": "querying"
      },
      {
        "text": "Limited View: A mode of viewing search results that restricts the amount of information displayed for each result.",
        "lecture": "querying"
      },
      {
        "text": "Full View, Snippet View, and Limited View: MEDIUM (Knowledge of these modes is important but not as critical as querying concepts)",
        "lecture": "querying"
      },
      {
        "text": "*  Google Scholar: freely accessible search engine that indexes the full text or metadata of scholarly literature",
        "lecture": "querying"
      },
      {
        "text": "* Peer-to-peer processing: \"a way to utilize peer-to-peer networking for distributed computing\" ()",
        "lecture": "querying"
      },
      {
        "text": "* Distributed computing: not explicitly defined, but implied as a concept related to peer-to-peer processing ()",
        "lecture": "querying"
      },
      {
        "text": "**Category-based search**: Searching for a category, such as \"rock bands\" or \"german cars\", to get related searches and top members.",
        "lecture": "querying"
      },
      {
        "text": "**Index**: A data structure that enables fast lookup, insertion, and deletion of data in a database.",
        "lecture": "querying"
      },
      {
        "text": "* Auto-completion: predicting word or phrase that the user wants to type in without actually typing it in completely",
        "lecture": "querying"
      },
      {
        "text": "* Relevance feedback: a form of auto-completion",
        "lecture": "querying"
      },
      {
        "text": "*  **Autocomplete**: A feature that predicts the next character or word a user is likely to type",
        "lecture": "querying"
      },
      {
        "text": "1. **Related search**: A search that is suggested by the autocomplete feature based on the user's input.",
        "lecture": "querying"
      },
      {
        "text": "*  None explicitly stated, but implied concepts include:",
        "lecture": "querying"
      },
      {
        "text": "1. **Bing Auto-Completion**: Bing's feature that suggests possible queries or keywords based on previous searches and the user's input",
        "lecture": "querying"
      },
      {
        "text": "2. **Previous Queries**: Searches made by users that are stored and used to make suggestions for future queries",
        "lecture": "querying"
      },
      {
        "text": "*  Reciprocal rank: the multiplicative inverse of the rank of the first correct answer",
        "lecture": "querying"
      },
      {
        "text": "2. DEFINITIONS:",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Indexing**: The process of creating a database of web pages for searching",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Query Matching**: The process of matching user queries with relevant web pages",
        "lecture": "se-basics"
      },
      {
        "text": "* FTP (File Transfer Protocol) - an anonymous method of transferring files over the internet",
        "lecture": "se-basics"
      },
      {
        "text": "* Archie - a tool that assembled lists of files available on many FTP servers",
        "lecture": "se-basics"
      },
      {
        "text": "* Veronica - a search tool for text files available through Gopher servers",
        "lecture": "se-basics"
      },
      {
        "text": "* Jughead - a search tool for text files available through Gopher servers (not explicitly defined in the slide, but implied)",
        "lecture": "se-basics"
      },
      {
        "text": "* The Gopher protocol - a TCP/IP application layer protocol designed for distributing, searching, and retrieving documents over the Internet",
        "lecture": "se-basics"
      },
      {
        "text": "* Web crawler: A Perl-based program that crawled the web and collected URLs (marked as )",
        "lecture": "se-basics"
      },
      {
        "text": "*  Crawling: collecting data from web pages",
        "lecture": "se-basics"
      },
      {
        "text": "*  Bandwidth: the amount of data transferred over a network",
        "lecture": "se-basics"
      },
      {
        "text": "*  ALIWEB: an alternative to traditional Web search engines that uses user-submitted meta information for indexing",
        "lecture": "se-basics"
      },
      {
        "text": "**Overture**: A company that owned AltaVista and was later acquired by Yahoo!.",
        "lecture": "se-basics"
      },
      {
        "text": "**Yahoo! Search**: The search engine developed by Yahoo! using some of the technology from AltaVista.",
        "lecture": "se-basics"
      },
      {
        "text": "PPC (Pay-Per-Click): A business model where advertisers pay for each ad click on their site.",
        "lecture": "se-basics"
      },
      {
        "text": "Syndication: The practice of distributing content, such as paid listings, to multiple websites or portals.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Spam sites: Websites that are considered irrelevant or unwanted content",
        "lecture": "se-basics"
      },
      {
        "text": "Web search engine: searches for information on the public Web.",
        "lecture": "se-basics"
      },
      {
        "text": "Enterprise search engines: searches on intranets (internal corporate networks).",
        "lecture": "se-basics"
      },
      {
        "text": "Personal search engines: searches individual personal computers.",
        "lecture": "se-basics"
      },
      {
        "text": "* Crawler/Spider : Software program that traverses the web, discovers new pages, and updates existing indexes",
        "lecture": "se-basics"
      },
      {
        "text": "* Indexer : Component responsible for creating and maintaining search indexes",
        "lecture": "se-basics"
      },
      {
        "text": "* Query Processor : Module that processes user queries, retrieves relevant documents, and generates ranked results",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Corpus**: a collection of web pages built by the spider",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Inverted index**: an index created by the indexer to speed up queries",
        "lecture": "se-basics"
      },
      {
        "text": "* Semi-structured data storage in tables",
        "lecture": "se-basics"
      },
      {
        "text": "* Structured data storage in databases (NS)",
        "lecture": "se-basics"
      },
      {
        "text": "* Scale of larger than previous text corpora",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Informational intent**: want to learn about something (~40%)",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Navigational intent**: want to go to that page (~25%)",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Transactional intent**: want to do something (web-mediated) (~35%)",
        "lecture": "se-basics"
      },
      {
        "text": "1. Stop words: unnecessary words filtered from the query (e.g. \"the\", \"and\")",
        "lecture": "se-basics"
      },
      {
        "text": "*  Actor - a person who performs in plays, films, or television shows",
        "lecture": "se-basics"
      },
      {
        "text": "*  Filmmaker - a person who creates and produces films",
        "lecture": "se-basics"
      },
      {
        "text": "*  Golden Globe Awards - awards given to recognize excellence in film and television",
        "lecture": "se-basics"
      },
      {
        "text": "**Evaluation metrics**: Measures used to assess the performance of a search engine, e.g., precision, recall, F1-score.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Relevance**: The degree to which a document is relevant to a user's query.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Precision**: The ratio of true positives (relevant documents) to total number of retrieved documents.",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Precision/recall: Measures of relevance and recall in information retrieval",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Mean Average Precision (MAP): A metric to evaluate the ranking quality of a search engine",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Harmonic Mean (HM) and Measure: Metrics for evaluating multiple performance measures",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Precision**: # (relevant items retrieved) divided by #(all retrieved items)",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **Recall**: # (relevant items retrieved) divided by #(all relevant items)",
        "lecture": "se-evaluation"
      },
      {
        "text": "**True Positive (tp)**: Correctly identified relevant information",
        "lecture": "se-evaluation"
      },
      {
        "text": "**False Positive (fp)**: Incorrectly identified irrelevant information",
        "lecture": "se-evaluation"
      },
      {
        "text": "**True Negative (tn)**: Correctly identified irrelevant information",
        "lecture": "se-evaluation"
      },
      {
        "text": "**False Negative (fn)**: Incorrectly identified relevant information",
        "lecture": "se-evaluation"
      },
      {
        "text": "Relevant documents: a set of documents that are relevant to the search query (implied, not explicitly stated)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Retrieved documents: a set of documents returned by the search system or algorithm (implied, not explicitly stated)",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Recall**: Not explicitly defined, but implied as a measure of how well a system retrieves relevant documents ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **Precision**: Not explicitly defined, but implied as a measure of how accurate the retrieved documents are ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Arithmetic mean: not explicitly defined, but mentioned as a well-known concept",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Geometric mean: the nth root of the product of numbers",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Harmonic mean: strongly tends toward the least element of the list, making it useful in search engine results analysis",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** Precision: (no definition provided, but mentioned as one of the components of F-score)",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Recall: (no definition provided, but mentioned as one of the components of F-score)",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. **** F-measure: a measure that combines precision and recall",
        "lecture": "se-evaluation"
      },
      {
        "text": "4. **** F-score: another name for the F-measure",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Recall (#/6)**: The number of relevant documents retrieved out of a total of 6 (example used in the slide).",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Precision (#/4)**: The number of relevant documents retrieved out of a total of 4 (example used in the slide).",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Precision: not explicitly defined, but implied as the ratio of relevant documents to total documents (  )",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Recall: not explicitly defined, but implied as the ratio of relevant documents retrieved to total relevant documents (  )",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Average Precision (AveP(q))** : The area under the precision-recall curve, representing the ratio of relevant documents to total documents at different recall levels.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **Relevance Judgments** : Assessments of which documents are relevant to a particular query.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Recall**: The proportion of relevant documents retrieved out of all relevant documents ( exact formula not provided )",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Precision**: The proportion of relevant documents retrieved out of all documents retrieved ( exact formula not provided )",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Assuming precision of zero for a relevant document that never gets retrieved is \"reasonable\"",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** Binary assessment: An assessment that categorizes search results as relevant or not relevant (no nuance).",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Heavily skewed by collection/authorship: A result that is biased towards a particular collection of documents or author.",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** DCG: Discounted Cumulative Gain",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** CG (Cumulative Gain): The sum of graded relevance values of search results.",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. **** rel_i (Graded Relevance): The graded relevance value of the result at position i.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Rank**: The position of a document in the search results.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Discount factor**: A value used to divide the relevance grade, commonly chosen as log2(rank + 1).",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Binary Precision (P): \"The relevance of the top-ranked result\"",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Average Precision (AP): \"Relevance to user scanning low-rank results sequentially\"",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Graded Cumulative Gain (CG): \"Information gain from a set of results\"",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Discount Cumulative Gain (DCG): \"Information gain with positional weighting\"",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Normalized DCG (nDCG): \"How close the results are to the best possible\"",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** Recall: ability of a search engine to retrieve all relevant documents from a collection",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Precision: measure of how accurate the search results are, often calculated at the top positions (e.g., top 10)",
        "lecture": "se-evaluation"
      },
      {
        "text": "1.  Raters: Individuals who evaluate search results and search experience.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2.  General Guidelines: Overarching principles used by raters to evaluate search results.",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Vital**  - A special rating category that requires further review (Section 4.1 of the Rating Guidelines).",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **Useful**  - A page that is very helpful for most users.",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. **Relevant**  - A page that is helpful for many or some users.",
        "lecture": "se-evaluation"
      },
      {
        "text": "4. **Slightly Relevant**  - A page that is somewhat related to the query, but not very helpful for most users.",
        "lecture": "se-evaluation"
      },
      {
        "text": "5. **Off-Topic or Useless**  - A page that is helpful for very few or no users.",
        "lecture": "se-evaluation"
      },
      {
        "text": "6. **Unrateable**  - A page that cannot be evaluated (Section 4.6 of the Rating Guidelines).",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Variants: Two different versions of a web page being compared in an A/B test ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  **CIKM (Conference on Information and Knowledge Management)**: An international forum for presentation and discussion of research on information and knowledge management.",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  **CIKM Conference**: A conference that provides an international forum for presentation and discussion of research on information and knowledge management.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Query log files: records of user interactions with a search engine, including queries submitted, results clicked on, and timestamps",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Correlation between clicks and relevance judgments",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Autocomplete** () - A feature that suggests possible search queries based on a user's input.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **Directions** () - A type of search result that provides directions to a location.",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. **Knowledge Graph traffic** () - The information provided by the Knowledge Graph, such as entity relationships and attributes.",
        "lecture": "se-evaluation"
      },
      {
        "text": "IR: Information Retrieval",
        "lecture": "text_processing"
      },
      {
        "text": "Text classification: classification of documents as relevant or not relevant",
        "lecture": "text_processing"
      },
      {
        "text": "Standing queries: periodic search for new documents on a specific topic",
        "lecture": "text_processing"
      },
      {
        "text": "**Parser**: A component of NLP that attempts to assign a syntactic structure to a given input sentence (mentioned in the second tweet by @Robertoross)",
        "lecture": "text_processing"
      },
      {
        "text": "**Tagger**: A component of NLP that assigns a part-of-speech tag to each word in a sentence (mentioned in the second tweet by @Robertoross)",
        "lecture": "text_processing"
      },
      {
        "text": "**Document Representation**: The way in which text documents are represented in a computer.",
        "lecture": "text_processing"
      },
      {
        "text": "**Bag of Words Space**: A high-dimensional space where each dimension represents a word in the vocabulary.",
        "lecture": "text_processing"
      },
      {
        "text": "**Classification Function**: A function that takes a document as input and outputs its category.",
        "lecture": "text_processing"
      },
      {
        "text": "1. **NA intelligence** ( likely referring to \"Natural Language Processing\" or \"NLP\")",
        "lecture": "text_processing"
      },
      {
        "text": "2. **Temporal semantics collection**",
        "lecture": "text_processing"
      },
      {
        "text": "3. **Optimization network**",
        "lecture": "text_processing"
      },
      {
        "text": "*  Manual classification: Used by original Yahoo! Directory, Looksmart, about.com, ODP, PubMed",
        "lecture": "text_processing"
      },
      {
        "text": "IDE (Integrated Development Environment): a software tool for writing rules for hand-coded rule-based classifiers.",
        "lecture": "text_processing"
      },
      {
        "text": "**Document**: A piece of text that is being classified.",
        "lecture": "text_processing"
      },
      {
        "text": "**Class (C)**: A fixed set of categories or labels that documents can be assigned to (e.g. Cp, Cy, ..., Cf).",
        "lecture": "text_processing"
      },
      {
        "text": "**Training Set**: A collection of labeled documents used to train a classifier.",
        "lecture": "text_processing"
      },
      {
        "text": "**Naive Bayes**: a simple and common supervised learning algorithm",
        "lecture": "text_processing"
      },
      {
        "text": "**k-Nearest Neighbors (k-NN)**: a simple and powerful supervised learning algorithm",
        "lecture": "text_processing"
      },
      {
        "text": "**Support-vector machines (SVMs)**: a newer and generally more powerful supervised learning algorithm",
        "lecture": "text_processing"
      },
      {
        "text": "*  Feature (in supervised learning): any sort of characteristic used to describe a document or text",
        "lecture": "text_processing"
      },
      {
        "text": "*  URL, email address, punctuation, capitalization, dictionaries, network features: examples of features that can be used in text processing",
        "lecture": "text_processing"
      },
      {
        "text": "1.  **Feature Selection**: The process of selecting a subset of relevant features from the original set of features.",
        "lecture": "text_processing"
      },
      {
        "text": "Spam filtering (no definition provided)",
        "lecture": "text_processing"
      },
      {
        "text": "*  Vector: each document is represented by one component for each term (word)",
        "lecture": "text_processing"
      },
      {
        "text": "*  Dimensionality: 10,000+ dimensions or even 100,000+",
        "lecture": "text_processing"
      },
      {
        "text": "+ \"Documents\" can be considered as : Input data points or instances in the text classification problem",
        "lecture": "text_processing"
      },
      {
        "text": "+ \"Classes\" can be considered as : Categories or labels assigned to documents (e.g., spam/not spam, positive/negative review)",
        "lecture": "text_processing"
      },
      {
        "text": "Government",
        "lecture": "text_processing"
      },
      {
        "text": "Sci (Science)",
        "lecture": "text_processing"
      },
      {
        "text": "Arts",
        "lecture": "text_processing"
      },
      {
        "text": "Copyright Ellis Horowitz 2011-2012",
        "lecture": "text_processing"
      },
      {
        "text": "*  **D**: the set of all documents that belong to class",
        "lecture": "text_processing"
      },
      {
        "text": "*  Centroid/Prototype: A simple representative formed by Rocchio for each class",
        "lecture": "text_processing"
      },
      {
        "text": "*  k-neighborhood: The set of the k-nearest neighbors to a document d",
        "lecture": "text_processing"
      },
      {
        "text": "**Copyright**: Ownership or rights over original work (relevant for the Voronoi diagram citation).",
        "lecture": "text_processing"
      },
      {
        "text": "* **Testing instance** (an example to be classified)",
        "lecture": "text_processing"
      },
      {
        "text": "* **Database D** (a set of labeled training examples)",
        "lecture": "text_processing"
      },
      {
        "text": "*  Atypical example: a single example that is significantly different from the others in its category.",
        "lecture": "text_processing"
      },
      {
        "text": "*  Noise: an error in the category label of a single training example.",
        "lecture": "text_processing"
      },
      {
        "text": "*  Googlebot: Google's crawler",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Yahoo! Slurp: Yahoo's former web crawler (now retired)",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Bingbot, Adidxbot, MSNbot, MSNBotMedia, BingPreview: Bing's five crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **Coverage**: percentage of the web that should be covered",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **Relative Coverage**: comparison of coverage between competitors",
        "lecture": "web_crawling"
      },
      {
        "text": "Queue: a data structure that holds URLs to be fetched",
        "lecture": "web_crawling"
      },
      {
        "text": "Fetch: retrieving a web page from the internet",
        "lecture": "web_crawling"
      },
      {
        "text": "Parse: analyzing the content of a web page",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Unseen Web",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Seed URLs",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Frontier (of crawled pages)",
        "lecture": "web_crawling"
      },
      {
        "text": "1.  - Spider traps: dynamically generated pages that can trap crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "2.  - Robots.txt stipulations: rules set by webmasters to control crawling behavior",
        "lecture": "web_crawling"
      },
      {
        "text": "3.  - Politeness: avoiding hitting a server too often",
        "lecture": "web_crawling"
      },
      {
        "text": "Robotstxt.org: a protocol that defines limitations for web crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "Robots.txt file: a file placed in the root directory to announce crawling requests",
        "lecture": "web_crawling"
      },
      {
        "text": "Crawling: the process of a web crawler visiting and retrieving data from websites",
        "lecture": "web_crawling"
      },
      {
        "text": "**robots.txt file**: a text file placed in the root directory of a website to control crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "**Allow**: specifies URLs that can be crawled (opposite of Disallow)",
        "lecture": "web_crawling"
      },
      {
        "text": "**User-agent directive**: a rule that applies to a specific type of robot",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Robots.txt file: a text file placed at the root of a website's domain that contains instructions for web crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "*  User-agent: a software program that acts on behalf of a user, such as a web crawler",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **AP(r)**: Not explicitly defined, but it appears to be a measure or score associated with each robot name (e.g., -0.0291).",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **robot.txt**: A file used by web servers to communicate with crawlers and spiders about which parts of the website should not be crawled.",
        "lecture": "web_crawling"
      },
      {
        "text": "+  Level: The hierarchical structure of web pages, where each page has a level (e.g., level 0 for the starting page, level 1 for its direct neighbors, etc.)",
        "lecture": "web_crawling"
      },
      {
        "text": "**At each step move to page down the tree**: This phrase describes the behavior of the Depth-first Search algorithm, where it moves down one level in the tree at each iteration.",
        "lecture": "web_crawling"
      },
      {
        "text": "PageRank: an algorithm for determining the value of a page",
        "lecture": "web_crawling"
      },
      {
        "text": "BFS (Breadth-First Search): a graph traversal algorithm that visits nodes level by level",
        "lecture": "web_crawling"
      },
      {
        "text": "* Queue (Q): a data structure used to hold URLs to be crawled, with the front of the queue being the next URL to be processed",
        "lecture": "web_crawling"
      },
      {
        "text": "* Inverted index: an index that maps words or phrases to their corresponding documents or webpages",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Accrawler: A type of web crawler that efficiently indexes URLs as well as already visited pages.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Web graph: The structure of links between web pages, which is bidirectional and cyclic.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Relative URL**: A URL that must be completed to form a complete absolute URL.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Absolute URL**: A complete URL that can be used as is, without any additional processing.",
        "lecture": "web_crawling"
      },
      {
        "text": "* **Terabyte (TB)**: 1 trillion bytes = 1,000 GB",
        "lecture": "web_crawling"
      },
      {
        "text": "* **Petabyte (PB)**: 1 million terabytes = 1,000 TB",
        "lecture": "web_crawling"
      },
      {
        "text": "* **Trie data structure**: a compact digital trie, or prefix tree, used to store and retrieve strings",
        "lecture": "web_crawling"
      },
      {
        "text": "* **Delta-encoded text file**: a method of storing URLs as the difference between consecutive URLs",
        "lecture": "web_crawling"
      },
      {
        "text": "**Endmarker symbol**: A special character ($), used to indicate the end of a word.",
        "lecture": "web_crawling"
      },
      {
        "text": "**Viterbi algorithm**: Not explicitly defined, but mentioned as having time complexity O(NK).",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Hash**: A unique string of characters generated from a URL, used to identify it.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Canonicalization**: The process of selecting a single, preferred version of a URL when there are multiple variations.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Scheme: refers to the protocol used in a URL (e.g., http, https)",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Host: refers to the domain name or IP address of a website",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Percent-encoding triplet: a sequence of characters preceded by a percentage sign (%)",
        "lecture": "web_crawling"
      },
      {
        "text": "Spider Trap: A situation where a crawler re-visits the same page over and over again due to unique IDs in URLs.",
        "lecture": "web_crawling"
      },
      {
        "text": "Session ID: A unique identifier used to keep track of visitors.",
        "lecture": "web_crawling"
      },
      {
        "text": "* Keyword stuffing: using high frequency of repeated terms to score high on search engines",
        "lecture": "web_crawling"
      },
      {
        "text": "* Cloaking: technique used by spammers to return different page to crawlers than users",
        "lecture": "web_crawling"
      },
      {
        "text": "* Doorway page: a page designed to rank highly for certain keywords but returns commercial page when browser requests it",
        "lecture": "web_crawling"
      },
      {
        "text": "**DNS Resolver**: A process that resolves domain names to IP addresses.",
        "lecture": "web_crawling"
      },
      {
        "text": "**Client-Server Architecture**: The design pattern where a client requests and receives data from a server.",
        "lecture": "web_crawling"
      },
      {
        "text": "**Caching**: Storing frequently accessed data in a temporary storage area to reduce the number of requests made to the original source.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Refresh Strategies: the frequency at which the crawling process is restarted.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Duplicate pages: pages that have been previously crawled and do not need to be recrawled.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Mirror sites: websites that contain identical or similar content as another website.",
        "lecture": "web_crawling"
      },
      {
        "text": "DNS lookup: the process of resolving domain names to IP addresses",
        "lecture": "web_crawling"
      },
      {
        "text": "DNS caching: storing previously resolved IP-domain name mappings for future use",
        "lecture": "web_crawling"
      },
      {
        "text": "Pre-fetching client: making DNS resolution requests while parsing a page",
        "lecture": "web_crawling"
      },
      {
        "text": "UDP (User Datagram Protocol): a protocol used for DNS resolution",
        "lecture": "web_crawling"
      },
      {
        "text": "* Thread: the smallest sequence of programmed instructions that can be managed independently by the scheduler",
        "lecture": "web_crawling"
      },
      {
        "text": "* Process: a component of which threads are part",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Parallel crawler**: a process that consists of multiple crawling processes communicating via local network (intra-site parallel crawler)",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Incremental update: generally \"cheap\" due to compression and differential updates",
        "lecture": "web_crawling"
      },
      {
        "text": "* **** Independent strategy: \"no coordination, every process follows its extracted links\"",
        "lecture": "web_crawling"
      },
      {
        "text": "* **** Dynamic assignment: \"a central coordinator dynamically divides the web into small partitions and assigns each partition to a process\"",
        "lecture": "web_crawling"
      },
      {
        "text": "* **** Static assignment: \"Web is partitioned and assigned without central coordinator before the crawl starts\"",
        "lecture": "web_crawling"
      },
      {
        "text": "Inter-partition links: Links between different partitions in a web crawling system",
        "lecture": "web_crawling"
      },
      {
        "text": "Firewall mode: A method of handling inter-partition links where the process does not follow them",
        "lecture": "web_crawling"
      },
      {
        "text": "Cross-over mode: A method of handling inter-partition links where the process follows them and discovers more pages",
        "lecture": "web_crawling"
      },
      {
        "text": "Exchange mode: A method of handling inter-partition links where processes exchange URLs",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Exchange mode: a method of communication in web crawling where processes exchange information",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Batch communication: sending multiple URLs at once from one process to another",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Replication: duplicating popular URLs at each process to reduce exchange overhead",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Selection policy: states which pages to download",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Re-visit policy: states when to check for changes to the pages",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Politeness policy: states how to avoid overloading websites",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Parallelization policy: states how to coordinate distributed web crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **LastModified indicator**: A timestamp indicating the last time a page was modified.",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **** Shadowing: a method of collecting and storing new pages separately from the current database",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **** In-place updating: updating the index by replacing old versions with new ones without separating them",
        "lecture": "web_crawling"
      },
      {
        "text": "* Uniform policy: re-visiting all pages with the same frequency, regardless of change rate ()",
        "lecture": "web_crawling"
      },
      {
        "text": "* Proportional policy: re-visiting pages more often based on their estimated change frequency ()",
        "lecture": "web_crawling"
      },
      {
        "text": "* Sitemap: A list of pages of a web site accessible to crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "* XML (Extensible Markup Language): Used as the standard for representing sitemaps",
        "lecture": "web_crawling"
      },
      {
        "text": "Fully-qualified URL: A URL that includes the domain name and path (e.g., https://example.com/path/to/page).",
        "lecture": "web_crawling"
      },
      {
        "text": "**Reverse DNS Lookup**: A process of looking up an IP address in the DNS system to retrieve its corresponding domain name.",
        "lecture": "web_crawling"
      },
      {
        "text": "**Forward DNS Lookup**: A process of looking up a domain name in the DNS system to retrieve its corresponding IP address.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Recrawl: Requesting Google to crawl new or updated content on a site.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Googlebot: A program that crawls the web for Google search engine.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Googlebot: a search engine's software agent that crawls the web and indexes content.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  DOM (Document Object Model): a hierarchical representation of an HTML document.",
        "lecture": "web_crawling"
      },
      {
        "text": "* Internet penetration rate: The percentage of people in a region who use the internet ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Region: A geographic area such as North America, Europe and Central Asia, etc. ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. Baidu - Chinese search engine ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. Tencent - Chinese holding company of internet properties ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. Sohu.com Inc. - Chinese online media and community service provider ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "4. comScore - a company that tracks website traffic and user behavior ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Y/Y growth rate: a measure of change from one year to another, calculated as the percentage increase or decrease over the same period.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** WeChat: a messaging and social media app in China",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** Tencent Video: an online video streaming service owned by Tencent",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. **** Baidu Browser: a web browser developed by Baidu",
        "lecture": "web_serving_basics"
      },
      {
        "text": "4. **** AliPay: a mobile payment platform owned by Alibaba",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Zettabyte: 1 zettabyte = 1,024 exabytes",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Exabyte: 1 exabyte = 1,024 petabytes",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Petabyte: 1 petabyte = 1,024 terabytes",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Terabyte: 1 terabyte = 1,024 gigabytes",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  StatCounter: A tool used to track and analyze website traffic data (not explicitly defined, but implied)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** Cumulative unit shipments: the total number of units shipped over a certain period of time (in this case, 12 quarters).",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** Post-launch: referring to the period after a product is launched in the market.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Mainframe: an early type of computer that served as a centralized system for processing data and applications",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Mini: refers to the miniaturization of computers in the 1970s, which led to smaller and more portable devices",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Personal Desktop: a type of computer designed for individual use, popularized in the 1980s",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Internet Mobile: refers to the shift towards mobile internet access and computing devices (smartphones, tablets)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. Wintel ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. Amiga ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. TRS-80 ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* S3: AWS' storage product, used as a proxy for AWS scale/growth",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* TLD: Top-Level Domain (e.g. .com, .tk, etc.)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "**DEFINITIONS **",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **Content languages for websites**: Refers to the primary language used on a website.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **Primary language**: The language in which most content is written on a website or spoken by its users.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "**HTML (Hypertext Markup Language)**: A standard markup language used to create structure and content on the web.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Apache Tika toolkit: detects and extracts metadata and text content from documents using existing parser libraries",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Indexing technology: used to organize and retrieve data efficiently (e.g., Lucene, Solr)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  TLD (Top-Level Domain): e.g., .com, .org, etc.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  In-degree and out-degree distribution: refers to the number of incoming and outgoing links in a network",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Languages in which documents are written: English (55%), French, German, Spanish, Chinese ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Yahoo! Directory Search Results: a list of search results from Yahoo!'s directory.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Umbrella organization for car sharing companies in Europe: an organization that oversees and coordinates the activities of car sharing companies in Europe.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **Open Directory Project (ODP)**:  A collaborative effort to organize the web, started by Netscape (HIGH)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **Distributed directory**:  A system where data is stored and managed across multiple locations or servers (MEDIUM)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Science category in the Open Directory (104,420 entries)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Computers: Computer Science category in the Open Directory (2,971 entries)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Petabytes: a unit of digital information storage (approximately 1 petabyte = 1,000 terabytes)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Seed sites: initial websites used as starting points for web crawling",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** Deep Web Technologies: Refers to the technologies used to access and navigate the Deep Web.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** Tor: A browser designed for anonymity, used to access the Deep Web.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. **** Dark Web: A subset of the Deep Web characterized by illicit activities.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1.  **Metadata**: Data associated with a video, such as author, title, creation date, duration, coding quality, tags, description, subtitles, and transcription.",
        "lecture": "youtube"
      },
      {
        "text": "2.  **Video Recognition**: The process of identifying and extracting metadata from a video.",
        "lecture": "youtube"
      },
      {
        "text": "+  CastTV: A web-wide video search engine founded in 2006 (no longer existing)",
        "lecture": "youtube"
      },
      {
        "text": "+  Munax: An all-content search engine that powers both nationwide and worldwide search engines with video search",
        "lecture": "youtube"
      },
      {
        "text": "+  ScienceStage: An integrated universal search engine for science-oriented videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Subscription video on demand service: a service that allows users to access content for a fee",
        "lecture": "youtube"
      },
      {
        "text": "*  Major media companies: large corporations involved in the production and distribution of media content (e.g., Disney, Comcast)",
        "lecture": "youtube"
      },
      {
        "text": "**SRT**: stands for “SubRip Subtitle” file, a common subtitle/caption file format in text format.",
        "lecture": "youtube"
      },
      {
        "text": "*  Video hosting website: a platform that stores and delivers video content (not explicitly defined in the text, but implied)",
        "lecture": "youtube"
      },
      {
        "text": "*  Web traffic analysis company: a company that analyzes online traffic patterns (specifically mentioned as Alexa Internet)",
        "lecture": "youtube"
      },
      {
        "text": "*  Content Distribution Network (CDN): a system for distributing videos worldwide",
        "lecture": "youtube"
      },
      {
        "text": "*  ContentID system: YouTube's monetization system",
        "lecture": "youtube"
      },
      {
        "text": "*  Registered user: a user who has created an account on YouTube to upload videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Channel: a type of account on YouTube that includes uploaded video thumbnails, subscribed members, favorite videos, and friends' lists",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTuber: an individual with a YouTube channel",
        "lecture": "youtube"
      },
      {
        "text": "*  Upload status: current state of uploaded video (Select language >)",
        "lecture": "youtube"
      },
      {
        "text": "**Syndication**: The process of reusing or redistributing content (audio/video) on other platforms",
        "lecture": "youtube"
      },
      {
        "text": "**Captioning**: Adding text to videos for accessibility and comprehension",
        "lecture": "youtube"
      },
      {
        "text": "**Embedding**: Incorporating YouTube videos into external websites or platforms",
        "lecture": "youtube"
      },
      {
        "text": "1. **Vevo**: A video hosting service that specializes in music videos",
        "lecture": "youtube"
      },
      {
        "text": "View Count: A measure of the number of views a video has received on YouTube.",
        "lecture": "youtube"
      },
      {
        "text": "Ranking Algorithm: The method used by YouTube to determine the order and visibility of search results.",
        "lecture": "youtube"
      },
      {
        "text": "Meta Data: video titles, descriptions, tags that are core ranking factors",
        "lecture": "youtube"
      },
      {
        "text": "HD (High Definition) videos rank higher than low quality videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Aspect ratio: the ratio of width to height of a video",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube Search: A feature that returns search results based on user queries.",
        "lecture": "youtube"
      },
      {
        "text": "* None explicitly mentioned, but \"query\" can be considered a definition:  A query is likely referring to the user's search or request for video recommendations.",
        "lecture": "youtube"
      },
      {
        "text": "**Co-visitation count**: Number of times two videos are co-watched.",
        "lecture": "youtube"
      },
      {
        "text": "**Relatedness (r(vi,vj))**: Measure of how related two videos are, based on co-visitation counts.",
        "lecture": "youtube"
      },
      {
        "text": "**Normalization function (f(v; vj))**: Function that takes into account the global popularity of both seed and candidate videos.",
        "lecture": "youtube"
      },
      {
        "text": "* Video-rich snippet",
        "lecture": "youtube"
      },
      {
        "text": "* Web search",
        "lecture": "youtube"
      },
      {
        "text": "CDN: Content Distribution Network",
        "lecture": "youtube"
      },
      {
        "text": "Server: A computer that provides services or resources to other computers over a network",
        "lecture": "youtube"
      },
      {
        "text": "* Unique identifier assigned by YouTube:  A fixed-length, 11 character string, base 64 identifier",
        "lecture": "youtube"
      },
      {
        "text": "**Metadata**: Additional information about a video, such as its title, description, and tags.",
        "lecture": "youtube"
      },
      {
        "text": "*  **DNS (Domain Name System)**: System for translating domain names into IP addresses",
        "lecture": "youtube"
      },
      {
        "text": "*  **HTTP GET Request**: Type of request sent to a server to retrieve data",
        "lecture": "youtube"
      },
      {
        "text": "*  **Front-end Web Server**: Server that handles requests and delivers web content",
        "lecture": "youtube"
      },
      {
        "text": "*  Anycast namespace: a mechanism that allows packets to be sent to one of multiple destinations.",
        "lecture": "youtube"
      },
      {
        "text": "*  Unicast namespace: a mechanism that allows packets to be sent to a single destination.",
        "lecture": "youtube"
      },
      {
        "text": "*  CDN (Content Delivery Network): a system of distributed servers that store and deliver content to users based on geographic proximity",
        "lecture": "youtube"
      },
      {
        "text": "*  RTT (Round Trip Time): the time it takes for data to travel from a user's device to a server and back",
        "lecture": "youtube"
      },
      {
        "text": "*  Fingerprinting: a technique used to identify unique characteristics of digital content, such as videos or audio files",
        "lecture": "youtube"
      },
      {
        "text": "1. **** Content ID: A system used by YouTube to identify copyright infringement.",
        "lecture": "youtube"
      },
      {
        "text": "2. **** Transcoding: The process of converting video into multiple formats.",
        "lecture": "youtube"
      },
      {
        "text": "3. **** Spectrogram: A visual representation of audio frequencies over time.",
        "lecture": "youtube"
      },
      {
        "text": "*  Digitized audio signal: An audio signal converted into a digital format",
        "lecture": "youtube"
      },
      {
        "text": "*  Amplitude: The magnitude or strength of a sound wave",
        "lecture": "youtube"
      },
      {
        "text": "**Finite-state transducers**: A mathematical model used for computing hash functions, suggested as the proprietary method used by YouTube (referring to Eugene Weinstein and Pedro J. Moreno's 2007 ICASSP paper).",
        "lecture": "youtube"
      },
      {
        "text": "*  None mentioned, but Kryder's Law is linked to a Wikipedia article for further information.",
        "lecture": "youtube"
      },
      {
        "text": "*  TB (Terabyte): a unit of digital information storage, equivalent to 1 trillion bytes (TB = 1,000 GB)",
        "lecture": "youtube"
      },
      {
        "text": "*  PB (PetaByte): a unit of digital information storage, equivalent to 1 quadrillion bytes (PB = 1,000 TB)",
        "lecture": "youtube"
      },
      {
        "text": "1. **Load Balancer**: A network device that distributes incoming requests across multiple servers to optimize performance and prevent bottlenecks.",
        "lecture": "youtube"
      },
      {
        "text": "2. **Transcoding Server**: A server that converts and optimizes video files into various formats, accommodating diverse user devices and network conditions.",
        "lecture": "youtube"
      },
      {
        "text": "1. **** CDN: A network of servers that cache and serve content to users based on their geographical location.",
        "lecture": "youtube"
      },
      {
        "text": "2. **** Load Balancer: A mechanism that distributes incoming traffic across multiple web servers to improve responsiveness, reliability, and scalability.",
        "lecture": "youtube"
      },
      {
        "text": "3. **** Transcoding Servers: Specialized servers that convert video formats for smooth playback on different devices.",
        "lecture": "youtube"
      }
    ],
    "formulas": [
      {
        "text": "*  **O(log n)**: time complexity for searching in sorted order (Note: no actual formula or equation is given in this content)",
        "lecture": "deduplication"
      },
      {
        "text": "Euclidean distance: D([X}..-Xn], [¥1.--sYnl) = sqrt(Sum(x;-y;)*2) i=1...0",
        "lecture": "deduplication"
      },
      {
        "text": "Jaccard distance: D(x,y) = 1 — SIM(x,y)",
        "lecture": "deduplication"
      },
      {
        "text": "Cosine distance: (usually represented as an angle between 0 and 180 degrees)",
        "lecture": "deduplication"
      },
      {
        "text": "* d(A, B) = 0 if A and B are the same",
        "lecture": "deduplication"
      },
      {
        "text": "* d(A, B) ∈ [0, ∞] (distance is in the range [0, infinity])",
        "lecture": "deduplication"
      },
      {
        "text": "* s(A, B) = 1 if A and B are the same",
        "lecture": "deduplication"
      },
      {
        "text": "* s(A, B) ∈ [0, 1] (similarity is in the range [0, 1])",
        "lecture": "deduplication"
      },
      {
        "text": "* d(A, B) = 1 - s(A, B) (relationship between distance and similarity)",
        "lecture": "deduplication"
      },
      {
        "text": "* JS(A, B) = size( intersection A ∩ B )/size( union A ∪ B )",
        "lecture": "deduplication"
      },
      {
        "text": "* SISau(A, B) = IS(A∪B) = size( ( {C1, C3} intersect {C1, C2, C3, C4} )/( {C1, C3} union {C1, C2, C3, C4})",
        "lecture": "deduplication"
      },
      {
        "text": "**Jaccard(A,B) = size of (S(A,w) intersect S(B,w)) / size of (S(A,w) union S(B,w))**",
        "lecture": "deduplication"
      },
      {
        "text": "**Containment(A,B) = size of (S(A,w) intersect S(B,w)) / size of (S(A,w))**",
        "lecture": "deduplication"
      },
      {
        "text": "*  Jaccard similarity (J(A,B)) = |A ∩ B| / |A ∪ B|",
        "lecture": "deduplication"
      },
      {
        "text": "*  Jaccard distance = 1 - J(A,B) or {(union B) - (intersect B)}/(union-B)",
        "lecture": "deduplication"
      },
      {
        "text": "Documents D1 and D2 are near duplicates iff Hamming-Distance(Simhash(D1), Simhash(D2)) ≤ κ",
        "lecture": "deduplication"
      },
      {
        "text": "1. V[i] = 1 if bit i of hash is set, otherwise V[i] = -1",
        "lecture": "deduplication"
      },
      {
        "text": "2. simhash bit i is 1 if V[i] > 0, otherwise it is 0",
        "lecture": "deduplication"
      },
      {
        "text": "Replace each 0 by -1; multiply each 1 or -1 by weight (freq), sum these for each column:",
        "lecture": "deduplication"
      },
      {
        "text": "*  **O(n*(n-1)/2)**: original runtime calculation for checking all combinations",
        "lecture": "deduplication"
      },
      {
        "text": "*  **O(log n) + O(n) + O(log n)**: improved runtime calculation using adjacent pair approach",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Dv = (di1, d12, ..., dim)**: Document D represented as a vector of index terms, where di is the weight of the j-th term in the document.",
        "lecture": "info_retrieval"
      },
      {
        "text": "idf, = log, (W/ df)",
        "lecture": "info_retrieval"
      },
      {
        "text": "+  idf, = log, (W/ df)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* idf = log(N/df) , where N is the total number of documents (1,000,000)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  w = (1 + log(tf))*log(N/df)",
        "lecture": "info_retrieval"
      },
      {
        "text": "Note that there are no mathematical formulas or step-by-step algorithms in this content, so I did not mark anything as  or [ALGORITHM]. However, the properties and procedures mentioned may be useful to study for the midterm exam.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* a|~ = √(a_1^2 + ... + a_n^2) (  ) - implied formula for calculating vector length",
        "lecture": "info_retrieval"
      },
      {
        "text": "* cos(θ)=d⋅d' / (√(d^2) \\* √(d'^2)) (  ) - explicit cosine formula",
        "lecture": "info_retrieval"
      },
      {
        "text": "+  sim(d,q) = dsq = ∑ w_id \\* w_qi",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  sim(d,q) = dsq = ∑ w_id \\* w_qi",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  (for binary vectors) Hamming distance: size of intersection between query and document terms",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** Size of vector = size of vocabulary = 7",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** similarity(D, Q) = inner product of D and Q vectors",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** sim(D, , Q) = weighted sum of term frequencies",
        "lecture": "info_retrieval"
      },
      {
        "text": "CosSim(d, q) = (d · q) / (√(d^2) * √(q^2))",
        "lecture": "info_retrieval"
      },
      {
        "text": "dj = sqrt(Dwj^2)",
        "lecture": "info_retrieval"
      },
      {
        "text": "CosSim(D1, Q) = 10 / V(4+9+25)(0+0+44) = 0.81",
        "lecture": "info_retrieval"
      },
      {
        "text": "CosSim(D2, Q) = 2 / V(9+4941)(040+44) = 0.13",
        "lecture": "info_retrieval"
      },
      {
        "text": "Note that there are no mathematical formulas or algorithms in the provided content, so I couldn't mark any as  or [ALGORITHM]. The priority ratings are subjective and based on the assumption that a midterm exam would focus on understanding how an inverted index is created and used.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* 110100 AND 110111 AND 101111 = 100100 ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* The size of the term-document matrix is approximately 500K x 1M = half-trillion elements, but with no more than one billion 1's ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Note that there are no mathematical formulas or equations related to inverted indexing in this content, so I did not mark any as .",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**FORMULAS/EQUATIONS: **",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* O(m+n) operations for merge ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Note that I did not mark any of the content as  since there are no mathematical formulas presented.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Note that the mathematical formulas are minimal in this content, so there is no  category.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  12 + 34+ 10 * (150/ 7) = 260.285714 (example math expression)",
        "lecture": "querying"
      },
      {
        "text": "Note: There are no mathematical formulas or equations in this content, so there's no need to mark any with .",
        "lecture": "querying"
      },
      {
        "text": "None",
        "lecture": "querying"
      },
      {
        "text": "*  MRR = ∑(1/rank) / n, where n is the number of queries and rank is the reciprocal rank of each query response",
        "lecture": "querying"
      },
      {
        "text": "3. FORMULAS/EQUATIONS:",
        "lecture": "se-basics"
      },
      {
        "text": "Note: Since there are no mathematical formulas, I didn't mark any as .",
        "lecture": "se-basics"
      },
      {
        "text": "*  Mean Average Precision (MAP): P(R|q) = 1/N ∑ [p(r) * rel(r)] where N is the number of relevant documents, p(r) is the precision at rank r, and rel(r) is the relevance of the document at rank r",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  Harmonic Mean (HM): HM = 2 * MAP * Precision",
        "lecture": "se-evaluation"
      },
      {
        "text": "Precision = tp / (tp + fp)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Recall = tp / (tp + fn)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Accuracy = (tp + tn) / (tp + fp + fn + tn)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Precision = |A| / (|A| + |AN B|)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Recall = |A| / |AN B|",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Geometric mean formula: nth-root( product of numbers ) = √[n] (product of numbers)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Formula for harmonic mean calculation: (1/number 1 + 1/number 2 + ... + 1/number n) / (n-1) = 1/(sum of reciprocals / (n-1))",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** F = 2RP / (R + P): formula for calculating F-score",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Fg = ((α+1)RP / (R + αP)): more general form of F-measure with parameter α",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Recall = # Relevant items Retrieved / Total # Relevant Items**",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Precision = # Relevant items Retrieved / Total # Items Retrieved**",
        "lecture": "se-evaluation"
      },
      {
        "text": "* (Ranking #1): (1.0 + 0.67 + 0.75 + 0.8 + 0.83 + 0.6) /6 = 0.78",
        "lecture": "se-evaluation"
      },
      {
        "text": "* (Ranking #2): (0.5 + 0.4+0.5 + 0.57 + 0.56 + 0.6) /6 = 0.52",
        "lecture": "se-evaluation"
      },
      {
        "text": "Reca 0.2 02 04 04 04 06 06 06 08 1.0 5 +.4+.43)/8 = 0.55 (  )",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **MAP Formula** : MAP = (1/n) \\* Σ AveP(q)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Recall = number of relevant documents retrieved / total number of relevant documents",
        "lecture": "se-evaluation"
      },
      {
        "text": "Precision = number of relevant documents retrieved / total number of documents retrieved",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** pcG = ∑ rel_i / log2(i+1) - log2(j+1)",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** DCG, = ∑ (rel_i / log2(z+1)) for z=0 to i-1",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Discount factor calculation**: log2(rank + 1)",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Weighting formula**: (weight) = (relevance grade) / (discount factor)",
        "lecture": "se-evaluation"
      },
      {
        "text": "Note that there are no mathematical formulas (algorithms or equations) in the given content, so there is no  category.",
        "lecture": "text_processing"
      },
      {
        "text": "*  P(c|d) ≈ #(c)/ for larger k (Note: P(c|d) represents the probability of class c given document d, and #(c) is the number of documents in class c)",
        "lecture": "text_processing"
      },
      {
        "text": "Note that there are no mathematical formulas or algorithms in this content, so there is no  or [ALGORITHM] category.",
        "lecture": "web_crawling"
      },
      {
        "text": "Note that there are no mathematical formulas or examples in this content, so the categories  and [EXAMPLE] remain empty. However, since this algorithm is a key concept in web crawling, it's essential to focus on understanding how it works and applying it correctly.",
        "lecture": "web_crawling"
      },
      {
        "text": "**Search time for trie**: O(K)",
        "lecture": "web_crawling"
      },
      {
        "text": "**Search time for binary search tree**: O(K \\* log N)",
        "lecture": "web_crawling"
      },
      {
        "text": "* No formulas or equations are explicitly stated, so no  label.",
        "lecture": "web_crawling"
      },
      {
        "text": "Note that there are no mathematical formulas or algorithms in this content, so I did not mark anything with  or [ALGORITHM].",
        "lecture": "web_serving_basics"
      },
      {
        "text": "Note that there are no mathematical formulas in the provided content, so I did not mark any as .",
        "lecture": "youtube"
      },
      {
        "text": "r(vi,vj) = cij / √(ci × cj)",
        "lecture": "youtube"
      },
      {
        "text": "*  24TB * 4x (for profiles) * 365 days = 35PB/year",
        "lecture": "youtube"
      },
      {
        "text": "*  86MB * 4 (for profiles) * 1,000,000,000 = 320PB",
        "lecture": "youtube"
      }
    ],
    "algorithms": [
      {
        "text": "The process of deduplication involves identifying and removing duplicate data or records, which can be achieved through various methods such as:",
        "lecture": "deduplication"
      },
      {
        "text": "1. Deconstructing web page structure by focusing on content blocks",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Finding the right download for a project**: Start at the project's webpage or resource page, rather than browsing links directly on Apache.org.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Smarter Crawling Algorithm**:",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Better Connectivity Analysis Algorithm**:",
        "lecture": "deduplication"
      },
      {
        "text": "None explicitly mentioned in the slide content.",
        "lecture": "deduplication"
      },
      {
        "text": "*  Compute fingerprints using cryptographic hashing",
        "lecture": "deduplication"
      },
      {
        "text": "*  Syntactic similarity with edit-distance measure",
        "lecture": "deduplication"
      },
      {
        "text": "1.  Deduplication process using shingling:",
        "lecture": "deduplication"
      },
      {
        "text": "*  Step 1: Divide text into 3-shingles (sets of three consecutive words)",
        "lecture": "deduplication"
      },
      {
        "text": "*  Step 2: Calculate hash value for each 3-shingle",
        "lecture": "deduplication"
      },
      {
        "text": "*  Step 3: Select a subset of hash values that meet certain criteria (e.g., divisible by some number)",
        "lecture": "deduplication"
      },
      {
        "text": "*",
        "lecture": "deduplication"
      },
      {
        "text": "* Pick a hash size (e.g., 32 bits)",
        "lecture": "deduplication"
      },
      {
        "text": "* Initialize an array V of length hash size with zeros",
        "lecture": "deduplication"
      },
      {
        "text": "* Break down the input phrase into shingles",
        "lecture": "deduplication"
      },
      {
        "text": "* Hash each feature using a normal 32-bit hash algorithm (e.g., MD5 or SHA)",
        "lecture": "deduplication"
      },
      {
        "text": "* For each hash, update V[i] accordingly",
        "lecture": "deduplication"
      },
      {
        "text": "* If bit i of hash is set, add 1 to V[i], otherwise subtract 1 from V[i]",
        "lecture": "deduplication"
      },
      {
        "text": "Vector formed by summing weights:",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Adjacent Pair Approach**:",
        "lecture": "deduplication"
      },
      {
        "text": "* Rotate bits (bit shift left and replace lowest order bit with highest order bit) to create new fingerprints",
        "lecture": "deduplication"
      },
      {
        "text": "* Sort by fingerprint to identify pairs of identical elements",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Statistical models**: used to infer meaning from large amounts of data",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  **Machine learning models**: used to infer meaning from large amounts of data",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Taking into account the order of words in the query",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Search Process:",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. Stemming",
        "lecture": "info_retrieval"
      },
      {
        "text": "Note that there is no explicit mention of \"algorithms\" or step-by-step processes, so I did not mark anything as . Similarly, there are no concrete examples or case studies mentioned, so I did not mark any content as [EXAMPLE].",
        "lecture": "info_retrieval"
      },
      {
        "text": "* None explicitly mentioned, but the process of normalizing term frequency is implied",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Query-document scoring:",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Normalizing a vector by dividing each component by its length (  )",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  To do efficient ranking:",
        "lecture": "info_retrieval"
      },
      {
        "text": "1. Compute cosine similarity score between query and document vectors that contain the query term:",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. Rank documents by score:",
        "lecture": "info_retrieval"
      },
      {
        "text": "Note that I didn't include \"Search\" as an  since it's more of a process description rather than a step-by-step procedure. If you'd like to treat it as an algorithm, let me know!",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Partial matching and ranked results: This implies that the algorithm is capable of returning multiple results, ordered by their relevance or importance.",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Boolean model limitations (e.g., requiring term to appear in document)",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Processing Query on Linked Inverted Index: an algorithmic process to retrieve relevant documents from a linked inverted index",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Distributed Indexing: a distributed processing algorithm for scaling inverted indexes across multiple machines",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ For each word, listing all documents and text positions where it occurs",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Creating an inverted index from a file involves creating a list of words by position, where each entry is a word and its position in the original file",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* To create an inverted file, create a list of positions by word, where each entry is a word and its corresponding positions in the original file",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* [PRIORITY] MEDIUM: Creating an inverted index from a file and creating an inverted file",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** The process of creating an inverted index can be represented as a sparse matrix, where each row corresponds to a term and each column corresponds to a document.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. Take the vectors for the relevant documents (e.g., Brutus, Caesar, and Calpurnia) ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. Complement the vectors (implied as necessary for the AND operation) ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "3. Perform bitwise AND on the complemented vectors to retrieve matching documents ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Dynamic space allocation process:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "[PRIORITY] MEDIUM for  Dynamic space allocation process",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **Sorting of Inverted Index**: Sorting the list of terms by frequency or other criteria",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. **Updating of Inverted Index**: Updating the index when new documents are added to the corpus",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**ALGORITHMS: **",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **Inverted Index Construction Algorithm**: A step-by-step process to create an inverted index from a set of documents ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Building an Inverted Index:**",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Merge the two postings (postings are document ids)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Walk through two postings simultaneously in linear time ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**AND Operator Algorithm**:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Technique of Skip Pointers for merging postings**:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Stepping through lists to process postings until a certain point (e.g., processing 8 on each list).",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Identifying the successor of a posting.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Determining the skip successor of a posting based on its position in a lower list.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1.  **Simple Heuristic for Placing Skips**: use sqrt(P) evenly-spaced skip pointers for postings list of length P",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. Breaking queries longer than 2 words into biword segments",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. Matching the query to terms in the index",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Inverted Indexing Algorithm:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1.  **Analyzing N-gram statistics**: The study involved analyzing the frequency distributions of n-grams in English and Chinese texts, but no specific algorithm is mentioned.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2.  **Calculating the ratio of Chinese characters to English words**: Although not explicitly stated as an algorithm, the conclusion that 1.5 Chinese characters correspond to 1 English word can be seen as a result of analyzing n-gram statistics.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Task Assignment**: Break up indexing into sets of parallel tasks, and assign each task to an idle machine from the pool using a master machine as the coordinator.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Fault-tolerance Mechanism**: Implementing mechanisms to handle unpredictable slowdowns or failures in individual machines.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Assigning postings to a sign (assign...-| Master ]-----...)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Deletion process:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Periodic re-indexing into one main index",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** Building an inverted index involves the following steps:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  **Advanced Search**: Not explicitly defined as an algorithm, but mentioned as a feature in search engines",
        "lecture": "querying"
      },
      {
        "text": "*  None explicitly stated, but the example of querying different variations (e.g., \"yahoo/news\" vs. \"Yahoo!News\") implies a process or procedure for handling queries.",
        "lecture": "querying"
      },
      {
        "text": "* ****",
        "lecture": "querying"
      },
      {
        "text": "*  Step-by-step process to avoid common query words in URLs and links:",
        "lecture": "querying"
      },
      {
        "text": "+ Example: \"pirates of the caribbean\"",
        "lecture": "querying"
      },
      {
        "text": "*  \"cere Tec oe rin that Google has\" is not a clear algorithm, but it could be interpreted as an informal description of the process. However, it's likely referring to Google's search algorithm, which is a complex system.",
        "lecture": "querying"
      },
      {
        "text": "**ALGORITHMS **",
        "lecture": "querying"
      },
      {
        "text": "**Google's reading level classification algorithm**: While not explicitly described, the lecture mentions that Google used a statistical model to classify webpages into different reading levels.",
        "lecture": "querying"
      },
      {
        "text": "*  The methodology employed by Google Code Search combined trigram indexing with a custom-built regular expression engine.",
        "lecture": "querying"
      },
      {
        "text": "*  The service supported fast indexed regular expression searches over local code.",
        "lecture": "querying"
      },
      {
        "text": "*  **Patent search algorithm**: No specific steps mentioned, but it's implied that there is a process for searching patents.",
        "lecture": "querying"
      },
      {
        "text": "The process of querying is not explicitly described as an algorithm in this slide. However, based on the context:",
        "lecture": "querying"
      },
      {
        "text": "Search process:",
        "lecture": "querying"
      },
      {
        "text": "**B-tree Indexing**:",
        "lecture": "querying"
      },
      {
        "text": "+ Spelling corrections algorithms to assist in making guesses",
        "lecture": "querying"
      },
      {
        "text": "* The challenge is to search large index or long list of popular queries in very short amount of time so the results pop up while the user is typing",
        "lecture": "querying"
      },
      {
        "text": "* Google's autocomplete algorithm",
        "lecture": "querying"
      },
      {
        "text": "* Correction and suggestion algorithm for misspelled words",
        "lecture": "querying"
      },
      {
        "text": "*  None explicitly stated, but implied concept is the process of generating auto-complete suggestions based on user input.",
        "lecture": "querying"
      },
      {
        "text": "*  **Query Correction Algorithm**: Bing's algorithm for correcting spelling errors in user queries",
        "lecture": "querying"
      },
      {
        "text": "4. ALGORITHMS:",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Indexing and Query Processing**: Not explicitly described, but implied to be a key aspect of search engine development",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Link-Based Ranking**: Mentioned as a feature of Google's ranking algorithm",
        "lecture": "se-basics"
      },
      {
        "text": "* None explicitly mentioned, but the process of crawling the web and collecting URLs can be considered an algorithmic process (marked as )",
        "lecture": "se-basics"
      },
      {
        "text": "Lycos' indexing process (mentioned as identifying and cataloging documents)",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Spider's process**:",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Indexer's process**: creates inverted indexes with various policies (e.g., word stemming, capitalization, support for Unicode)",
        "lecture": "se-basics"
      },
      {
        "text": "6. Maintaining a user profile",
        "lecture": "se-basics"
      },
      {
        "text": "**Ranking Algorithm**: A step-by-step process for ranking documents based on relevance and other factors.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. Take the nth root of the product",
        "lecture": "se-evaluation"
      },
      {
        "text": "3. Take the reciprocal of the result",
        "lecture": "se-evaluation"
      },
      {
        "text": "Note: There are no explicit algorithms or step-by-step processes mentioned in this slide, so I did not mark any content as . Also, there are no concrete examples or case studies provided, so I did not mark any content as [EXAMPLE].",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Ranking documents with discounting**: Assign high weights to high-ranked documents and low weights to low-ranked documents using the discount factor.",
        "lecture": "se-evaluation"
      },
      {
        "text": "5. Evaluate with automatic measure like click-through on first result ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Clickstream analysis**: A process of analyzing the sequence of user clicks on a website to identify patterns, trends, and areas for improvement. (Note: This is not a mathematical formula, but rather an algorithmic process.)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Using clickthrough data to predict preferences between pairs of documents (implied, but not explicitly stated as an algorithm)",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Search algorithm** () - An algorithm that determines the order and relevance of search results based on user input.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **Spelling correction algorithm** () - An algorithm that corrects spelling errors in search queries.",
        "lecture": "se-evaluation"
      },
      {
        "text": "Not explicitly stated, but implied:",
        "lecture": "text_processing"
      },
      {
        "text": "**CKY parser**: A bottom-up parsing algorithm for context-free grammars, implemented as an assignment for Stanford's nip-class (mentioned in the third link)",
        "lecture": "text_processing"
      },
      {
        "text": "1. **Training learning planning programming garbage...** ( likely referring to a step-by-step process in machine learning or AI)",
        "lecture": "text_processing"
      },
      {
        "text": "The process of creating rules for hand-coded rule-based classifiers, which involves writing specific criteria to classify text.",
        "lecture": "text_processing"
      },
      {
        "text": "*  Simplest bag of words view of documents:",
        "lecture": "text_processing"
      },
      {
        "text": "1.  **Simple Feature Selection Method**: Use the most common terms for feature selection, but no particular foundation is required.",
        "lecture": "text_processing"
      },
      {
        "text": "Normalization of vectors to unit length (no specific steps mentioned)",
        "lecture": "text_processing"
      },
      {
        "text": "* Build surfaces to delineate classes in the space",
        "lecture": "text_processing"
      },
      {
        "text": "None explicitly mentioned in this slide content",
        "lecture": "text_processing"
      },
      {
        "text": "*  The Rocchio algorithm forms a centroid/prototype for each class",
        "lecture": "text_processing"
      },
      {
        "text": "*  Classification is performed by finding the nearest prototype/centroid",
        "lecture": "text_processing"
      },
      {
        "text": "*",
        "lecture": "text_processing"
      },
      {
        "text": "+ Assign  the category of the most similar example in D.",
        "lecture": "text_processing"
      },
      {
        "text": "*  1-Nearest Neighbor (1NN) algorithm: uses only the closest example to make predictions, prone to errors due to atypical examples and noise.",
        "lecture": "text_processing"
      },
      {
        "text": "*  k-Nearest Neighbors algorithm: finds examples and returns the majority category of these, typically with an odd value of k to avoid ties.",
        "lecture": "text_processing"
      },
      {
        "text": "*  Crawling process:",
        "lecture": "web_crawling"
      },
      {
        "text": "+  BREADTH-FIRST SEARCH algorithm:",
        "lecture": "web_crawling"
      },
      {
        "text": "**Depth-first Search Algorithm:**",
        "lecture": "web_crawling"
      },
      {
        "text": "BFS crawling process (although not explicitly described, it's mentioned as a method that brings high-quality pages early)",
        "lecture": "web_crawling"
      },
      {
        "text": "Web Crawling Algorithm:",
        "lecture": "web_crawling"
      },
      {
        "text": "Focused Crawling: New Approach by S. Chakrabarti et al (algorithm for re-ordering URLs in the queue based on their relevance and frequency of change)",
        "lecture": "web_crawling"
      },
      {
        "text": "*  To determine if a URL has already been seen:",
        "lecture": "web_crawling"
      },
      {
        "text": "*  To determine if a new page has already been seen:",
        "lecture": "web_crawling"
      },
      {
        "text": "HIGH",
        "lecture": "web_crawling"
      },
      {
        "text": "2. Use trie data structure to determine if path/resource is same as one in URL database",
        "lecture": "web_crawling"
      },
      {
        "text": "1. Store each entry as the difference (delta) between current and previous URL",
        "lecture": "web_crawling"
      },
      {
        "text": "+ Checkpointing: store full URL periodically",
        "lecture": "web_crawling"
      },
      {
        "text": "**Viterbi algorithm**: Not explicitly defined, but mentioned as having time complexity O(NK).",
        "lecture": "web_crawling"
      },
      {
        "text": "**Greedy algorithm (grep)**: Not explicitly defined, but mentioned as being used to determine if a new URL is in the set.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  URL Normalization:",
        "lecture": "web_crawling"
      },
      {
        "text": "**Web Crawler Algorithm**:",
        "lecture": "web_crawling"
      },
      {
        "text": "**DNS Request-Response Process**:",
        "lecture": "web_crawling"
      },
      {
        "text": "1. DNS caching:",
        "lecture": "web_crawling"
      },
      {
        "text": "* Distribute URL's to threads to guarantee equitable distribution of requests across different hosts",
        "lecture": "web_crawling"
      },
      {
        "text": "* Early Google spider had multiple coordinated crawlers with about 300 threads each, downloading over 100 pages per second in 2010",
        "lecture": "web_crawling"
      },
      {
        "text": "* **Crawl strategy**: a method for determining which pages to crawl next",
        "lecture": "web_crawling"
      },
      {
        "text": "* **** None explicitly stated, but implied that each strategy has its own algorithm for web crawling",
        "lecture": "web_crawling"
      },
      {
        "text": "*",
        "lecture": "web_crawling"
      },
      {
        "text": "**Method 1 for Verifying Googlebot's IP Address**",
        "lecture": "web_crawling"
      },
      {
        "text": "**Method 2 for Verifying Googlebot's IP Address**",
        "lecture": "web_crawling"
      },
      {
        "text": "*  3-phase process for Googlebot to process web pages with JavaScript:",
        "lecture": "web_crawling"
      },
      {
        "text": "*  The process of using voice-based mobile platform front-ends to replace typing, but no specific steps are provided.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "**HTTP Request-Response Cycle**: A step-by-step process of how a client's request is processed by a server, including:",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Unifies parsers under a single interface",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. Identify language they belong to (using N-grams)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Automatically opens links on those pages and archives content",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Wayback Machine's database growth rate: approximately 100TB of data per month",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1.  **Indexing Algorithm**: Acquiring meta-data associated with the video (e.g., author, title, creation date, duration, coding quality, tags, description).",
        "lecture": "youtube"
      },
      {
        "text": "The YouTube Recommendation System (no specific details provided, but mentioned as a key concept)",
        "lecture": "youtube"
      },
      {
        "text": "+ Add custom thumbnail",
        "lecture": "youtube"
      },
      {
        "text": "*  Encoding process: encode into streamable file format for faster video/audio quality (no specific steps provided)",
        "lecture": "youtube"
      },
      {
        "text": "1. No specific algorithm mentioned, but \"search algorithms\" or \"optimization techniques\" might be relevant ( Potential answer:  \"Search optimization algorithm\")",
        "lecture": "youtube"
      },
      {
        "text": "*  The process of maximizing watch time through recommendations is an algorithmic approach.",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube's recommendation algorithm uses computer algorithms to choose the first result (based on \"ee eo\" query).",
        "lecture": "youtube"
      },
      {
        "text": "+ Selection ()",
        "lecture": "youtube"
      },
      {
        "text": "+ Subsequent selection of videos (not specified as a separate algorithm, but rather part of the overall process) ()",
        "lecture": "youtube"
      },
      {
        "text": "To determine related videos:",
        "lecture": "youtube"
      },
      {
        "text": "*  **YouTube Video Delivery Process**:",
        "lecture": "youtube"
      },
      {
        "text": "*  Complicated re-direction scheme (not explicitly described, but mentioned as \"complicated\")",
        "lecture": "youtube"
      },
      {
        "text": "*  Not explicitly mentioned in the content",
        "lecture": "youtube"
      },
      {
        "text": "*  The process of checking new video uploads against the Content ID database and flagging copyright violations if a match is found",
        "lecture": "youtube"
      },
      {
        "text": "1. ****",
        "lecture": "youtube"
      },
      {
        "text": "2. ****",
        "lecture": "youtube"
      },
      {
        "text": "3. ****",
        "lecture": "youtube"
      },
      {
        "text": "1. **** Content Delivery Network (CDN) process:",
        "lecture": "youtube"
      },
      {
        "text": "2. **** Load Balancer process:",
        "lecture": "youtube"
      }
    ],
    "examples": [
      {
        "text": "1. **Web Crawling with De-Duplication** -  (identifying identical and nearly identical web pages and indexing only a single version to return as search result)",
        "lecture": "deduplication"
      },
      {
        "text": "1. **Same page with different URLs**:  - Two URLs (http://espn.go.com, http://www.espn.com) can point to the same page due to virtual hosts and distinct URL structures.",
        "lecture": "deduplication"
      },
      {
        "text": "2. **Virtual hosts example**:  - A website with multiple hostnames sharing the same document folder, but having different domain names.",
        "lecture": "deduplication"
      },
      {
        "text": "The SCOP database provides an example of deduplication in action, where multiple URLs point to the same page, demonstrating the removal of duplicate data.",
        "lecture": "deduplication"
      },
      {
        "text": "Two web pages from www.nytimes.com with slight differences in content (ads and photo).",
        "lecture": "deduplication"
      },
      {
        "text": "Analyzing a web page's DOM structure using HTML tags (e.g., Document, Head, Body)",
        "lecture": "deduplication"
      },
      {
        "text": "List of countries with Apache mirrors (e.g. \"fi\", \"ge\", \"gr\", etc.)",
        "lecture": "deduplication"
      },
      {
        "text": "Number of sites in 55 regions (281)",
        "lecture": "deduplication"
      },
      {
        "text": "Update frequency (4 hours)",
        "lecture": "deduplication"
      },
      {
        "text": "**Apache Software Foundation projects**: Various software releases listed, including:",
        "lecture": "deduplication"
      },
      {
        "text": "*  **“If that fails you can try: <mirror>/samepath”**: An example of adding redundancy in result listings",
        "lecture": "deduplication"
      },
      {
        "text": "*  Identifying related articles describing the same event.",
        "lecture": "deduplication"
      },
      {
        "text": "*  Extracting and categorizing information from a collection of similar pages (e.g., movie reviews).",
        "lecture": "deduplication"
      },
      {
        "text": "*  Identifying near-duplicates arising out of revisions, modifications, copying or merging of documents within a domain.",
        "lecture": "deduplication"
      },
      {
        "text": "*  URL matching using cryptographic hashing",
        "lecture": "deduplication"
      },
      {
        "text": "*  Detecting identical web pages using hash values",
        "lecture": "deduplication"
      },
      {
        "text": "Example of how a small change in the input text can produce a major difference in the output hash value (referenced to Wikipedia article: https://en.wikipedia.org/wiki/Cryptographic_hash_function)",
        "lecture": "deduplication"
      },
      {
        "text": "**Verisign**: an example of a certificate authority that uses cryptographic hash functions.",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Web pages with common prefix**: example of a problem where simple hashing approaches fail (e.g., all web pages start with \"<HTML>\")",
        "lecture": "deduplication"
      },
      {
        "text": "* Consider sets A = {0, 1, 2, 5, 6} and B = {0, 2, 3, 5, 7, 9}",
        "lecture": "deduplication"
      },
      {
        "text": "* Suppose we divide our items into four clusters...",
        "lecture": "deduplication"
      },
      {
        "text": "Shingleing example: \"rose is rose is rose\" produces a set S(D,w) with 5 items",
        "lecture": "deduplication"
      },
      {
        "text": "*  Tropical fish text with 3-shingles and their corresponding hash values",
        "lecture": "deduplication"
      },
      {
        "text": "*  Example of selecting a subset of hash values (e.g., those divisible by some number)",
        "lecture": "deduplication"
      },
      {
        "text": "*  Testing if two pages are near duplicates using Jaccard similarity (e.g., greater than 0.9)",
        "lecture": "deduplication"
      },
      {
        "text": "1. **** Example 1: Three strings \"the cat sat on the mat\", \"the cat sat on mat\", and \"we all scream for ice cream\" are hashed using Ahash and Simhash functions",
        "lecture": "deduplication"
      },
      {
        "text": "2. **** Analysis of bitwise Hamming distance between similar items (p1,p2) and dissimilar items (p1,p3) and (p2,p3)",
        "lecture": "deduplication"
      },
      {
        "text": "Tropical fish text example, illustrating how to create a fingerprint from 8-bit hash values:",
        "lecture": "deduplication"
      },
      {
        "text": "**Sorting and Deduplication Example**: The given example illustrates how sorting a list of numbers can lead to adjacent pairs with low bitwise Hamming distance. Specifically:",
        "lecture": "deduplication"
      },
      {
        "text": "*  **Low Hamming distance issue**: a pair with low Hamming distance that ends up apart due to sorting only picking up differences in lower order bits",
        "lecture": "deduplication"
      },
      {
        "text": "+ Hamming distance: same value under rotation and sorting",
        "lecture": "deduplication"
      },
      {
        "text": "+ Hamming distance: same value under rotation and sorting",
        "lecture": "deduplication"
      },
      {
        "text": "- (2,4), (3,6), (5,8)",
        "lecture": "deduplication"
      },
      {
        "text": "1. **** None explicitly mentioned, but the content references a video by Jurafsky and Manning, which may provide examples or case studies.",
        "lecture": "info_retrieval"
      },
      {
        "text": "5. **EXAMPLES** : The list (1. Docl, 2. Doc2, 3. Doc3) could be an example of documents being retrieved, with \"Doc\" possibly representing a document ID.",
        "lecture": "info_retrieval"
      },
      {
        "text": "- **Lexis-Nexis**: http://www.lexisnexis.com/  (MEDIUM [PRIORITY])",
        "lecture": "info_retrieval"
      },
      {
        "text": "- **Dialog**: http://www.dialog.com/  (LOW [PRIORITY])",
        "lecture": "info_retrieval"
      },
      {
        "text": "- **MEDLINE**: http://www.medlineplus.gov/  (MEDIUM [PRIORITY])",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  YouTube's recommendation system",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Amazon's recommendation system",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  TREC (Text REtrieval Conference) has had a Question/Answer track since 1999 (http://trec.nist.gov/data/qa.html)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Web pages are mostly unstructured",
        "lecture": "info_retrieval"
      },
      {
        "text": "* The DOM can provide some clues about web page structure",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Bayesian Network that represents the probabilistic relationships between diseases and symptoms",
        "lecture": "info_retrieval"
      },
      {
        "text": "1. **** Watch \"The Beauty of Data Visualization\" (18 min) to learn about data visualization.",
        "lecture": "info_retrieval"
      },
      {
        "text": "2. **** Data scientists spend a lot of time collecting and cleaning data, as data is never clean.",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Sample Boolean query with explicit AND, OR, NOT operators: [[Rio & Brazil] | [Hilo & Hawaii]] & hotel & !Hilton]",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Advanced Search example using AND, OR, and NOT operators in Google",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Simple query: \"Lincoln\" (too many matches)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* More detailed query: \"President AND Lincoln\" (returns incorrect results)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Even more detailed query: \"president AND Lincoln AND NOT (automobile OR car)\" (better, but still not ideal)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Successful query refinement: \"President AND lincoln AND (biography OR life OR birthplace OR gettysburg) AND NOT (automobile OR car)\"",
        "lecture": "info_retrieval"
      },
      {
        "text": "**Term weights and document representation**: D1 = 2T + 3T + 5T; Q = 0T + 0T + 2T",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Calculations for A, B, and C terms with given frequencies and document frequencies",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** Binary vector representation: oS Ss ws (example of binary vector with 7 terms)",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** Weighted vector representations:",
        "lecture": "info_retrieval"
      },
      {
        "text": "* **** Calculating similarity using weighted vectors: sim(D, , Q) = 10 and sim(D, , Q) = 2",
        "lecture": "info_retrieval"
      },
      {
        "text": "D1 = [3T, 7T, 1T]; CosSim(D1, Q) = 2 / V(9+4941)(040+44) = 0.13",
        "lecture": "info_retrieval"
      },
      {
        "text": "D2 is 6 times better than D1 using cosine similarity but only 5 times better using inner product",
        "lecture": "info_retrieval"
      },
      {
        "text": "* Two-term query \"B\" may prefer document containing frequently but not B, over document that contains both and B, but both less frequently",
        "lecture": "info_retrieval"
      },
      {
        "text": "*  Examples of Inverted Indices: real-world applications or scenarios where inverted indexing is used",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  biwords: an example of using n-grams to improve search results",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** System 1 document with term \"ae\" and its frequency.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2. **** Postings list for the term \"system 1\", which would contain a list of document IDs where this term appears.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "- Last entry: \"°\" at last word",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** The slide provides a concrete example of an inverted index represented as a sparse matrix:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Querying multiple documents simultaneously using bitwise AND ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Example: 110100, 110111, and 101111 (vectors for Brutus, Caesar, and Calpurnia) ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Antony and Cleopatra, Act Ill, Scene ii: This example illustrates a situation where Agrippa observes Antony's emotional response to the death of Julius Caesar.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Hamlet, Act Ill, Scene ii: This example shows Lord Polonius misinterpreting Brutus' actions in relation to the death of Julius Caesar.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Caesar —_—_> (example of a term with associated documents)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Document parsing: \"Tem Doct and these are saved with the document ces ID i. sequence of (Modified token, peesar : ...)\" is an example of how documents are parsed to extract words.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Example of document ID pairing: \"(brutus 1, Doc 1), (Caesar was killed Caesar. The noble ——— ' 1 , noble) becomes ((brutus, 1), (doc_1)), ((caesar, 2), (doc_2))\"",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**EXAMPLES: **",
        "lecture": "inverted_indexing"
      },
      {
        "text": "The provided example illustrates a simplified inverted index for a small document collection, showing how terms are indexed and their frequencies stored.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+ Merged postings: combined document ids for both words",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* Example of merging postings: \"If the list lengths are m and n, the merge takes O(m+n) operations\" ()",
        "lecture": "inverted_indexing"
      },
      {
        "text": "**Query Example**:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Processing postings until reaching a specific point, such as getting to 16 on the top list and realizing its successor is 32.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  Identifying the skip successor of a posting (8 on the lower list) as 31, allowing for skipping ahead.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+  friends romans",
        "lecture": "inverted_indexing"
      },
      {
        "text": "+  romans countrymen",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. Querying a 4-word phrase (stanford university palo alto) and breaking it down into Boolean queries on biwords",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* **** Document 1 has the following postings for term \"be\":",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* **** Document 2 has the following postings for term \"be\":",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*",
        "lecture": "inverted_indexing"
      },
      {
        "text": "* TREC Patent Frequency — Phrase Frequency Phrase*",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. Common n-grams are usually made up of stop words (e.g. \"and the\", \"there is\")",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  The Google n-gram dataset, which contains 1 trillion tokens, is an example of a large-scale inverted indexing system.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  The number of bigrams (314,843,401) and trigrams (977,069,902) in the Google n-gram sample are examples of how inverted indexing can be used to analyze text data.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1.  **Study on 200 million randomly sampled English and Chinese Web pages**: The study provides an example of analyzing large text datasets.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "2.  **Frequency distributions of uni-grams, bi-grams, 3-grams, and 4-grams in English and Chinese texts**: The study provides examples of frequency distributions for different types of n-grams.",
        "lecture": "inverted_indexing"
      },
      {
        "text": "Document corpus splitting example:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "The use of posting and inverted indexing for fast searching (no specific example given)",
        "lecture": "inverted_indexing"
      },
      {
        "text": "1. **** Inverted Index for the given document collection:",
        "lecture": "inverted_indexing"
      },
      {
        "text": "*  The query \"apple AND orchard BUT NOT computer\" still returns Apple Computer results",
        "lecture": "querying"
      },
      {
        "text": "*  Google search result showing unexpected results (image of Google search engine with a red X marked through it)",
        "lecture": "querying"
      },
      {
        "text": "Entering a Boolean query in Google's Advanced Search page:",
        "lecture": "querying"
      },
      {
        "text": "*  **Google apple orchard computer**: Example query",
        "lecture": "querying"
      },
      {
        "text": "*  **sows wer Apple Orchard -Computer**: Example of a modified query with operators",
        "lecture": "querying"
      },
      {
        "text": "*  **Whally’ Orchard**: Example query result or entity related to an orchard",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for multiple keywords at once: \"disney disneyland pirates\" is the same as searching for \"disney AND disneyland AND pirates\".",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for the phrase \"pirates of the Caribbean\" within quotes will show pages that contain the exact phrase.",
        "lecture": "querying"
      },
      {
        "text": "*  Using the AND operator, searching for \"disney\" AND \"disneyland\" AND \"pirates of the Caribbean\" will show pages that contain all three words.",
        "lecture": "querying"
      },
      {
        "text": "Query [child bicycle helmet]: finds pages with similar words to search terms",
        "lecture": "querying"
      },
      {
        "text": "Query with only Stop Words, e.g. [the who]: gets treated as significant and returns relevant results (e.g. the Rock Group)",
        "lecture": "querying"
      },
      {
        "text": "*  The query [snake grass] finds pages about plants;",
        "lecture": "querying"
      },
      {
        "text": "*  The query [snake in the grass] finds pages about sneaky people",
        "lecture": "querying"
      },
      {
        "text": "*  [Red Cross ], [ red cross ], and [ RED CROSS ] return the same results.",
        "lecture": "querying"
      },
      {
        "text": "*  Querying examples:",
        "lecture": "querying"
      },
      {
        "text": "1. **Disney+ Streaming Service**:  - An example of a streaming service provided by The Walt Disney Company.",
        "lecture": "querying"
      },
      {
        "text": "2. **Database Management**:  - Using Disney's database to manage and retrieve data about their streaming service.",
        "lecture": "querying"
      },
      {
        "text": "* Example 1: disney disneyland OR “pirates of the caribbean”",
        "lecture": "querying"
      },
      {
        "text": "1. abORc dis treated as  AND ( OR c) AND",
        "lecture": "querying"
      },
      {
        "text": "2. aORb  OR dis treated as ( OR b) AND ( OR d)",
        "lecture": "querying"
      },
      {
        "text": "3. aOR “ c” dis treated as ( OR (“ c”)) AND",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for \"it' small world\" instead of \"it's small world\"",
        "lecture": "querying"
      },
      {
        "text": "*  Using quotes to search for exact phrases (e.g. \"It'  Wonderful World\")",
        "lecture": "querying"
      },
      {
        "text": "*  Effect of stop words on search queries:",
        "lecture": "querying"
      },
      {
        "text": "*  Using advanced search operators (e.g. daterange:, filetype:) to refine search results",
        "lecture": "querying"
      },
      {
        "text": "* ****",
        "lecture": "querying"
      },
      {
        "text": "* Example 1: filetype:doc will NOT return .docx files",
        "lecture": "querying"
      },
      {
        "text": "* Explanation of how to use filetype: to restrict search results to specific file types, e.g. filetype:doc for Microsoft Word documents",
        "lecture": "querying"
      },
      {
        "text": "* Example 1: `[restaurants inanchor:gourmet]` returns pages where anchor text contains \"gourmet\" and page contains \"restaurants.\"",
        "lecture": "querying"
      },
      {
        "text": "* Example 2: `[allinanchor: best museums sydney]` returns only pages where anchor text contains all three words \"best\", \"museums\", and \"sydney\".",
        "lecture": "querying"
      },
      {
        "text": "*  Example 1: Searching for \"Pirates of the Caribbean\" with intext: \"Disney.com\"",
        "lecture": "querying"
      },
      {
        "text": "*  Example 2: Understanding the difference between searching by text, URLs, and titles",
        "lecture": "querying"
      },
      {
        "text": "* Searching with intitle: operator: \"intitle:pirates\" or \"intitle: 'pirates of the caribbean'\"",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for \"masters site:cs\" to restrict results to Stanford University's Computer Science department",
        "lecture": "querying"
      },
      {
        "text": "*  Importance of exact matching between `site:` and domain (e.g. \"masters site:cs.stanford.edu\")",
        "lecture": "querying"
      },
      {
        "text": "*  pirates site:disney.com",
        "lecture": "querying"
      },
      {
        "text": "*  pirates -site:disney.com",
        "lecture": "querying"
      },
      {
        "text": "*  Pirates -site:com",
        "lecture": "querying"
      },
      {
        "text": "*  Pirates site:edu",
        "lecture": "querying"
      },
      {
        "text": "* \"Could Improve Brain-Computer Interfaces for People with 24 Disabilities\"",
        "lecture": "querying"
      },
      {
        "text": "* Using Ctrl+ or ⌘- (Mac) and the find bar to quickly search on a webpage",
        "lecture": "querying"
      },
      {
        "text": "* Google's related: feature",
        "lecture": "querying"
      },
      {
        "text": "* Various university computer science department websites listed as examples of \"related:\" results",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for specific information on a particular web page (illustrating how users interact with the search engine)",
        "lecture": "querying"
      },
      {
        "text": "* Google stocks query with \"a0pl\", \"£aQ s#@\", and \"Co\" as stock ticker symbols",
        "lecture": "querying"
      },
      {
        "text": "*  Using @ symbol to search social media: e.g., @twitter",
        "lecture": "querying"
      },
      {
        "text": "*  Searching within a price range using $ symbol: e.g., camera $50..$100",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for synonyms using tilde symbol: e.g., ~car repair",
        "lecture": "querying"
      },
      {
        "text": "**EXAMPLES **",
        "lecture": "querying"
      },
      {
        "text": "1. **Google Phonebook Operators Example**:  - Different Google phonebook search operators (phonebook, rphonebook, bphonebook) explained.",
        "lecture": "querying"
      },
      {
        "text": "**Ovarian cancer information webpage**: The lecture uses this example to demonstrate how the feature classifies content into different reading levels.",
        "lecture": "querying"
      },
      {
        "text": "*  In 2009, Google introduced the Wonder Wheel as a flash-based interface that provided possible interpretations for search queries.",
        "lecture": "querying"
      },
      {
        "text": "*  The Wonder Wheel was removed in 2011 but restored in 2012 with a renaming of the \"wheel of possible interpretations\".",
        "lecture": "querying"
      },
      {
        "text": "*  Searching for open-source code using the \"file:\" operator to match common file extensions for a language.",
        "lecture": "querying"
      },
      {
        "text": "*  Using regular expressions in queries to search for specific patterns in code.",
        "lecture": "querying"
      },
      {
        "text": "**Google Patents Page example**: The slide shows an image of a Google Patents page with various patent results. This could be considered an example of how to use the patent search system.",
        "lecture": "querying"
      },
      {
        "text": "*  0 & Secure itps:/fchola google.com #2 (Note: This appears to be an example URL, but its relevance and importance are unclear without further context)",
        "lecture": "querying"
      },
      {
        "text": "* Google's use of peer-to-peer processing (mentioned as an example) ()",
        "lecture": "querying"
      },
      {
        "text": "+ Storage Device Evolution: General-Purpose Peer Processing ()",
        "lecture": "querying"
      },
      {
        "text": "**Related Searches for German Cars**: If you search for \"german cars\", Google will show the most popular members of that category, e.g. Audi, Volkswagen, BMW, etc.",
        "lecture": "querying"
      },
      {
        "text": "**Related Searches for Rock Bands**: If you search for \"rock bands\", related searches will include top rock bands such as Metallica and Led Zeppelin.",
        "lecture": "querying"
      },
      {
        "text": "*  Yahoo!'s 3rd result for query \"jaguar\" is Bing, indicating a ranking or algorithmic decision.",
        "lecture": "querying"
      },
      {
        "text": "*  Extensive ads for the car at the top and side of search results indicate that searches for the animal are rarer.",
        "lecture": "querying"
      },
      {
        "text": "**Query Example**: A user searches for all customers in a database who live in a specific city. The query optimizer uses an index on the customer's address to quickly locate the relevant data.",
        "lecture": "querying"
      },
      {
        "text": "* Browser filling in your name, address and/or email in a form",
        "lecture": "querying"
      },
      {
        "text": "* Search engines using past history, phonetic Soundex algorithms, and spelling corrections algorithms to assist in making guesses",
        "lecture": "querying"
      },
      {
        "text": "*  Google's auto-complete suggesting \"Anniversary Gifts\" when typing \"A\"",
        "lecture": "querying"
      },
      {
        "text": "*  Auto-complete offering different possibilities after entering the second character",
        "lecture": "querying"
      },
      {
        "text": "*  Examples of websites with autocomplete features (e.g., Google Maps, Amazon)",
        "lecture": "querying"
      },
      {
        "text": "+ \"amazon kindle\"",
        "lecture": "querying"
      },
      {
        "text": "+ Squiggly red line under \"anaheim duks\" suggesting the correct spelling \"Anaheim Ducks\"",
        "lecture": "querying"
      },
      {
        "text": "1. **Autocomplete example**: When searching for \"Mikhail Gorbachev\", the search engine also suggests related searches for \"Mike Gorbachev\" and displays links to relevant profiles on social media platforms.",
        "lecture": "querying"
      },
      {
        "text": "*  Yahoo's auto-complete system running out of alternatives after a certain number of characters are typed.",
        "lecture": "querying"
      },
      {
        "text": "*  Google's auto-complete system starting to provide suggestions from the first character typed.",
        "lecture": "querying"
      },
      {
        "text": "*  Examples of websites (e.g., \"mad mike garbage pail kids\") that have implemented auto-complete systems.",
        "lecture": "querying"
      },
      {
        "text": "* Yahoo search results for \"Gorbachev\" vs. \"gorbachev\" (mark with )",
        "lecture": "querying"
      },
      {
        "text": "* Search results for \"Mike Gorbachev\" (mark with )",
        "lecture": "querying"
      },
      {
        "text": "1. **Example of Bing Auto-Completion**: After entering \"mike garbac\", Bing finally comes up with the correct spelling, showing how it uses previous queries to improve search results",
        "lecture": "querying"
      },
      {
        "text": "*  **Bing Search Results**:",
        "lecture": "querying"
      },
      {
        "text": "*  **Corrected Spelling**: Bing offers corrected spelling suggestions for user queries",
        "lecture": "querying"
      },
      {
        "text": "*  Example 1: A system tries to translate English words to their plurals, with three sample queries and their corresponding results (cats, tori, virus)",
        "lecture": "querying"
      },
      {
        "text": "5. EXAMPLES:",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Early Search Engines (1991-1994)**: List of specific early search engines (Gopher, Archie, Veronica, Wanderer, ALIWeb, Excite, etc.)",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Successful Search Engines (1995-2000s)**: List of successful search engines that emerged in the late 1990s and early 2000s (Infoseek, Metacrawler, SavvySearch, LookSmart, Inktomi, HotBot, AskJeeves, Goto)",
        "lecture": "se-basics"
      },
      {
        "text": "* In 1990, Alan Emtage, P. Deutsch, et al of McGill Univ. developed Archie",
        "lecture": "se-basics"
      },
      {
        "text": "* Veronica and Jughead were developed in 1993 to search names of text files available through Gopher servers",
        "lecture": "se-basics"
      },
      {
        "text": "*  Excite came from the project Architext, which was started in February 1993 by six Stanford undergrad students",
        "lecture": "se-basics"
      },
      {
        "text": "*  Excite was bought by @Home for $6.5 billion and later filed for bankruptcy",
        "lecture": "se-basics"
      },
      {
        "text": "* World Wide Web Wanderer as a concrete example of an early web robot (marked as )",
        "lecture": "se-basics"
      },
      {
        "text": "*  Submission of web pages by users with their own page descriptions using ALIWEB",
        "lecture": "se-basics"
      },
      {
        "text": "*  Use case where people did not know how to submit their site, highlighting a limitation of ALIWEB",
        "lecture": "se-basics"
      },
      {
        "text": "**AltaVista's features**: The slide describes several examples of AltaVista's innovative features, such as natural language queries, advanced searching techniques, and inbound link checking.",
        "lecture": "se-basics"
      },
      {
        "text": "Lycos going public with a catalog of 54,000 documents on July 20, 1994",
        "lecture": "se-basics"
      },
      {
        "text": "Lycos reaching 394,000 documents in August 1994",
        "lecture": "se-basics"
      },
      {
        "text": "Lycos indexing over 60 million documents by November 1996",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Yahoo's early days**: The story of David Filo and Jerry Yang posting web pages with links on them in 1994.",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Yahoo's decline and acquisition**: The fact that Yahoo was purchased by Verizon in 2017 for $4.48 billion after years of decline.",
        "lecture": "se-basics"
      },
      {
        "text": "WiseNut search engine was bought by LookSmart in March 2002, but failed to gain traction.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Inktomi's database of spam sites was accidentally made public in 2001, containing over 1 million URLs",
        "lecture": "se-basics"
      },
      {
        "text": "*  Inktomi sold out to Yahoo! for approximately $235 million in December 2003",
        "lecture": "se-basics"
      },
      {
        "text": "* The launch of Ask Jeeves in 1997 as a natural language search engine",
        "lecture": "se-basics"
      },
      {
        "text": "* The use of DirectHit technology by Ask Jeeves, which proved vulnerable to spam",
        "lecture": "se-basics"
      },
      {
        "text": "* The release of Teoma search engine and its clustering technology",
        "lecture": "se-basics"
      },
      {
        "text": "* Google",
        "lecture": "se-basics"
      },
      {
        "text": "* Overture (goto.com)",
        "lecture": "se-basics"
      },
      {
        "text": "*  Multi-billion dollar business",
        "lecture": "se-basics"
      },
      {
        "text": "*  Strains the boundaries of trademark and intellectual property laws",
        "lecture": "se-basics"
      },
      {
        "text": "*  N/A (no concrete examples provided on this slide)",
        "lecture": "se-basics"
      },
      {
        "text": "* Dynamic generation of content on the web",
        "lecture": "se-basics"
      },
      {
        "text": "* Growing and expanding nature of web content",
        "lecture": "se-basics"
      },
      {
        "text": "* Chrome conflating search bar with URL address field",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Informational intent example**: Low hemoglobin",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Navigational intent example**: United Airlines",
        "lecture": "se-basics"
      },
      {
        "text": "*  **Transactional intent examples**:",
        "lecture": "se-basics"
      },
      {
        "text": "*  - Customizing bookmarks (e.g., \"weather los angeles ca- 6\")",
        "lecture": "se-basics"
      },
      {
        "text": "*  George Clooney's career as an actor and filmmaker",
        "lecture": "se-basics"
      },
      {
        "text": "*  The search engine results page (SERP) for \"George Clooney\" showcasing various news articles, images, and videos",
        "lecture": "se-basics"
      },
      {
        "text": "*  Sheraton Times Square Hotel website example: illustrating main pages, features, and amenities.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Sofitel New York Hotel in Manhattan near Times Square: another hotel website example.",
        "lecture": "se-basics"
      },
      {
        "text": "*  Viewing search history on Google Web History page",
        "lecture": "se-basics"
      },
      {
        "text": "*  Filtering search history by category (e.g., Today's searches, Yesterday's searches)",
        "lecture": "se-basics"
      },
      {
        "text": "* Yahoo's revenue fluctuations from 2013 to 2021",
        "lecture": "se-basics"
      },
      {
        "text": "* Baidu's revenue growth from 2014 to 2021",
        "lecture": "se-basics"
      },
      {
        "text": "* Google's sweetheart deal with Apple",
        "lecture": "se-basics"
      },
      {
        "text": "* Other search engines like DuckDuckGo relying on Bing's index",
        "lecture": "se-basics"
      },
      {
        "text": "*  Searching for:",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. Viewing multiple pages of Google results often does not improve precision at all ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Calculate the arithmetic mean for the numbers 3, 6, 9, and 12: (3+6+9+12)/4 = 7.5",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Calculate the geometric mean for the numbers 3, 6, 9, and 12: nth-root(1944) = 6.64",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. Take the reciprocal of the result: 1/.17 = 5.88",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Bing vs. Google**: An example used to compare the recall and precision of two search engines.",
        "lecture": "se-evaluation"
      },
      {
        "text": "**3/6 vs. 1/2**: Examples used to illustrate different ratios for recall and precision.",
        "lecture": "se-evaluation"
      },
      {
        "text": "+ Recall scores: 0.17, 0.17, 0.33, 0.5, 0.67, 0.83, 0.83, 0.83, 0.83, 1.0",
        "lecture": "se-evaluation"
      },
      {
        "text": "+ Precision scores: 1.0, 0.5, 0.67, 0.75, 0.8, 0.83, 0.71, 0.63, 0.56, 0.6",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **** Click-through on first result as a non-relevance-based measure of evaluation",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **** Studies of user behavior in lab settings and A/B testing",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. The document provides examples and guidelines for evaluators to rate search results.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* : search results (HIGH)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* : 118,812 people are shown two different sets of search results and asked which they prefer (HIGH)",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Comparing two web pages to see which one gives a better conversion rate ()",
        "lecture": "se-evaluation"
      },
      {
        "text": "**Case study:** Analyzing click data from an e-commerce website showed that users who clicked on product images were more likely to make a purchase than those who did not.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* CIKM 2008 conference held in Napa Valley, California",
        "lecture": "se-evaluation"
      },
      {
        "text": "* CIKM 2007 conference held in Lisbon, Portugal, with a best interdisciplinary paper award",
        "lecture": "se-evaluation"
      },
      {
        "text": "* CIKM 2009 conference held in Hong Kong",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  **Case Study: CIKM 2007**: The best interdisciplinary paper award at CIKM 2007 went to Fei Wu and Daniel Weld for Autonomously Semantifying Wikipedia.",
        "lecture": "se-evaluation"
      },
      {
        "text": "*  **Conference Locations**: Examples of locations where the CIKM conference has been held, such as Napa Valley Marriott Hotel & Spa in California (2008) and Hong Kong (2009).",
        "lecture": "se-evaluation"
      },
      {
        "text": "1. **Searching by voice** () - An example of how users can search using voice commands.",
        "lecture": "se-evaluation"
      },
      {
        "text": "2. **People Also Ask boxes** () - A feature that displays related questions and answers below the main search results.",
        "lecture": "se-evaluation"
      },
      {
        "text": "* Wikipedia link provided for comparison of web search engines (marked as )",
        "lecture": "se-evaluation"
      },
      {
        "text": "Unrest in the Niger delta region: a specific topic of interest",
        "lecture": "text_processing"
      },
      {
        "text": "Google Alerts: modern mass instantiation of standing queries",
        "lecture": "text_processing"
      },
      {
        "text": "**Tokenization using PTBTokenizer**: The first tweet by @Robertoross provides an example of tokenizing a text file using the Java class edu.stanford.nlp.process.PTBTokenizer",
        "lecture": "text_processing"
      },
      {
        "text": "- A 22-year-old person buying 6 properties using the ebook's methods ()",
        "lecture": "text_processing"
      },
      {
        "text": "+ The anecdotal claim of buying 6 properties at age 22 ()",
        "lecture": "text_processing"
      },
      {
        "text": "*  Accurate when job is done by experts (this is an example of a scenario where manual classification is effective)",
        "lecture": "text_processing"
      },
      {
        "text": "News agencies and intelligence agencies use hand-coded rule-based classifiers.",
        "lecture": "text_processing"
      },
      {
        "text": "Vendors provide \"IDE\" for writing such rules.",
        "lecture": "text_processing"
      },
      {
        "text": "**Building up data by amateurs**: can refine and improve the training dataset over time",
        "lecture": "text_processing"
      },
      {
        "text": "**Commercial systems using mixture of methods**: often combine multiple machine learning techniques to achieve best results",
        "lecture": "text_processing"
      },
      {
        "text": "1. **EXAMPLE**:  - A review of a movie (with satirical humor, romantic, etc.) that discusses its characteristics.",
        "lecture": "text_processing"
      },
      {
        "text": "* Some classifiers can't deal with 1,000,000 features",
        "lecture": "text_processing"
      },
      {
        "text": "* Training time for some methods is quadratic or worse in the number of features",
        "lecture": "text_processing"
      },
      {
        "text": "Paul Graham's Plan for Spam (http://www.paulgraham.com/spam.html) is an example of how Naive Bayes is used in spam filtering.",
        "lecture": "text_processing"
      },
      {
        "text": "None provided in this slide content",
        "lecture": "text_processing"
      },
      {
        "text": "*  The example given is not a concrete example, but rather an explanation of the concept",
        "lecture": "text_processing"
      },
      {
        "text": "*  The 1NN algorithm is prone to errors due to atypical examples (e.g., a single outlier) and noise in the category label of a single training example.",
        "lecture": "text_processing"
      },
      {
        "text": "*  Classes can influence each other - Small changes to one class can have ripple effect",
        "lecture": "text_processing"
      },
      {
        "text": "*  List of web crawlers at http://en.wikipedia.org/wiki/Web_crawler",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Google's crawler is called googlebot (http://support.google.com/webmasters/bin/answer.py?hl=en&answer=182072)",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Yahoo!'s web crawler is/was called Yahoo! Slurp (http://wikipedia.org/wiki/Yahoo_Search)",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Online edition of \"Our textbook\" (cited from Cambridge UP, 2009)",
        "lecture": "web_crawling"
      },
      {
        "text": "Ticketmaster.com/robots.txt: an example of a website's robots.txt file",
        "lecture": "web_crawling"
      },
      {
        "text": "**robots.txt example**:",
        "lecture": "web_crawling"
      },
      {
        "text": "*  ScienceDirect's robots.txt file (https://www.sciencedirect.com/robots.txt)",
        "lecture": "web_crawling"
      },
      {
        "text": "1. **Table 2: Top 10 favored and disfavored robots**: A concrete example showing the top 10 robot names, their frequencies, and AP(r) values.",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **Figure 2: Most frequently used robot names**: An illustration of the most common robot names in robots.txt files.",
        "lecture": "web_crawling"
      },
      {
        "text": "*",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Convert \"HTTP://www.Example.com/\" to lowercase: http://www.example.com/",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Capitalize letters in escape sequences: \"%3A\" -> \"%C2%\"",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Decode percent-encoded octets of unreserved characters: http://www.example.com/%7Eusername/ -> http://www.example.com/~username/",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Remove default port: http://www.example.com:80/bar.html -> http://www.example.com/bar.html",
        "lecture": "web_crawling"
      },
      {
        "text": "www.webmasterworld.com/page.php?id=2646844 13484654: An example URL with a unique ID that can lead to a spider trap.",
        "lecture": "web_crawling"
      },
      {
        "text": "* Example of keyword stuffing: \"We sell custom cigar humidors. Our frequency custom cigar humidors are handmade.\"",
        "lecture": "web_crawling"
      },
      {
        "text": "* Explanation of cloaking technique",
        "lecture": "web_crawling"
      },
      {
        "text": "* Google downloads ~50,000 pages/second or a billion+ pages in a day (as estimated in 2021)",
        "lecture": "web_crawling"
      },
      {
        "text": "* Early Google spider's coordinated crawlers with multiple threads",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Scenario 1: Centralized crawler control on LAN**: describes a system where one main crawler controls multiple parallel crawlers running on a local area network",
        "lecture": "web_crawling"
      },
      {
        "text": "*  **Scenario 2: Distributed crawling on widely distributed machines**: describes a system where multiple crawlers run on widely distributed machines with or without cross communication",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Organizing crawlers by country, region, or available bandwidth as a strategy for managing distributed crawlers",
        "lecture": "web_crawling"
      },
      {
        "text": "* **** No specific example given, but understanding of the three strategies can be applied to various scenarios",
        "lecture": "web_crawling"
      },
      {
        "text": "* Example XML sitemap for a three-page website",
        "lecture": "web_crawling"
      },
      {
        "text": "Hreflang tags can be used to indicate alternate URLs for different languages or regions:",
        "lecture": "web_crawling"
      },
      {
        "text": "* The mention of specific examples (e.g., Googlebot Images, AdsBot Mobile Web) can be considered as examples",
        "lecture": "web_crawling"
      },
      {
        "text": "2. **Examples of Specific Crawlers**: Googlebot Images, AdsBot Mobile Web, Googlebot Video, etc.",
        "lecture": "web_crawling"
      },
      {
        "text": "* Creating an empty robots.txt file to prevent \"File not found\" in website error log",
        "lecture": "web_crawling"
      },
      {
        "text": "* Using \"nofollow\" meta tag to prevent Googlebot from following any links on a page",
        "lecture": "web_crawling"
      },
      {
        "text": "* Adding \"rel='nofollow'\" attribute to individual links to prevent Googlebot from following them",
        "lecture": "web_crawling"
      },
      {
        "text": "*  Crawl rate example: 5 requests per second.",
        "lecture": "web_crawling"
      },
      {
        "text": "*  URL for requesting recrawl: https://developers.google.com/search/docs/advanced/crawling/ask-google-to-recrawl",
        "lecture": "web_crawling"
      },
      {
        "text": "*  None explicitly mentioned on this slide, but it's likely that the lecture will provide examples of web trends and measurements later on.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Building a web search engine today requires understanding the different dimensions of the web",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Many early slides come from Mary Meeker, a prominent venture capital firm (Kleiner Perkins Caufield & Byers)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The chart shows the share of population using the internet for different regions ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The fact that 5.03 billion people use the internet today is an example of the internet's widespread usage ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Amazon.com ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. China's growing influence on the internet, with >86% of users outside America ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "The comparison of China Mobile Internet Users' growth rate between +9% and +8%.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** Tencent dominates mobile internet usage in China with 71% market share",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** WeChat leads with ~200 minutes per user, average QQ",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The growth of digital information from documents to pictures to tweets in online information.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Yahoo making a major upgrade to Flickr (+500 million)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Instagram being purchased by Facebook for $1 billion (2010)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Snapchat's photo messaging application developed by Stanford students ($9B valuation)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "The example of YouTube uploading 500 hours of video every minute in February 2020",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Q1 2015 to Q2 2022: Period of time considered in the analysis (example of a specific timeframe)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** The comparison of iPhone and iPad cumulative unit shipments over 12 quarters post-launch, with specific numbers provided.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** The illustration of tablet growth (iPad) being ~3x faster than smartphone growth (iPhone).",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* 36% Search Engine",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Wearables Coming on Strong: an example of a technology cycle accelerating faster than the typical 10-year trend",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. Global Market Share of Personal Computing Platforms by Operating System Shipments ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Amazon AWS = Microsoft Azure = Google Cloud  (note: this is more of a comparison than an example)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Google Assistant: An example of a voice-based mobile platform front-end",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Nearly 70% of Requests are Natural / Conversational Language: A statistic illustrating the prevalence of natural language processing in user requests",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Amazon Echo as an example of a voice assistant",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Measuring the Web by various methods, such as number of websites, languages of web pages, rate of change of pages, etc.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The graph showing the growth of websites from 1991 to 2021 is an example of how website numbers have increased over time ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Tokelau (a country) has a high number of domains in the .tk TLD",
        "lecture": "web_serving_basics"
      },
      {
        "text": "**EXAMPLES **",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **Study by the United Nations**: Examined pages on search engines to identify primary languages, resulting in 0-80% distribution of content languages among websites as of 2014.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **English language dominance (1996-2008)**: Occupied roughly 80% of web pages during this period.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Google's storage requirement: 24 petabytes per day",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  Internet Archive's storage size: over 10 petabytes",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Categories of Content: pornography, spam, mirrors ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+ Cost of storing 1 petabyte: under $1,000 ()",
        "lecture": "web_serving_basics"
      },
      {
        "text": "European Car Sharing Business and Economy - an example of a structured directory of car sharing companies in Europe.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "EuroNCAP (European New Car Assessment Programme) - aims to provide consumers with realistic and independent assessment of the safety performance of cars sold in Europe.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **DMOZ's categorization structure**:  An example of how DMOZ organizes websites into categories, such as Arts, Reference, Regional, etc. (HIGH)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **RDF format usage**:  An example of how RDF format is used to represent data on the web (LOW)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The Open Directory listing for \"Science\" and its subcategories",
        "lecture": "web_serving_basics"
      },
      {
        "text": "*  The number of entries in the Science category (104,420)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* The Internet Archive has been taking snapshots every two months since 1997",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Over 412 billion web pages saved over time",
        "lecture": "web_serving_basics"
      },
      {
        "text": "* Rapid growth rate of the Wayback Machine's database (approximately 100TB/month)",
        "lecture": "web_serving_basics"
      },
      {
        "text": "1. **** Medical Records: Example of sensitive information stored in databases, which may be part of the Deep Web.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "2. **** Competitor Websites: Examples of websites that may provide valuable insights for businesses or organizations.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "3. **** Social Media: Platforms where individuals and organizations share information, potentially including subscription-based content.",
        "lecture": "web_serving_basics"
      },
      {
        "text": "+  CastTV (no longer existing)",
        "lecture": "youtube"
      },
      {
        "text": "+  Munax (released their first version in 2005 and no longer active)",
        "lecture": "youtube"
      },
      {
        "text": "+  ScienceStage (no longer active)",
        "lecture": "youtube"
      },
      {
        "text": "+  Blinkx/RhythmOne (uses speech recognition and visual analysis to process downloaded video)",
        "lecture": "youtube"
      },
      {
        "text": "* Vimeo.com, the first to support HD video, focuses on short, arty films",
        "lecture": "youtube"
      },
      {
        "text": "* Vevo.com, a joint venture of major music companies, hosts high-quality videos",
        "lecture": "youtube"
      },
      {
        "text": "* Dailymotion.com, owned by Vivendi, hosts high-quality videos",
        "lecture": "youtube"
      },
      {
        "text": "*  Hulu's ownership structure (jointly owned by Walt Disney, 21st Century Fox, Comcast, and Time Warner)",
        "lecture": "youtube"
      },
      {
        "text": "*  Netflix's evolution from delivering DVDs to developing original content",
        "lecture": "youtube"
      },
      {
        "text": "*  Recent entry of Disney+ into the market",
        "lecture": "youtube"
      },
      {
        "text": "**TalkMiner System**: an example of Text Recognition using OCR on video slides to detect words. (see https://www.youtube.com/watch?v=7N6L_m9LywM)",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube generated revenue of $19.8 billion in 2020",
        "lecture": "youtube"
      },
      {
        "text": "*  The website was ranked as the second most popular site by Alexa Internet in January 2022",
        "lecture": "youtube"
      },
      {
        "text": "*  Pscrib With 1 million subscribers as a YouTuber",
        "lecture": "youtube"
      },
      {
        "text": "*  Wop seagull Ellis Horowitz's YouTube channel",
        "lecture": "youtube"
      },
      {
        "text": "*  Example of a YouTube upload status (\"Select language >\")",
        "lecture": "youtube"
      },
      {
        "text": "**Paid Promotion**: A creator specifies that their video contains paid product placement, sponsorships, or endorsements",
        "lecture": "youtube"
      },
      {
        "text": "1. Search results for \"Katy Perry\" on YouTube",
        "lecture": "youtube"
      },
      {
        "text": "2. Vevo's role in hosting music videos for artists like Katy Perry",
        "lecture": "youtube"
      },
      {
        "text": "Computer Science Education Challenges: The video \"Computer science education: why does it suck so much and what can be done about it?\" highlights the challenges faced by computer science students, including lack of resources and outdated curriculum.",
        "lecture": "youtube"
      },
      {
        "text": "Successful Online Courses: The MIT 6.0 Introduction to Computer Science course with over 420,000 views demonstrates the effectiveness of online learning platforms in reaching a large audience.",
        "lecture": "youtube"
      },
      {
        "text": "*  Recent tennis match with short life cycle (example of a video with limited lifespan)",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube apps exist for Android and iPhone devices",
        "lecture": "youtube"
      },
      {
        "text": "*  Examples of other devices that can play YouTube videos:",
        "lecture": "youtube"
      },
      {
        "text": "*  A search for \"computer algorithms\" returns approximately 521,000 results.",
        "lecture": "youtube"
      },
      {
        "text": "*  The article mentioned at the end of the slide (\"Algorithms Take Over YouTube's Recommendations...\") is an example of how algorithms affect human behavior.",
        "lecture": "youtube"
      },
      {
        "text": "* Searching for \"tutorial on bitcoin\"",
        "lecture": "youtube"
      },
      {
        "text": "Some sights operate their own CDN, e.g. Google, YouTube",
        "lecture": "youtube"
      },
      {
        "text": "Third-party companies that offer CDN services such as Akamai, Limelight and Level 3 Communications (now part of Century Link)",
        "lecture": "youtube"
      },
      {
        "text": "Google's data centers map (https://www.google.com/about/datacenters/inside/locations/index.html)",
        "lecture": "youtube"
      },
      {
        "text": "Geographic distribution of YouTube video cache locations across the world (Africa, Atlantic Ocean, etc.)",
        "lecture": "youtube"
      },
      {
        "text": "*  Rare video requested in California: first request came from the Netherlands, but future requests were served from California",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube has paid $1 billion to rights holders via Content ID since 2007 (highlighting the effectiveness of the system)",
        "lecture": "youtube"
      },
      {
        "text": "1. **** The introduction of YouTube's new version of Content ID, Copyright Match (recent development).",
        "lecture": "youtube"
      },
      {
        "text": "*  Graph with two dimensions: time and frequency; third dimension represented by intensity or color (no specific example given)",
        "lecture": "youtube"
      },
      {
        "text": "**98% of copyright claims are automatically resolved**: This stat shows the effectiveness of Content ID, but it's not an example of a process or procedure.",
        "lecture": "youtube"
      },
      {
        "text": "*  $100 can buy 5000 gigabytes of storage (illustrating exponential growth)",
        "lecture": "youtube"
      },
      {
        "text": "*  YouTube storing approximately 35 PB of new data every year",
        "lecture": "youtube"
      },
      {
        "text": "*  Estimated total storage needs of YouTube to date (320 PB)",
        "lecture": "youtube"
      },
      {
        "text": "1. The example of YouTube's **Content Delivery Network (CDN)** ensuring seamless streaming for users worldwide.",
        "lecture": "youtube"
      },
      {
        "text": "2. The example of **Transcoding Servers** converting and optimizing video files into various formats.",
        "lecture": "youtube"
      }
    ]
  }
}