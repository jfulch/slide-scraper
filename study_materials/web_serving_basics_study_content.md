# ðŸ“– Web Serving Basics - Study Content

Here's the extracted content organized into the requested sections:

**DEFINITIONS & KEY TERMS**

* **Surface Web**: The part of the web that can be accessed through standard search engines like Google.
* **Deep Web**: The part of the web that is not indexed by standard search engines and requires special software or browsers to access.
* **Dark Web**: A subset of the Deep Web that is intentionally hidden from public view and often used for illicit activities.
* **Internet Archive**: A digital library that preserves websites, books, and other digital content.
* **Apache Nutch**: An open-source web crawling framework used by the Internet Archive.
* **Wa back Machine**: The database used by the Internet Archive to store snapshots of the web.
* **RDF (Resource Description Framework)**: A standard for describing data on the web.
* **DMOZ (Directory Mozilla Open Directory Project)**: A human-edited directory of websites that was shut down in 2016.

**ALGORITHMS & PROCESSES**

1. **Apache Nutch Algorithm**
	* Step-by-step breakdown:
		+ Crawls the web using a set of "seed sites"
		+ Follows links from those sites to capture more content
		+ Archives content every two months
	* When/why it's used: Used by the Internet Archive to take snapshots of the web.
	* Key characteristics: Open-source, uses a broad crawling approach.
2. **Internet Archive Crawler**
	* Step-by-step breakdown:
		+ Surveys the web every two months
		+ Uses Apache Nutch algorithm to crawl and archive content
	* When/why it's used: Used by the Internet Archive to take snapshots of the web.
	* Key characteristics: Uses a combination of broad crawling and targeted crawling.

**FORMULAS & CALCULATIONS**

None mentioned in the slides.

**IMPORTANT FACTS & NUMBERS**

* **4.5 petabytes**: The size of the Internet Archive's database.
* **412 billion web pages saved over time**: The number of web pages preserved by the Internet Archive.
* **100TB of data per month**: The amount of new data added to the Internet Archive every month.
* **55% in English**: The percentage of content on the web that is written in English.

**CORE CONCEPTS EXPLAINED**

1. **The Web as a Graph**: The web can be thought of as a graph, where each node represents a website and each edge represents a link between websites.
2. **Crawling vs. Indexing**: Crawling refers to the process of discovering new content on the web, while indexing refers to the process of storing that content in a database for later retrieval.
3. **The Deep Web**: The deep web is the part of the web that is not indexed by standard search engines and requires special software or browsers to access.

**EXAMPLES & APPLICATIONS**

1. **Internet Archive's Wa back Machine**: A digital library that preserves websites, books, and other digital content.
2. **DMOZ (Directory Mozilla Open Directory Project)**: A human-edited directory of websites that was shut down in 2016.
3. **Yahoo!'s Directory**: A hierarchical directory of websites that is still used today.

Note that some of the examples and applications mentioned in the slides are not explicitly stated as such, but can be inferred from the context.